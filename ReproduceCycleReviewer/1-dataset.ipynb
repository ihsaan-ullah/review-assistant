{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e484a4ba",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset_dir = \"./review-5k-dataset\"\n",
    "# dataset = load_dataset(\n",
    "#     \"WestlakeNLP/Review-5K\",\n",
    "#     cache_dir=dataset_dir\n",
    "# )\n",
    "# dataset.save_to_disk(\"review-5k-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f96086",
   "metadata": {},
   "source": [
    "# Load from Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456b0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages', 'rates', 'review_contexts', 'decision', 'title', 'id'],\n",
      "        num_rows: 4189\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages', 'rates', 'review_contexts', 'decision', 'title', 'id'],\n",
      "        num_rows: 781\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"review-5k-dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f69557",
   "metadata": {},
   "source": [
    "### Extract 150 accepted and 150 rejected from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accepted, train_rejected = [], []\n",
    "# for paper in dataset[\"train\"]:\n",
    "#     if len(train_accepted) == 150 and len(train_rejected) == 150:\n",
    "#         break\n",
    "#     if \"Reject\" in paper[\"decision\"] and len(train_rejected) < 150:\n",
    "#         train_rejected.append(paper)\n",
    "#         continue\n",
    "#     if \"Accept\" in paper[\"decision\"] and len(train_accepted) < 150:\n",
    "#         train_accepted.append(paper)\n",
    "#         continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ef2e9",
   "metadata": {},
   "source": [
    "### Sample 300 from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be17af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_300 = dataset[\"train\"].shuffle(seed=42).select(range(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d3403",
   "metadata": {},
   "source": [
    "### Save selected 300 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d84c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_json_to_folder(filename, data):\n",
    "    file_to_save = os.path.join(\"review-5k-dataset\",\"300\", filename)\n",
    "    with open(file_to_save, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "for p in sample_300:\n",
    "    save_json_to_folder(f\"{p['id']}.json\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb128e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"You are an expert academic reviewer tasked with providing a thorough and balanced evaluation of research papers. For each paper submitted, conduct a comprehensive review addressing the following aspects:\\n\\n1. Summary: Briefly outline main points and objectives.\\n2. Soundness: Assess methodology and logical consistency.\\n3. Presentation: Evaluate clarity, organization, and visual aids.\\n4. Contribution: Analyze significance and novelty in the field.\\n5. Strengths: Identify the paper's strongest aspects.\\n6. Weaknesses: Point out areas for improvement.\\n7. Questions: Pose questions for the authors.\\n8. Rating: Score 1-10, justify your rating.\\n9. Meta Review: Provide overall assessment and recommendation (Accept/Reject).\\n\\nMaintain objectivity and provide specific examples from the paper to support your evaluation.\\n\\nYou need to fill out **4** review opinions.\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'Title: ZipIt! Merging Models from Different Tasks without Training\\n\\nABSTRACT\\n\\n\\nTypical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce “ZipIt!”, a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren’t shared between models, we expand the model merging problem to allow for merging features within each model by defining a general “zip” operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for 20-60% improvement over prior work, making it more feasible to merge models trained on disjoint tasks without retraining.\\n\\n1 INTRODUCTION\\n\\nEver since AlexNet (Krizhevsky et al., 2017) popularized deep learning in computer vision, the field has thrived under the reign of massive models with an ever increasing number of parameters. Many vision problems once considered difficult or impossible are now benchmark tasks: classification with tens of thousands of classes (Deng et al., 2009; Zhou et al., 2017; Gemmeke et al., 2017), fast instance segmentation (He et al., 2017; Bolya et al., 2019), realistic image generation (Karras et al., 2018; Ho et al., 2020; Rombach et al., 2022), and more.\\n\\nThere are an abundance of independent, carefully tuned models out there for many tasks. However, if we want to expand an existing model’s capabilities, we run into many potential issues. If we try training the model on an additional task, we face catastrophic forgetting (Kirkpatrick et al., 2017; Li & Hoiem, 2017; De Lange et al., 2021). If we evaluate the same model on different data without adaptation, we often find it doesn’t generalize to out of domain samples (Blanchard et al., 2011; Muandet et al., 2013; Wang et al., 2022). We can try so called “intervention” strategies (Wang et al., 2022; De Lange et al., 2021) to mitigate these effects, but these often require further training which can be expensive (Dosovitskiy et al., 2020; Zhai et al., 2022; Dehghani et al., 2023). Instead, it would be nice if we could expand a model’s capacity to solve new tasks by simply “zipping” it with other models trained on those tasks without additional training.\\n\\nCombining multiple models into one has recently started to gain traction in the vision community. Model Soups (Wortsman et al., 2022a) can add multiple models finetuned from the same pretrained initialization to improve accuracy and robustness. Git Re-Basin (Ainsworth et al., 2022) generalizes further to models trained on the same data but with different initializations, though with a significant accuracy drop. REPAIR (Jordan et al., 2022) improves on Git Re-Basin by adding new parameters and adjusting model batch norms where applicable. However, all of these methods only combine models trained on the same task. In this paper, we take this line of work to a logical extreme: merging differently initialized models trained on completely separate tasks (see Fig. 1ab). We show that this is an incredibly difficult problem for prior work and employ two simple strategies to make it feasible.\\n\\n∗Equal Contribution. Code: https://github.com/gstoica27/ZipIt.\\n\\n\\n1\\n\\n\\n\\n\\n\\nFigure 1: Setting and ZipIt! (a) Prior work merges differently initialized models from the same dataset with the same label sets: e.g., merging two models both trained to classify dog breeds. (b) Our setting expands this to merging models from different datasets with different label sets: e.g., merging a model that classifies dog breeds with one that classifies bird species. (c) ZipIt! merges these models without retraining by identifying shared features.\\n\\nFirst, we note that prior work focuses on permuting one model to the other when merging them. This creates a 1-1 mapping between the two models, inherently assuming that most features across them are correlated. Since this isn’t necessarily the case for models trained on different tasks, we cannot rely on permutation alone. Instead, we generalize model merging to support “zipping” any combination of correlated features within and across each model. We find that on some tasks, this alone improves accuracy by up to 20% vs. permutation-based approaches. Moreover, we prove that merging within models can yield a better result in the theoretical setting of Entezari et al. (2021).\\n\\nSecond, existing methods merge the entire network. While this might work for extremely similar models trained in the same setting, the features of models trained on disjoint tasks become less correlated over the course of the network (Kornblith et al., 2019). To solve this, we introduce partial zipping, where we only “zip” up to a specified layer. Afterward, we feed the merged model’s outputs to the remaining unmerged layers of the original networks, creating a multi-head model. Depending on task difficulty, this can improve accuracy by over 15% while still keeping most layers merged.\\n\\nIncorporating both of these strategies, we introduce ZipIt! (Fig. 1c), a general method for “zipping” any number of models trained on different tasks into a single multitask model without retraining. By deriving a general graph-based algorithm for merging and unmerging (Sec. 4), we can zip models of the same architecture together, merge features within each model, and partially zip them to create a multi-task model. We validate our approach by merging models trained on entirely disjoint sets of CIFAR (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) categories, as well as merging several models trained on completely independent datasets into one, significantly outperforming prior work (Sec. 5). Finally, we ablate and analyze our method’s capabilities on these scenarios (Sec. 6).\\n\\n2 RELATED WORK\\n\\nModel merging combines the weights of two or more models into a one. Our work differs from prior work in that we adapt mode connectivity techniques to target models trained on disjoint tasks (Fig. 1).\\n\\nMerging Finetuned Models. If two models are finetuned from the same pretrained checkpoint, they often lie in the same error basin (Neyshabur et al., 2020). Several works (Huang et al., 2017; Izmailov et al., 2018; Von Oswald et al., 2020; Wortsman et al., 2022b; Ilharco et al., 2022b; Don-Yehiya et al., 2023) have exploited this property to average together the weights of a model at different stages of training. Tarvainen & Valpola (2017); Cai et al. (2021); Grill et al. (2020); Caron et al. (2021); Baevski et al. (2022) use an “exponential moving average” of training checkpoints as a teacher for self-supervised learning. Other works merge models initialized from the same pretrained base, but that were finetuned independently, either by simply averaging their weights (McMahan et al., 2017; Wortsman et al., 2022a; Choshen et al., 2022; Ram´e et al., 2022), permuting one model to the other (Ashmore & Gashler, 2015; Yurochkin et al., 2019; Wang et al., 2020), combining meaningful weight regions (Ilharco et al., 2022a; Gueta et al., 2023; Yadav et al., 2023; Sung et al., 2023), or maximizing an objective (Matena & Raffel, 2021). Our setting differs, as we do not assume the same initialization.\\n\\nMerging Differently Initialized Models. Merging models with different initializations is a much more challenging problem. Works in this space often rely on mode connectivity (Freeman & Bruna, 2016; Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020), attempting to interpolate between models along a low loss path (e.g., Tatro et al. (2020); Singh & Jaggi (2020); Liu et al. (2022)). Most recent work follow the intuition, later formalized by Entezari et al. (2021), that\\n\\n2\\n\\n\\n\\n\\n\\nFigure 2: Task Loss Landscapes for models in Tab. 1b. Model A and Model B lie in low loss basins for their own tasks, but not for the other task. Thus, any interpolation between Model A and a permuted Model B (e.g., Git Re-basin) lies outside the minima for both tasks and thus performs poorly. In contrast, ZipIt! improves the merge by finding a model that lies in a low loss basin for both.\\n\\nmodels permuted to the same loss basin can be merged by averaging their weights. Most notably, Git Re-Basin (Ainsworth et al., 2022) permutes models by comparing the similarity between their weights. REPAIR (Jordan et al., 2022) improves the accuracy of Git Re-Basin by instead computing the correlation between their intermediate layer feature activations, and adding several batch norms to the network. Pe˜ na et al. (2022) find permutations using global rather than local optimization, though they don’t support skip connections. Some of these works (e.g., Singh & Jaggi (2020); Ainsworth et al. (2022)) evaluate on on a setting where each model sees varying numbers of instances per class. And Pe˜ na et al. (2022) evaluates on a continual learning setting with disjoint categories, but their method requires training optimization. Similarly, He et al. (2018) merges models of different tasks, but requires jointly finetuning after each layer merge. As far as we are aware, we present the first general method to successfully merge models trained on disjoint tasks without additional training.\\n\\n3 BACKGROUND AND MOTIVATION\\n\\nModel merging stems from mode connectivity (Garipov et al., 2018), where it is conjectured that models trained with SGD on the same dataset lying in the same loss basin (i.e., region or mode of low loss) can be combined into a single model that’s just as performant as the original models. If these models can be combined well by linearly interpolating their weights, they are said to be linearly mode connected (LMC) (Garipov et al., 2018; Entezari et al., 2021). Our work is similar to finding LMC, but across different datasets with disjoint label sets (i.e., separate tasks as in De Lange et al. (2021)).\\n\\nConsider a model L as a collection of layers Li ∈L, each of which may have some parameters (e.g., Wi, bi for a linear layer). If LA and LB are finetuned from the same checkpoint, several works (e.g., Izmailov et al. (2018); Wortsman et al. (2022a)) find merging them is as easy as linearly interpolating their weights (i.e., they are LMC). E.g., if Li is a linear layer, the new weight matrix W ∗ i is simply\\n\\nW ∗ i = γW A i + (1 −γ)W B i (1) with an interpolation constant γ ∈[0, 1], usually set to 1/ 2. However, if LA and LB were not finetuned from the same checkpoint, they often do not lie in the same mode (Entezari et al., 2021; Ainsworth et al., 2022) and cannot be interpolated. Indeed, Eq. 1 typically results in random accuracy.\\n\\nTo fix this, Entezari et al. (2021) conjecture that large enough models are likely LMC modulo permutation. This is because (1) many neural networks can be permuted internally without affecting their outputs and (2) permutation can move two models into the same basin, allowing much of the lost accuracy to be recovered. More concretely, let Pi be a permutation matrix that permutes outputs of layer LB i to the space of LA i . Then for each layer, permutation works apply\\n\\nW ∗ i = γW A i + (1 −γ)PiW B i P T i−1 (2)\\n\\nNote that here we permute the output space of W B i , but we also need to permute its input space to undo the permutation from the previous layer (hence the use of P T i−1).\\n\\nProblems with Permutation. Eq. 2 relies on the likelihood of model B lying in the same basin as model A after permutation being high. However, this is far less likely when the models are trained\\n\\n3\\n\\n\\n\\n\\n\\non different tasks, as each model optimizes for basins containing distinct task-specific information. In this case the optimal permutation of model B to model A still lies in a strong basin on task B but doesn’t lie in a basin on task A, as shown in Figure 2. This causes the interpolated model to perform worse than either the two original models. Thus, we explore alternative merging methods.\\n\\n4 ZIPIT!\\n\\nIn this work, we treat model merging as combining the checkpoints (i.e., collection of weights) of multiple models into a single checkpoint that can perform all the tasks of its constituents. We do this by merging the layers of the models together. For instance, suppose Li ∈L is a linear layer with parameters Wi ∈Rni×mi, bi ∈Rni with input features x ∈Rmi and outputs features fi ∈Rni:\\n\\nfi = Li(x) = Wix + bi (3)\\n\\nOur goal is to take LA i ∈LA from model A and LB i ∈LB from model B and merge them into a layer L∗ i that combines their feature spaces such that information from both f A i and f B i is retained in f ∗ i . We accomplish this by merging each layer of one model with the corresponding layer in the other, both merging features in one across both layers or within the same layer. This is in contrast to permutation-based merging method, which only combine features across layers.\\n\\nWhy should we merge within? Features of models trained on different tasks may be dissimilar, as the models solve different problems. Forcibly combining these dissimilar features can yield merges that don’t perform well on either original task (Fig 2). Instead, those features may be more compatible with others within the same model, which would better retain performance when combined.\\n\\nIn fact, we can prove that methods which allow merges within each model (as well as across both) perform equal to or better than those which only merge across models (e.g., permutation-reliant approaches) in a limited but prevalent setting. Specifically, we obtain a tighter bound over Theorem 3.1 from Entezari et al. (2021) when redundancy exists within a model and is leveraged. Both Theorem 3.1 and our Theorem 1 (see Appendix G for formalization and proof) bound the degree to which the loss of a merged two-layer model with d-input dimensions and h-intermediate dimensions increases compared to the losses of the original models. Theorem 3.1 bounds this increase to ˜ O(h−1/ (2d + 4)). However, if features within a model are redundant, then we reduce the bound to\\n\\nLoss Increase of Merged Model ≤\\n\\n\\n\\uf8f1 \\uf8f2\\n\\n\\uf8f3\\n\\n\\n˜ O \\x12\\x10 h\\n\\n1−2Γ \\x11− 1\\n\\n2d+4 \\x13 Γ < 0.5\\n\\n0 otherwise (4)\\n\\nwith Γ ∈[0, 1] measuring what portion of features are redundant. This bound is 2d+4 √\\n\\n1 −2Γ ≤1 times that of Theorem 3.1 when Γ < 0.5 (equal only when Γ = 0) and explicitly zero when Γ ≥0.5.\\n\\nHow do we merge features? In prior work, each of the merged features f ∗ i is the result of combining one feature from f A i and one from f B i . However in our case, we can also merge features by combining two from just f A i or two from just f B i . To account for this, we concatenate f A i and f B i into a single feature vector: f A i ∥f B i ∈R2ni. Then, like prior work (e.g. Li et al. (2015); Jordan et al. (2022)), we define feature similarity as the pairwise correlation between between neuron activations over a small set of images (without labels). However, unlike those works, we compute correlations between every activation in the full concatenated space f A i ∥f B i . Our approach thus measures the similarity of every feature f A i and f B i to all features in both models, rather than solely between f A i and f B i .\\n\\nNext, if two features are well correlated, we can average them without losing much information. Thus, we can construct f ∗ i by finding ni pairs of similar features in f A i ∥f B i and averaging them together. By default, we do this greedily: i.e., iteratively match the features with the highest correlation without replacement; though we explore extensions to this in Sec. 4.3 and test other methods in Tab. 4. Then we can use these matches to construct f ∗ i . Formally, we define a “merge matrix” Mi ∈Rni×2ni s.t.\\n\\nf ∗ i = Mi\\n\\n\\x00\\n\\n\\nf A i ∥f B i\\n\\n\\x01 (5)\\n\\nMi averages the matched features, with each match corresponding to one output feature in f ∗ i . For instance, if uth match is between indices s, t ∈{1, . . . , 2ni} of f A i ∥f B i , then the uth row of Mi would be 1/ 2 at columns s and t and 0 elsewhere. This results in f ∗ i [u] = 1\\n\\n2(f A i ∥f B i )[s] + 1\\n\\n2(f A i ∥f B i )[t].\\n\\n4\\n\\n\\n\\n\\n\\nFigure 3: ZipIt! merges models layer-wise by exploiting redundancy in their features. (a) Output features f A and f B from two disjoint layers are (b) paired with other features based on the similarity of their activations. (c) We produce a merge matrix M to combine the pairs into a single shared output feature space, and a corresponding unmerge matrix U that undoes this operation. (d) We then propagate U up the network to align the next layer’s input space, and simultaneously receive the previous layer’s U to align our input space. (e) We apply Eq. 8 to “zip” the layers together using the M for the output and U for the input, producing a single layer (f). We then repeat (a) on the next layer.\\n\\nThus, applying Mi has the effect of interpolating with γ = 1/ 2 but is more general (e.g., allows for merging more than 2 models at once, see Sec. 4.3).\\n\\nWhat about the next layer? After merging features in one layer, we now have the problem that the next layers, LA i+1, LB i+1, are incompatible with f ∗ i . Instead, we need to undo the merge operation before passing the features to the next layer. Thus, we define an “unmerge” matrix Ui ∈R2ni×ni s.t.\\n\\nUif ∗ i ≈f A i ∥f B i (6)\\n\\nUi is the pseudoinverse of Mi and in the case of the matching from earlier is simply 2MiT . Note that strict equality is unlikely here. Like in prior work, merging models is a lossy operation.\\n\\nWe split this unmerge matrix in half along its rows into U A i , U B i ∈Rni×ni that act individually to produce f A i and f B i . With this, we can evaluate the next layers using the merged features:\\n\\nf A i+1 ≈LA i+1(U A i f ∗ i ) f B i+1 ≈LB i+1(U B i f ∗ i ) (7)\\n\\n4.1 THE “ZIP” OPERATION\\n\\nWe now have all the necessary pieces, and can derive a general operation to merge LA i and LB i at an arbitrary point in the network (Fig. 3). First, we compute Mi and Ui by matching features between f A i and f B i . We then pass Ui to the next layer and receive Ui−1 from the previous layer. Using Mi\\n\\nand Ui−1, we “fuse” the merge and unmerge operations into the layer’s parameters. For a linear layer:\\n\\n\\nW ∗ i = M A i W A i U A i−1 + M B i W B i U B i−1 (8)\\n\\nwhere M A i and M B i are Mi split along its columns. b∗ i has the same equation but without unmerging.\\n\\nNote the similarity between Eq. 8 and Eq. 2. This isn’t a coincidence: if we only allowed merging across models and not within models, our “zip” operation would be identical to Git Re-Basin’s permute-then-interpolate approach. Thus, Eq. 8 can be thought of as a generalization of prior work.\\n\\n4.2 ZIP PROPAGATION\\n\\nHowever, most modern neural networks are not simply collections of linear layers stacked on top of each other. In practice, we cannot combine merge and unmerge matrices into every layer of the network, as a local zip (Eq. 8) expects the layer to have a weight matrix—i.e., the layer has to have separate input and output spaces so that we can unmerge the input space and merge the output space. Other layers (e.g., BatchNorm, ReLU) don’t have such a weight matrix.\\n\\n5\\n\\n\\n\\n\\n\\nFigure 4: Zip Propagation. We propagate Mi backward until we hit a layer with weights, merging merging element-wise layers (e.g., BatchNorm) along the way.\\n\\nFigure 5: Partial Zip. (a) If we stop zipping early and (b) apply the latest U from the zip propagation to the inputs of the first unmerged layer in each model, (c) we get a multi-head model with a head for each task.\\n\\nThus, we “propogate” Mi and Ui through these layers. For instance, in Fig. 4, we show a common stack of layers found in a typical ConvNet. Following Jordan et al. (2022), we compute Mi and Ui using the activations of the network (i.e., after each ReLU). We can’t fuse Mi with the ReLU layer, as it doesn’t have any parameters. Similarly, we can merge the parameters of the preceding BatchNorm layer (i.e., in the same way as bias). But it doesn’t have a weight matrix, so we also can’t fuse Mi into it. Only once we’ve reached the Conv layer can we fuse Mi and Ui into it using Eq. 8 (in this case, treating each kernel element as independent).\\n\\nSimilar care needs to be taken with skip connections, as every layer that takes input from or outputs to a skip connection shares the same feature space. However, this too can be dealt with during propagation—we just need to propagate Mi backward and Ui forward to each layer connected by the same skip connection. In general, we can define propagation rules to handle many different types of network modules (see Appendix C).\\n\\n4.3 EXTENSIONS\\n\\nPartial Zip. We don’t always want to zip every layer of the two networks, especially if their output spaces are incompatible, or if doing so would lose too much accuracy. Instead, we can perform a partial zip. That is, we zip most of the layers together, but leave the later ones unzipped (Fig. 5).\\n\\nImplementing this operation is simple in our framework: zip as normal until the specified layer i, then the remaining unzipped layers will receive Ui through zip propagation. If we apply U A i to LA i+1\\n\\nand U B i to LB i+1, the remaining unzipped layers will form “heads” that expect merged features as input. We can then ensemble the heads or choose one to evaluate at runtime.\\n\\nRepeated Matching (α). In some cases, we’d like to merge more than two models together. To do this, we allow “repeated matches”. That is, when two features are matched in our greedy algorithm, they are removed and replaced with the resulting merged feature. To ensure that one feature doesn’t get merged endlessly, we set the correlations of the new feature to be the minimum of the old features’ similarities weighted by α ∈(0, 1]. We find a small value of α typically works best.\\n\\nSame-model Budget (β). To demonstrate the effectiveness of same-model merges, we introduce a “budget” parameter β ∈[0, 1] that denotes what percent of total merged features can come from models merging within themselves, with each model receiving an equal portion of this budget. Note that a budget of 0 results in Eq. 2, as in that case no features can be merged within models.\\n\\n5 RESULTS\\n\\nThere is no standard benchmark to evaluate merging approaches on models from distinct tasks, so we construct our own. We evaluate our approach in two different settings. (1) A versatile test-bed: disjoint category splits of the same dataset (i.e., same dataset and different label sets). (2) A very challenging setting: completely different datasets and tasks (i.e., different datasets and label sets).\\n\\nExperimental Details. For each experiment where we sample multiple disjoint splits of categories, we hold one split out for hyperparameter search and report mean and standard deviation on the rest. For experiments with models trained on different datasets, we subsample the validation set into a validation and test set to use for the same purpose. To compute correlations, we use a portion of the\\n\\n6\\n\\n\\n\\n\\n\\nAccuracies (%) Method FLOPs (G) Joint Task A Task B Avg\\n\\nModel A 0.68 48.2±1.0 97.0±0.6 45.1±8.6 71.0±4.4\\n\\nModel B 0.68 48.4±3.8 49.1±9.3 96.1±1.1 72.6±4.9\\n\\nW. Avg (Eq. 1) 0.68 43.0±1.6 54.1±1.4 67.5±1.2 60.8±4.5\\n\\nGit Re-Basin‡ 0.68 46.2±0.8 76.8±8.9 82.7±5.1 79.8±6.5\\n\\nPermute (Eq. 2) 0.68 58.4±6.8 86.6±2.1 87.4±1.1 87.4±1.4\\n\\nZipIt!20/20 0.68 79.1±1.1 92.9±1.1 91.2±1.4 92.1±1.0\\n\\nEnsemble 1.37 87.4±2.6 97.0±0.6 96.1±1.1 96.6±0.4\\n\\nZipIt!13/20 0.91 83.8±3.1 95.1±0.7 94.1±1.5 94.6±0.6\\n\\n(a) CIFAR-10 (5+5). ResNet-20 (4× width).\\n\\n\\nAccuracies (%) Method FLOPs (G) Joint Task A Task B Avg\\n\\nModel A 2.72 41.6±0.3 82.9±0.7 24.8±0.4 53.9±0.5\\n\\nModel B 2.72 41.6±0.2 25.1±1.2 82.8±0.2 54.0±0.6\\n\\nW. Avg (Eq. 1) 2.72 17.0±1.7 23.8±6.9 24.8±5.9 24.3±1.9\\n\\nGit Re-Basin‡ 2.72 40.9±0.2 57.3±1.5 56.7±0.7 57.0±0.8\\n\\nPermute (Eq. 2) 2.72 42.8±0.7 61.6±1.4 60.5±0.5 61.0±0.8\\n\\nZipIt!20/20 2.72 54.9±0.8 68.2±0.8 67.9±0.6 68.0±0.4\\n\\nEnsemble 5.45 73.5±0.4 82.9±0.7 82.8±0.2 82.8±0.4\\n\\nZipIt!13/20 3.63 70.2±0.4 80.3±0.8 80.1±0.7 80.2±0.6\\n\\n(b) CIFAR-100 (50+50). ResNet-20 (8× width).\\n\\n\\nTable 1: CIFAR Results. ZipIt! vs. baselines on combining a model trained on half the classes (Task A) with one trained on the other half (Task B) without extra training. We report both joint (10/100-way) and per-task (5/50-way) accuracy. ZipIt! significantly outperforms its baseline and closes in on the upper bound (ensemble accuracy). ‡ refers to Ainsworth et al. (2022).\\n\\ntraining set for each dataset as in Li et al. (2015) (see Appendix B). For a fair comparison, we reset the batch norms for all methods (including the original models) using the training data (following the recommendation in Jordan et al. (2022)). For our method, ZipIt!n/m indicates that n out of the m layers in the network have been zipped (Sec. 4.3). Note, all our models have different initializations.\\n\\nEvaluation. For the setting with disjoint class splits of the same dataset, we evaluate performance in two ways: joint accuracy and per task accuracy. For joint accuracy, we evaluate each model over all classes in the combined dataset. For per task accuracy, we compute the accuracy of each task individually (i.e., supposing we had task labels at runtime) and then report the average. The former is similar to a continual learning setting where we want to augment the knowledge of the model, while the latter is akin to a multi-task setting where we know which task we’re using at test time. For the scenario where we merge models trained on different datasets, we use the per task accuracy metric, as the label spaces are not comparable.\\n\\nBaselines. In addition to the default Weight Matching version of Git Re-Basin (Ainsworth et al., 2022), we compare to two baselines: Weight Averaging (Eq. 1) and Permute (Eq. 2) with γ = 1/ 2 using our framework (i.e., we set Mi and Ui such that Eq. 8 is equivalent). For Permute, we use linear sum assignment to find optimal permutations (following Li et al. (2015)). Note that our Permute is a strong baseline we create using our framework and is more accurate than Git Re-Basin in our settings. It’s also similar to REPAIR (Jordan et al., 2022), but without adding extra parameters to the model. Finally, with perfect merging, the merged model’s outputs would be identical to the originals. Thus we include Ensemble as an upper bound (executing and concatenating the results of both models).\\n\\n5.1 CIFAR-10 AND CIFAR-100\\n\\nWe train 5 pairs of ResNet-20 (He et al., 2016) from scratch with different initializations on disjoint halves of the CIFAR-10 and CIFAR-100 classes (Krizhevsky et al., 2009). While ZipIt! supports “partial zipping” to merge models with different outputs (in this case, disjoint label sets), prior methods without retraining do not. To make a fair comparison, we train these CIFAR models with a CLIP-style loss (Radford et al., 2021) using CLIP text encodings of the class names as targets. This way, both models output into the same CLIP-space regardless of the category. Note, this means the models are capable of some amount of zero-shot classification on the tasks they were not trained on.\\n\\nCIFAR-10 (5+5). In Tab. 1a, we merge models trained on disjoint 5 class subsets of CIFAR-10 using ResNet-20 with a 4× width multiplier (denoted as ResNet-20×4). In joint classification (i.e., 10-way), Git Re-Basin is unable to perform better than using either of the original models alone, while our Permute baseline performs slightly better. In stark contrast, our ZipIt! performs a staggering 32.9% better than Git Re-Basin and 20.7% better than our baseline. If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting). We also achieve similar results when merging VGG11 models in this setting (Appendix D).\\n\\nCIFAR-100 (50+50). We find similar results on disjoint 50 class splits of CIFAR-100 in Tab. 1b, this time using an 8× width multiplier instead. Like with CIFAR-10, Git Re-Basin fails to outperform even the unmerged models themselves in joint classification (i.e., 100-way), and this time Permute is only 1.2% ahead. ZipIt! again significantly outperforms prior work with +14% accuracy over Git Re-Basin for all layers zipped, and a substantial +29.2% if zipping 13/20 layers. At this accuracy,\\n\\n7\\n\\n\\n\\n\\n\\nZipIt!13/20 is again only 3.3% behind the ensemble for joint accuracy and 2.6% behind for average per task accuracy, landing itself in an entirely different performance tier compared to prior work.\\n\\n5.2 IMAGENET-1K (200+200)\\n\\nAccuracies (%) Method FLOPs (G) Joint Task A Task B Avg\\n\\nModel A 4.11 37.2±2.0 74.3±4.0 0.5±0.1 37.4±2.0\\n\\nModel B 4.11 35.3±1.6 0.5±0.1 70.5±3.2 35.5±1.6\\n\\nW. Avg (Eq. 1) 4.11 0.3±0.1 0.6±0.1 0.7±0.1 0.6±0.1\\n\\nGit Re-Basin‡ 4.11 3.1±1.2 5.3±2.6 5.7±2.4 5.5±1.7\\n\\nPermute (Eq. 2) 4.11 8.6±5.8 10.1±4.4 15.3±11.1 12.7±7.7\\n\\nZipIt!50/50 4.11 8.6±4.7 12.4±5.9 14.7±7.8 13.5±6.6\\n\\nEnsemble 8.22 63.3±4.9 74.3±4.0 70.5±3.2 72.4±2.5\\n\\nZipIt!22/50 6.39 55.8±4.1 65.9±2.5 64.1±3.0 65.0±2.3\\n\\nZipIt!10/50 7.43 60.9±4.1 70.7±3.0 69.0±2.9 69.9±1.9\\n\\nTable 2: ImageNet-1k (200+200) Results. Merging ResNet-50 models trained from scratch on disjoint 200 category subsets (Task A and B) of ImageNet-1k. Prior work performs poorly, but ZipIt! makes this task feasible. ‡Ainsworth et al. (2022).\\n\\nTo test our method on the much harder setting of large-scale data, we train 5 differently initialized ResNet-50 models with cross entropy loss on disjoint 200 class subsets of ImageNet-1k (Deng et al., 2009). To compare to prior work that doesn’t support partial zipping, we initialize the models with capacity for all 1k classes, but only train each on their subset.\\n\\nIn Tab. 2 we show results on exhaustively merging pairs from the 5 models. To compute joint (i.e., 400-way) accuracy, we softmax over each task’s classes individually (like in Ahn et al. (2021)), and take the argmax over the combined 400 class vector. On this extremely difficult task, Git Re-Basin only obtains 3.1% for joint accuracy (with random accuracy being 0.25%). Both the Permute baseline and ZipIt! with all layers zipped perform better, but with each at 8.6%, are still clearly lacking. Note that we find the same-model merging budget β to not matter for this set of models (see Fig. 6), which suggests that there’s not a lot of redundant information within each model in this setting. Thus, ZipIt! chooses to merge mostly across models instead, performing similarly to the permute baseline. We find this same trend in CIFAR with smaller models (see Fig. 6), and may be an artifact of model capacity. The story changes when we increase the capacity of the merged model by partial zipping: ZipIt!10/50 reaches close to upper bound ensemble accuracy on this extremely difficult task, while saving on FLOPs.\\n\\n5.3 MULTI-DATASET MERGING\\n\\nWe now take our model merging framework one step further by merging differently initialized models trained on completely separate datasets and tasks. We present two settings: merging multiple classification datasets and merging semantic segmentation with image classification.\\n\\nPer-Task Accuracies (%) Method FLOPs (G) SD OP CUB NAB Avg\\n\\nMerging Pairs\\n\\n\\nW. Avg (Eq. 1) 4.11 12.9 18.2 13.9 0.2 11.3 Permute (Eq. 2) 4.11 46.2 47.6 35.6 13.5 35.7\\n\\nZipIt!49/50 4.11 46.9 50.7 38.0 12.7 37.1\\n\\nEnsemble 8.22 72.7 81.1 71.0 77.2 75.5\\n\\nZipIt!22/50 6.39 62.6 71.2 62.8 53.0 62.4\\n\\nZipIt!10/50 7.42 66.5 75.8 65.6 66.8 68.7\\n\\nMerging All 4\\n\\n\\nW. Avg (Eq. 1) 4.12 0.8 3.0 0.6 0.3 1.2 Permute (Eq. 2) 4.12 15.7 26.1 14.0 5.3 15.3\\n\\nZipIt!49/50 4.12 21.1 33.3 8.6 3.9 16.8\\n\\nEnsemble 16.4 72.7 81.2 71.0 77.2 75.5\\n\\nZipIt!22/50 11.0 50.2 55.9 44.0 32.0 45.5\\n\\nZipIt!10/50 14.1 63.5 70.8 63.7 63.1 65.3\\n\\nTable 3: Multi-Dataset Results. Merging ResNet-50 models trained on completely different datasets: Stanford Dogs (SD), Oxford Pets (OP), CUB200 (CUB), and NABirds (NAB). We report average per-task accuracy over merging model pairs, and all four.\\n\\nImage Classification Datasets. Merging ResNet50 models trained on: Stanford Dogs (Khosla et al., 2011), Oxford Pets (Parkhi et al., 2012), CUB200 (Welinder et al., 2010), and NABirds (Van Horn et al., 2015). In Tab. 3, we show the average per task accuracy from exhaustively merging each pair and the much more difficult setting of merging all four at once. We report the accuracy of our baselines by applying them up until the last layer, but we can’t compare to prior work as they don’t support this setting. As in all our previous experiment we merge without retraining.\\n\\nFor pairs of models, ZipIt! slightly outperforms our permute baseline across all tasks and performs similarly when merging all 4 models at once. However, if we add capacity to the merged model through partial zipping, we perform up to 33% better on merging pairs and 50% better on merging all four models than the permute baseline. Partial zipping is a significant factor to obtain strong performance, especially with more than 2 models.\\n\\nMultiple Output Modalities. In Appendix F, we combine across modalities by merging the ResNet-50 backbone of a DeeplabV3 (Chen et al., 2017) segmentation model with an ImageNet-1k classification model. The resulting combined model can perform both semantic segmentation and image classification. Even with half of layers merged, ZipIt! retains good performance on both tasks.\\n\\n8\\n\\n\\n\\n\\n\\nFigure 6: Varying β and Model Scale. Left: We find when the model has enough capacity for the task, a high budget (Sec. 4.3) improves performance. Right: ZipIt! makes effective use of extra model capacity to quickly reach the ensemble on CIFAR-100 (50+50) when we increase the width of ResNet-20 models. In contrast, our baselines only slightly benefit from the extra scale.\\n\\n6 ANALYSIS\\n\\nMerging within Models. A critical piece of ZipIt! compared to prior work is the ability to merge within models, not just across models. In Sec. 4.3, we introduce a budget parameter β to limit the number of same-model merges, and here use CIFAR-100 (50+50) and ImageNet-1k (200+200) to illustrate its effectiveness (Fig. 6a). On CIFAR, same-model merges are very important, with the optimal budget being above 0.8, meaning 80% of merges are allowed to be within the same model. This is not the case, however, on ImageNet, where the difficulty of the task means there likely are much fewer redundant features within each model.\\n\\nModel Scale. In Fig. 6b, we test the effect of model scale directly by evaluating joint accuracy on our CIFAR-100 (50+50) setting with ResNet-20 models of increasing width. Here, we explicitly see that when the width of the models are too small for the task (e.g., < 4×), ZipIt! and the Permute baseline perform identically (though both much better than Git Re-Basin). However, when the scale increases, ZipIt! trends toward the ensemble upper bound of 75%, while both the Permute baseline and Git Re-Basin plateau at around 45%. This corroborates Eq. 4 and indicates our method uses the extra model capacity effectively, much better than prior work.\\n\\nAlgorithm A↔A/B↔B? Acc Time\\n\\nIdentity (Eq. 1) ✗ 43.0±3.1 1.8 ms Permute (Eq. 2) ✗ 58.4±1.3 28 ms K-Means ✓ 29.1±5.5 19 sec\\n\\nZip (Eq. 8) Optimal Match ✓ 79.6±1.7 11 min Greedy Match ✓ 79.0±1.8 1.1 sec Greedy, α=0.1 ✓ 79.1±2.1 1.2 sec\\n\\nTable 4: Comparing Matching Algorithms to use for Mi on CIFAR-10 (5+5) joint 10-way accuracy. Permuting B→A as in prior work (Eq. 2) performs poorly. We significantly improve by merging features within each model (Eq. 8). Our greedy approach is nearly as accurate as the optimal algorithm while being two orders of magnitude faster.\\n\\nMatching Algorithm. In Tab. 4, we compare matching algorithms used to compute Mi in Eq. 8. Using either the identity (weight averaging) or a permutation (as in prior work) underperforms on CIFAR-10 (5+5) joint 10-way classification. In contrast, we obtain up to 21.2% higher accuracy if we allow both permutations and merging within models. However, doing this optimally is difficult, as the standard linear sum assignment algorithm assumes bipartite matches. We could use a optimal graph-based solver (e.g., Hagberg et al. (2008)) instead, but doing so is prohibitively slow (11 minutes to transform a ResNet-20×4 model). Thus, we find matches greedily by repeatedly taking the most correlated pair of features without replacement. This performs almost as well, and is multiple orders of magnitude faster. If we allow repeated matches (Sec. 4.3), we obtain a slightly better result. Like Bolya et al. (2023), we find that matching is better for merging features than clustering (K-Means).\\n\\n7 CONCLUSION\\n\\nIn this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks without additional training. We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities. We introduce ZipIt!, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings, comprehensively analyzing each.\\n\\n9\\n\\n\\n\\n\\n\\nReproducibility Statement. To ensure reproducibility, we will release code for our algorithm, experiments, and baselines. We also include algorithm details in Section 4 and further in Appendix C, experimental details in Section 5 and Appendix B, and a proof of our Theorem 1 in Appendix G.\\n\\nAcknowledgements. This work was supported in part by funding from NSF CAREER #2144194, ARL, Google, and NSF GRFP. All views and conclusions expressed in this work are those of the authors and not a reflection of these sources.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '## Reviewer\\n\\n### Summary\\n\\nThis paper proposes a novel method for merging two different models trained from different tasks, maintaining the performance on both tasks. The method resolves the problem of decreasing performance on two disjoint tasks, where prior works fail, \\ufeffwhich \\ufeffpermutes the model to another one and the average of them. Moreover, this paper proposed partially zipping to get the trade-off between the performance and the computational complexity.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n2 fair\\n\\n### Contribution\\n\\n2 fair\\n\\n### Strengths\\n\\nThis paper proposes a novel method named “ZipIt!” which can maintain the performance after merging two models trained from two disjoint tasks, which prior works fail. The partially zipping method can further give the option to do a trade-off between the performance and the FLOPs. Finally, a theoretical proof is given to ensure the existence of a transformation matrix.\\n\\n### Weaknesses\\n\\nThe method seems not to be practical for real-world usage, while lacks insights and theoretical analysis of the model properties. The method works fine in the CIFAR dataset, but doesn’t have remarkable advantages in large datasets such as ImageNet, and disjoint tasks. Moreover, the paper lacks comparisons between its method and the model trained on both datasets directly, which limits its usage scenario.\\n\\n### Questions\\n\\n- I’ve noticed that the “permute” baseline is stronger than “Git Re-Basin”, which is a little strange. I’m wondering how it happens and what’s the experimental setting?\\n- The improvement on CIFAR is remarkable, while in the larger dataset ImageNet, the trade-off between the performance and FLOPs is debatable. Compared to the ensemble, the FLOPs don’t decrease a lot while getting lower accuracy.\\n- I’m curious about the comparison between “ZipIt!” method and a single model trained on both datasets. Did you think about this?\\n- The presentation for “Ensemble” results which uses light grey is a little misleading.\\n- Are there the results of Git Re-Basin method for \\ufeffMulti-Dataset in Table 3?\\n- Did you consider the time cost of computing transformation matrices in your method when comparing the FLOPs?\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n5: marginally below the acceptance threshold\\n\\n### Confidence\\n\\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\\n\\n**********\\n\\n## Reviewer\\n\\n### Summary\\n\\nThe paper proposes a method to merge two different models sharing the same architecture, possibly with different initialization and for different tasks, without re-training. The key idea is to merge the weights whose outputs are highly correlated, both within and across models. The method also allows weights to be partially merged, adding flexibility to adjust the amount of merging depending on resource budget and accuracy requirements. Experiments show that ZipIt! outperforms the existing baselines in several settings, including merging classification models and merging across tasks.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n4 excellent\\n\\n### Contribution\\n\\n3 good\\n\\n### Strengths\\n\\n1. This paper studies a new and interesting problem: merging differently initialized models trained on different tasks into a single model without retraining. This relaxes the condition of the model merging problem, recently studied in works such as Model Soups and Git Re-Basin, which assumes the same task over models. This makes the problem more challenging while allowing for broader use cases.\\n\\n2. The idea of merging weights both within and across models is simple, but achieves reasonably good performance under different conditions in the experiments. It\\'s great that experiments with merging tasks of different modalities (classification, segmentation) are also done and the method still achieves reasonable performances, showing that the method is generalizable to non-classification tasks. In addition, the ability to flexibly adjust the amount of merging is also a strong point.\\n\\n3. The paper is well written. It was easy to follow. It is also great that the code is already publicly available.\\n\\n### Weaknesses\\n\\n1. My main concern is the stability of the method. I am not sure if the method works robustly across different settings, because empirical observations are reported in the github issues that a small change (e.g. changing the learning rate and epoch numbers when training the models to merge) can cause the merged model to crash to low accuracy.\\n\\n2. These are not weaknesses but there are some limitations. The performance drop increase when merging multiple models. Accuracy drop looks large when merging models of different tasks (Table 8). All experiments used CNNs and transformers are not tested.\\n\\n### Questions\\n\\n1. The meaning of \"ensemble\" in the experiments is unclear to me. I don\\'t think I understood how it works.\\n\\n2. I would appreciate more experiments to test the stability of the method. It would also be nice to provide an analysis of when the method works and when it does not. Please see the weakness section for more details.\\n\\n3. It would also be interesting to include additional experiments on more compact architectures such as Mobilenet for a more comprehensive analysis of the behavior of the method. It is great that ZipIt! is already evaluated on models with larger widths (x1.5 in Table 7). I think it might be harder to merge models where the architectures are already compact because there might be less overlap and less redundancy of features.\\n\\n4. Can BN be removed before applying ZipIt by fusing it into Conv (using that BN is an affine transform when mean and variance are fixed)? I think then the process to reset BN after merge can be avoided.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\\n\\n**********\\n\\n## Reviewer\\n\\n### Summary\\n\\nThis paper proposes a method for merging trained models trained on different datasets without additional training based on feature correlations. The proposed method can further adjust the inference cost and accuracy by adjusting the number of layers to be merged.\\n\\n### Soundness\\n\\n2 fair\\n\\n### Presentation\\n\\n3 good\\n\\n### Contribution\\n\\n3 good\\n\\n### Strengths\\n\\n1. This paper addresses the interesting research topic of merging models on different datasets without additional training.\\n2. Large-scale experiments such as imagenet1k confirm the effectiveness of the proposed method. This paper conducts experiments in multi-dataset merging, classification and segmentation merging. In particular, this is the first effort, to my knowledge, to merge the different modalities of classification and segmentation.\\n3. For reproducibility, the authors mentioned that they plan to publish the source code; the cost of replication experiments would go down significantly if the checkpoints of the trained models were made public since they involve large scale experiments such as cifar100, imagenet1k, and multidataset.\\n\\n### Weaknesses\\n\\n1. The paper extends feature matching between models to within the model in order to merging models across different datasets. Its contribution should be examined in more detail. In particular, there is no theoretical support as to why matching the feature would lower the loss after merging. I did not understand the connection between Theorem 1 and the fact that joint Acc can be sufficiently large for merging models on different datasets. Is the extension to \"within merge\" not only more flexible in alignment, but also suitable for merging models on different datasets?\\n2. The proposed method is inaccurate for meging of imagenet1k and multi dataset unless partial zip is used, while the inference cost increases as the number of models increases with partial zip.\\n\\n### Questions\\n\\n1. Is git-rebasin activation matching considered equivalent to zipit without wighin merge? I would like to know the difference between git rebasin, permute and zipit.\\n 2. Does merge by zipit keep the output strictly invariant like permutation, even when using pseudoinverse matrix?\\n 3. What is the same model budget and explain how beta is introduced as an equation?\\n 4. Three matching methods were proposed in git-rebasin: activation matching, weight matching, and STE. Which method is the baseline in Table 1 and 2? I doubt the claim that permutation alone makes it difficult to merge models on different datasets. According to [1], using STE, which is a permutation base, and Merge models with cifar10(5+5), it is reported that 90% accuracy is achieved, which is 10% more accurate than zipit. I think the conditions are the same as zipit!20/20 in terms of combining all layers.\\n 5. Sec 5.1 says \" `If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting)`\", but how can ensemble be guaranteed to be uppper bound?  In [1], a merging model is reported that can achieve higher accuracy than an ensemble. The upper limit of joint accuracy is the accuracy of the model trained on all data, rather than the accuracy of the ensemble.\\n 6. Why is partially zipping more accurate than fully zipping?\\n 7. How much gpu is needed for Imagenet-1k (200+200)? I am concerned about the memory and computational cost required to compute the correlation matrix.  \\n[1]: https://arxiv.org/abs/2306.05641\\n\\n\\n\\n\\n===========================================\\n\\nComments after reading the rebuttal.\\n\\n===========================================\\n\\nSince I was unable to set the readers to everyone in the reply, I am writing my comments here.\\n\\nThank you for your kind reply.\\nI did not understand that merging models using loss like STE is outside the scope of this study.\\nSince this has been resolved in regards to many of my questions, I would like to raise my score.\\n\\nAs other reviwers have pointed out, it would be helpful to specify as a limitation in the camera-ready version of the paper that the model must be wide enough to perform.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\\n\\n**********\\n\\n## Reviewer\\n\\n### Summary\\n\\nThis paper tackles the problem of merging two models trained on different \"tasks\". Their best performing approach is two-fold: 1) identify redundant features within/across models and average those, and 2) only merge some layers. Combined they achieve performance which approaches the ensemble, but their improvements are most notable when the two \"tasks\" come from the same task, such as partitioning CIFAR in two parts.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n4 excellent\\n\\n### Contribution\\n\\n2 fair\\n\\n### Strengths\\n\\n- The presentation, including all Figures, is very clear!\\n- The idea of merging redundant features is interesting and effective. It seems to me that the authors should consider exploring this as a general way of speeding up inference and not just as a technique for merging models.\\n\\n### Weaknesses\\n\\n- I would appreciate more motivation for this problem. When would I find myself training from scratch on two different tasks?\\n- [Subjective] I suggest the authors are much more clear/transparent about their contributions early, because currently the readers get their hopes up a lot when in reality the best performance occurs either when the \"tasks\" really come from the same task like splitting CIFAR in half. I think there\\'s still a long way to go for training individually on two distinct datasets that are ImageNet level difficulty.\\n- Figure 1a/b is not exactly clear with respect to related work, for instance [1] (Sec 3.3), and [2] (Appendix E) do consider merging models trained on different tasks, albeit from the same initialization.\\n\\n[1] Merging Models with Fisher-Weighted Averaging (https://arxiv.org/abs/2111.09832)\\n[2] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time (https://arxiv.org/abs/2203.05482)\\n\\n### Questions\\n\\n- I would be curious when the authors believe this approach is best applied, e.g., when would I find myself training from scratch on two different tasks?\\n- Does this approach improve accuracy in more traditional model merging settings? It\\'s pitched for multi-task but is it more general?\\n- The \"scaling\" trend in Figure 6b is very interesting! I would be curious to hear if the authors would expect the same trend with a different method to increase FLOPs on the x-axis, such as depth scaling, or joint scaling, or training iterations.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\\n\\n**********\\n\\n## Meta Review\\n\\nThis paper tackles model merging in the challenging setting where the models-to-be-merged do not necessarily share an initialization but do share an architecture. There are two primary innovations: The first is to find the superset of features across the two models and ensure all of them remain in the merged model, and the second is to only merge the model up to a certain layer and keep the rest of the layers fixed. Both of these operations are \"relaxations\" of the classical merging setup in the sense that merging typically involves creating a single model that is no larger than the constituent models. This paper, however, provides compelling arguments that this is beneficial. It also builds naturally on merging methods that aim to account for permutation symmetries. Ultimately, on the task of merging vision models, the proposed framework improves performance drastically.\\n\\n### justification_for_why_not_higher_score\\n\\nWhile the gains in this paper are impressive, a major source of these gains is a relaxation of the merging problem where the merged model can have more parameters/be more computationally expensive than the constituent models. This paper proposes a good solution to this new problem setting, and I think it will motivate future work in the area.\\n\\n### justification_for_why_not_lower_score\\n\\nConsensus was generally to accept.\\n\\n**********\\n\\n## Paper Decision\\n\\nAccept (poster)'}],\n",
       " 'rates': [6, 5, 6, 6],\n",
       " 'review_contexts': [{'content': '## Reviewer\\n\\n### Summary\\n\\nThe paper proposes a method to merge two different models sharing the same architecture, possibly with different initialization and for different tasks, without re-training. The key idea is to merge the weights whose outputs are highly correlated, both within and across models. The method also allows weights to be partially merged, adding flexibility to adjust the amount of merging depending on resource budget and accuracy requirements. Experiments show that ZipIt! outperforms the existing baselines in several settings, including merging classification models and merging across tasks.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n4 excellent\\n\\n### Contribution\\n\\n3 good\\n\\n### Strengths\\n\\n1. This paper studies a new and interesting problem: merging differently initialized models trained on different tasks into a single model without retraining. This relaxes the condition of the model merging problem, recently studied in works such as Model Soups and Git Re-Basin, which assumes the same task over models. This makes the problem more challenging while allowing for broader use cases.\\n\\n2. The idea of merging weights both within and across models is simple, but achieves reasonably good performance under different conditions in the experiments. It\\'s great that experiments with merging tasks of different modalities (classification, segmentation) are also done and the method still achieves reasonable performances, showing that the method is generalizable to non-classification tasks. In addition, the ability to flexibly adjust the amount of merging is also a strong point.\\n\\n3. The paper is well written. It was easy to follow. It is also great that the code is already publicly available.\\n\\n### Weaknesses\\n\\n1. My main concern is the stability of the method. I am not sure if the method works robustly across different settings, because empirical observations are reported in the github issues that a small change (e.g. changing the learning rate and epoch numbers when training the models to merge) can cause the merged model to crash to low accuracy.\\n\\n2. These are not weaknesses but there are some limitations. The performance drop increase when merging multiple models. Accuracy drop looks large when merging models of different tasks (Table 8). All experiments used CNNs and transformers are not tested.\\n\\n### Questions\\n\\n1. The meaning of \"ensemble\" in the experiments is unclear to me. I don\\'t think I understood how it works.\\n\\n2. I would appreciate more experiments to test the stability of the method. It would also be nice to provide an analysis of when the method works and when it does not. Please see the weakness section for more details.\\n\\n3. It would also be interesting to include additional experiments on more compact architectures such as Mobilenet for a more comprehensive analysis of the behavior of the method. It is great that ZipIt! is already evaluated on models with larger widths (x1.5 in Table 7). I think it might be harder to merge models where the architectures are already compact because there might be less overlap and less redundancy of features.\\n\\n4. Can BN be removed before applying ZipIt by fusing it into Conv (using that BN is an affine transform when mean and variance are fixed)? I think then the process to reset BN after merge can be avoided.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "   'rate': '6'},\n",
       "  {'content': '## Reviewer\\n\\n### Summary\\n\\nThis paper proposes a novel method for merging two different models trained from different tasks, maintaining the performance on both tasks. The method resolves the problem of decreasing performance on two disjoint tasks, where prior works fail, \\ufeffwhich \\ufeffpermutes the model to another one and the average of them. Moreover, this paper proposed partially zipping to get the trade-off between the performance and the computational complexity.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n2 fair\\n\\n### Contribution\\n\\n2 fair\\n\\n### Strengths\\n\\nThis paper proposes a novel method named “ZipIt!” which can maintain the performance after merging two models trained from two disjoint tasks, which prior works fail. The partially zipping method can further give the option to do a trade-off between the performance and the FLOPs. Finally, a theoretical proof is given to ensure the existence of a transformation matrix.\\n\\n### Weaknesses\\n\\nThe method seems not to be practical for real-world usage, while lacks insights and theoretical analysis of the model properties. The method works fine in the CIFAR dataset, but doesn’t have remarkable advantages in large datasets such as ImageNet, and disjoint tasks. Moreover, the paper lacks comparisons between its method and the model trained on both datasets directly, which limits its usage scenario.\\n\\n### Questions\\n\\n- I’ve noticed that the “permute” baseline is stronger than “Git Re-Basin”, which is a little strange. I’m wondering how it happens and what’s the experimental setting?\\n- The improvement on CIFAR is remarkable, while in the larger dataset ImageNet, the trade-off between the performance and FLOPs is debatable. Compared to the ensemble, the FLOPs don’t decrease a lot while getting lower accuracy.\\n- I’m curious about the comparison between “ZipIt!” method and a single model trained on both datasets. Did you think about this?\\n- The presentation for “Ensemble” results which uses light grey is a little misleading.\\n- Are there the results of Git Re-Basin method for \\ufeffMulti-Dataset in Table 3?\\n- Did you consider the time cost of computing transformation matrices in your method when comparing the FLOPs?\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n5: marginally below the acceptance threshold\\n\\n### Confidence\\n\\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "   'rate': '5'},\n",
       "  {'content': '## Reviewer\\n\\n### Summary\\n\\nThis paper proposes a method for merging trained models trained on different datasets without additional training based on feature correlations. The proposed method can further adjust the inference cost and accuracy by adjusting the number of layers to be merged.\\n\\n### Soundness\\n\\n2 fair\\n\\n### Presentation\\n\\n3 good\\n\\n### Contribution\\n\\n3 good\\n\\n### Strengths\\n\\n1. This paper addresses the interesting research topic of merging models on different datasets without additional training.\\n2. Large-scale experiments such as imagenet1k confirm the effectiveness of the proposed method. This paper conducts experiments in multi-dataset merging, classification and segmentation merging. In particular, this is the first effort, to my knowledge, to merge the different modalities of classification and segmentation.\\n3. For reproducibility, the authors mentioned that they plan to publish the source code; the cost of replication experiments would go down significantly if the checkpoints of the trained models were made public since they involve large scale experiments such as cifar100, imagenet1k, and multidataset.\\n\\n### Weaknesses\\n\\n1. The paper extends feature matching between models to within the model in order to merging models across different datasets. Its contribution should be examined in more detail. In particular, there is no theoretical support as to why matching the feature would lower the loss after merging. I did not understand the connection between Theorem 1 and the fact that joint Acc can be sufficiently large for merging models on different datasets. Is the extension to \"within merge\" not only more flexible in alignment, but also suitable for merging models on different datasets?\\n2. The proposed method is inaccurate for meging of imagenet1k and multi dataset unless partial zip is used, while the inference cost increases as the number of models increases with partial zip.\\n\\n### Questions\\n\\n1. Is git-rebasin activation matching considered equivalent to zipit without wighin merge? I would like to know the difference between git rebasin, permute and zipit.\\n 2. Does merge by zipit keep the output strictly invariant like permutation, even when using pseudoinverse matrix?\\n 3. What is the same model budget and explain how beta is introduced as an equation?\\n 4. Three matching methods were proposed in git-rebasin: activation matching, weight matching, and STE. Which method is the baseline in Table 1 and 2? I doubt the claim that permutation alone makes it difficult to merge models on different datasets. According to [1], using STE, which is a permutation base, and Merge models with cifar10(5+5), it is reported that 90% accuracy is achieved, which is 10% more accurate than zipit. I think the conditions are the same as zipit!20/20 in terms of combining all layers.\\n 5. Sec 5.1 says \" `If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting)`\", but how can ensemble be guaranteed to be uppper bound?  In [1], a merging model is reported that can achieve higher accuracy than an ensemble. The upper limit of joint accuracy is the accuracy of the model trained on all data, rather than the accuracy of the ensemble.\\n 6. Why is partially zipping more accurate than fully zipping?\\n 7. How much gpu is needed for Imagenet-1k (200+200)? I am concerned about the memory and computational cost required to compute the correlation matrix.  \\n[1]: https://arxiv.org/abs/2306.05641\\n\\n\\n\\n\\n===========================================\\n\\nComments after reading the rebuttal.\\n\\n===========================================\\n\\nSince I was unable to set the readers to everyone in the reply, I am writing my comments here.\\n\\nThank you for your kind reply.\\nI did not understand that merging models using loss like STE is outside the scope of this study.\\nSince this has been resolved in regards to many of my questions, I would like to raise my score.\\n\\nAs other reviwers have pointed out, it would be helpful to specify as a limitation in the camera-ready version of the paper that the model must be wide enough to perform.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "   'rate': '6'},\n",
       "  {'content': '## Reviewer\\n\\n### Summary\\n\\nThis paper tackles the problem of merging two models trained on different \"tasks\". Their best performing approach is two-fold: 1) identify redundant features within/across models and average those, and 2) only merge some layers. Combined they achieve performance which approaches the ensemble, but their improvements are most notable when the two \"tasks\" come from the same task, such as partitioning CIFAR in two parts.\\n\\n### Soundness\\n\\n3 good\\n\\n### Presentation\\n\\n4 excellent\\n\\n### Contribution\\n\\n2 fair\\n\\n### Strengths\\n\\n- The presentation, including all Figures, is very clear!\\n- The idea of merging redundant features is interesting and effective. It seems to me that the authors should consider exploring this as a general way of speeding up inference and not just as a technique for merging models.\\n\\n### Weaknesses\\n\\n- I would appreciate more motivation for this problem. When would I find myself training from scratch on two different tasks?\\n- [Subjective] I suggest the authors are much more clear/transparent about their contributions early, because currently the readers get their hopes up a lot when in reality the best performance occurs either when the \"tasks\" really come from the same task like splitting CIFAR in half. I think there\\'s still a long way to go for training individually on two distinct datasets that are ImageNet level difficulty.\\n- Figure 1a/b is not exactly clear with respect to related work, for instance [1] (Sec 3.3), and [2] (Appendix E) do consider merging models trained on different tasks, albeit from the same initialization.\\n\\n[1] Merging Models with Fisher-Weighted Averaging (https://arxiv.org/abs/2111.09832)\\n[2] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time (https://arxiv.org/abs/2203.05482)\\n\\n### Questions\\n\\n- I would be curious when the authors believe this approach is best applied, e.g., when would I find myself training from scratch on two different tasks?\\n- Does this approach improve accuracy in more traditional model merging settings? It\\'s pitched for multi-task but is it more general?\\n- The \"scaling\" trend in Figure 6b is very interesting! I would be curious to hear if the authors would expect the same trend with a different method to increase FLOPs on the x-axis, such as depth scaling, or joint scaling, or training iterations.\\n\\n### Flag For Ethics Review\\n\\nNo ethics review needed.\\n\\n### Rating\\n\\n6: marginally above the acceptance threshold\\n\\n### Confidence\\n\\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "   'rate': '6'}],\n",
       " 'decision': '## Paper Decision\\n\\nAccept (poster)',\n",
       " 'title': 'ZipIt! Merging Models from Different Tasks without Training',\n",
       " 'id': 'LEYUkvdUhq'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999f842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def get_300_sampled_papers():\n",
    "    papers_path = os.path.join(\"review-5k-dataset\", \"300\")\n",
    "    sampled_300_papers = os.listdir(papers_path)\n",
    "    if \".DS_Store\" in sampled_300_papers:\n",
    "        sampled_300_papers.remove(\".DS_Store\")\n",
    "    return sampled_300_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0f47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = get_300_sampled_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4806207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"300_sampled_paper_ids.txt\", \"w\") as file:\n",
    "    for p in papers:\n",
    "        name = p.split(\".\")[0]\n",
    "        file.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349867d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reviewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
