{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6da4900",
   "metadata": {},
   "source": [
    "# Generate results using the ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219bcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "from keys import (\n",
    "    DEPLOYMENT_GPT_4o_MINI,\n",
    "    DEPLOYMENT_GPT_4o\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930348c",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1014489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_ground_truths():\n",
    "    ground_truth_path = os.path.join(\"review-5k-dataset\", \"300\")\n",
    "    ground_truth_files = os.listdir(ground_truth_path)\n",
    "    if \".DS_Store\" in ground_truth_files:\n",
    "        ground_truth_files.remove(\".DS_Store\")\n",
    "\n",
    "    ground_truths = {}\n",
    "    for file in ground_truth_files:\n",
    "        file_path = os.path.join(\"review-5k-dataset\", \"300\", file)\n",
    "        json_data = load_json_file(file_path)\n",
    "        decision = json_data[\"decision\"]\n",
    "        if \"accept\" in decision.lower():\n",
    "            decision = \"accept\"\n",
    "        else:\n",
    "            decision = \"reject\"\n",
    "        \n",
    "        ground_truths[file] = {\n",
    "            \"rates\": json_data[\"rates\"],\n",
    "            \"decision\": decision\n",
    "            }\n",
    "        \n",
    "    return ground_truths\n",
    "\n",
    "def load_predictions(deloyment):\n",
    "    predictions_path = os.path.join(\"LLM_Responses_Parsed\", deloyment)\n",
    "    prediction_files = os.listdir(predictions_path)\n",
    "    if \".DS_Store\" in prediction_files:\n",
    "        prediction_files.remove(\".DS_Store\")\n",
    "\n",
    "    predictions = {}\n",
    "    for file in prediction_files:\n",
    "        file_path = os.path.join(\"LLM_Responses_Parsed\", deloyment, file)\n",
    "        json_data = load_json_file(file_path)\n",
    "        reviews = json_data[\"reviews\"]\n",
    "        decision = json_data[\"decision\"]\n",
    "        ratings = [item['rating'] for item in reviews]\n",
    "        predictions[file] = {\n",
    "            \"rates\": ratings,\n",
    "            \"decision\": decision\n",
    "        }\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb46dd",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb130d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reviewer_n_minus_1_mae_mse_r2(ground_truth, use_all_possible_permutations=True):\n",
    "\n",
    "    # Step-1: prepare true scores and predicted scores\n",
    "    # True score is the ith rating\n",
    "    # Predicted score is the mean of all ratings except ith rating\n",
    "    true_scores = []\n",
    "    predicted_scores = []\n",
    "    \n",
    "    for paper_id, paper_dict in ground_truth.items():\n",
    "        ratings = paper_dict[\"rates\"]\n",
    "        n = len(ratings)\n",
    "\n",
    "        if use_all_possible_permutations:\n",
    "\n",
    "            for i in range(n):\n",
    "                true_rating = ratings[i]\n",
    "                other_ratings = [ratings[j] for j in range(n) if j != i]\n",
    "                pred_rating = np.mean(other_ratings)\n",
    "\n",
    "                true_scores.append(true_rating)\n",
    "                predicted_scores.append(pred_rating)\n",
    "        else:\n",
    "            # Randomly pick one index i from the n reviews\n",
    "            i = np.random.choice(n)\n",
    "            \n",
    "            true_rating = ratings[i]\n",
    "            other_ratings = [ratings[j] for j in range(n) if j != i]\n",
    "            pred_rating = np.mean(other_ratings)\n",
    "\n",
    "            true_scores.append(true_rating)\n",
    "            predicted_scores.append(pred_rating)\n",
    "\n",
    "    # Step-2: compute MAE,  MSE and R2\n",
    "    mae = round(mean_absolute_error(true_scores, predicted_scores), 2)\n",
    "    mse = round(mean_squared_error(true_scores, predicted_scores), 2)\n",
    "    r2 = round(r2_score(true_scores, predicted_scores), 2)\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "def compute_llm_reviewer_n_mae_mse_r2(ground_truth, predictions):\n",
    "\n",
    "    # Step-1: Preapre true scores and prediction scores. We use mean of the 4 ratings for each paper.\n",
    "    true_scores = []\n",
    "    pred_scores = []\n",
    "    for paper_id, paper_dict in ground_truth.items():\n",
    "        mean_human_rating = np.mean(ground_truth[paper_id][\"rates\"])\n",
    "        mean_llm_rating = np.mean(predictions[paper_id][\"rates\"])\n",
    "        true_scores.append(mean_human_rating)\n",
    "        pred_scores.append(mean_llm_rating)\n",
    "\n",
    "    # Step-2: compute MAE, MSE and R2\n",
    "    mae = round(mean_absolute_error(true_scores, pred_scores), 2)\n",
    "    mse = round(mean_squared_error(true_scores, pred_scores), 2)\n",
    "    r2 = round(r2_score(true_scores, pred_scores), 2)\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "def compute_accuracy_and_f1(ground_truth, predictions):\n",
    "    \n",
    "    # Step-1: Prepare true decisions and predicted decisions\n",
    "    true_decisions = []\n",
    "    pred_decisions = []\n",
    "    for paper_id in ground_truth:\n",
    "        true_decisions.append(ground_truth[paper_id][\"decision\"])\n",
    "        pred_decisions.append(predictions[paper_id][\"decision\"])\n",
    "\n",
    "    # Step-2: Compute Accuracy and Macro F1 as percentages\n",
    "    acc = round(accuracy_score(true_decisions, pred_decisions) * 100, 2)\n",
    "    f1 = round(f1_score(true_decisions, pred_decisions, average='macro') * 100, 2)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23b117",
   "metadata": {},
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432d1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = load_ground_truths()\n",
    "predictions_4o = load_predictions(DEPLOYMENT_GPT_4o)\n",
    "predictions_4o_mini = load_predictions(DEPLOYMENT_GPT_4o_MINI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a115f4",
   "metadata": {},
   "source": [
    "#### Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddda546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-1 MAE, MSE, R2\n",
    "expert = compute_reviewer_n_minus_1_mae_mse_r2(ground_truths, use_all_possible_permutations=False)\n",
    "n_minus_1_4o = compute_reviewer_n_minus_1_mae_mse_r2(predictions_4o, use_all_possible_permutations=False)\n",
    "n_minus_1_4o_mini = compute_reviewer_n_minus_1_mae_mse_r2(predictions_4o_mini, use_all_possible_permutations=False)\n",
    "\n",
    "# n MAE, MSE, R2\n",
    "n_4o = compute_llm_reviewer_n_mae_mse_r2(ground_truths, predictions_4o)\n",
    "n_4o_mini = compute_llm_reviewer_n_mae_mse_r2(ground_truths, predictions_4o_mini)\n",
    "\n",
    "# Accuracy, Macro F1\n",
    "af_4o = compute_accuracy_and_f1(ground_truths, predictions_4o)\n",
    "af_4o_mini = compute_accuracy_and_f1(ground_truths, predictions_4o_mini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84c8ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert:  {'MAE': 1.26, 'MSE': 2.62, 'R2': 0.08}\n",
      "4o n-1:  {'MAE': 0.55, 'MSE': 0.45, 'R2': 0.05}\n",
      "4o mini n-1:  {'MAE': 0.66, 'MSE': 0.68, 'R2': -0.47}\n",
      "4o n:  {'MAE': 2.22, 'MSE': 6.23, 'R2': -3.77}\n",
      "4o mini n:  {'MAE': 2.57, 'MSE': 7.88, 'R2': -5.04}\n",
      "4o af:  {'accuracy': 35.0, 'f1_score': 27.41}\n",
      "4o mini af:  {'accuracy': 33.67, 'f1_score': 25.19}\n"
     ]
    }
   ],
   "source": [
    "print(\"Expert: \", expert)\n",
    "print(\"4o n-1: \", n_minus_1_4o)\n",
    "print(\"4o mini n-1: \", n_minus_1_4o_mini)\n",
    "\n",
    "print(\"4o n: \", n_4o)\n",
    "print(\"4o mini n: \", n_4o_mini)\n",
    "\n",
    "print(\"4o af: \", af_4o)\n",
    "print(\"4o mini af: \", af_4o_mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bac6b",
   "metadata": {},
   "source": [
    "***\n",
    "# Results\n",
    "***\n",
    "\n",
    "#### **Proxy Reviewer = $n-1$**  \n",
    "For each paper, choose randomly $i^{th}$ rating (each paper has 4 ratings). This $i^{th}$ rating is considered **predicted rating**. Except the $i^{th}$ all other ratings (remaining 3 ratings for the paper) are averaged and the value is considered **ground truth rating**. We then compute MAE and MSE using this data.\n",
    "\n",
    "\n",
    "#### **Proxy Reviewer = $n$** \n",
    "For each paper, we have 4 ground truth ratings (by ICLR etc.). We have 4 more reviews from LLMs(GPT-4o and GPT-4o-Mini), named predicted retings. We average the ground truth ratings to have one ground truth rating per paper, similarly we average the predicted ratings to have one predicted rating per paper. We then compute MAE and MSE using this data.\n",
    "\n",
    "\n",
    "#### **Origianl Results (from paper)**\n",
    "| Method             | Proxy(n-1) MAE | Proxy(n-1) MSE | Proxy(n) MAE | Proxy(n) MSE   | Accuracy | Macro F1 |\n",
    "|--------------------|----------------|----------------|---------------|---------------|----------|----------|\n",
    "| Expert  Individual |      1.16      |      2.34      |       -       |       -       | 75.40%   | 75.39%   |\n",
    "| GPT 4o             |      2.24      |      6.61      |     2.30      |      6.53     | 52.58%   | 34.51%   |\n",
    "| GPT 4o Mini        |      1.53      |      3.44      |     1.40      |      2.98     | 53.06%   | 34.72%   |\n",
    "\n",
    "\n",
    "#### **Computed Results with 300 papers**\n",
    "| Method             | Proxy(n-1) MAE | Proxy(n-1) MSE | Proxy(n-1) $R^2$ | Proxy(n) MAE  | Proxy(n) MSE  | Proxy(n) $R^2$ | Accuracy | Macro F1 |\n",
    "|--------------------|----------------|----------------|------------------|---------------|---------------|----------------|----------|----------|\n",
    "| Expert  Individual |      1.16      |      2.35      |       0.2        |       -       |       -       |       -        |    -     |    -     |\n",
    "| GPT 4o             |      0.55      |      0.47      |       0.01       |     2.22      |      6.23     |   -3.78        |  35.0%   | 27.41%   |\n",
    "| GPT 4o Mini        |      0.66      |      0.67      |      -0.52       |     2.57      |      7.88     |   -5.04        | 33.67%   | 25.19%   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc1674",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cccace6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([199, 101]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = []\n",
    "for paper, paer_dict in ground_truths.items():\n",
    "    if paer_dict['decision'] == 'reject':\n",
    "        arr.append(0)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "np.unique(arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6415dd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([  4, 296]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = []\n",
    "for paper, paer_dict in predictions_4o.items():\n",
    "    if paer_dict['decision'] == 'reject':\n",
    "        arr.append(0)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "np.unique(arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b7e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([300]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = []\n",
    "for paper, paer_dict in predictions_4o_mini.items():\n",
    "    if paer_dict['decision'] == 'reject':\n",
    "        arr.append(0)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "np.unique(arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2e391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reviewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
