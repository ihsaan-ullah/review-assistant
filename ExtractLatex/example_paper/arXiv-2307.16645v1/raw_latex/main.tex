\documentclass{article}

\usepackage[numbers]{natbib}
\usepackage[preprint]{neurips_2022}

% \usepackage[dvipsnames]{xcolor}         % colors
\definecolor{linkColor}{rgb}{0.18,0.39,0.62}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true,linkcolor=linkColor,citecolor=linkColor,filecolor=linkColor,urlcolor=linkColor]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{longtable}
\usepackage{color, colortbl}

\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{csquotes}
\usepackage{epigraph}
\usepackage{wrapfig}
\usepackage{import}
\usepackage{chemformula}  

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\input{settings.tex}
\input{math_commands.tex}

% \newcommand{\cmark}{\ding{51}\xspace}%
\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}%
% \newcommand{\xmark}{\ding{55}\xspace}%
\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}%

\newcommand{\tblidx}[1]{{\scriptsize \texttt{[#1]}}}
\newtheorem{theorem}{Property}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{{\color{blue}\ding{51}}}%
\newcommand{\xmark}{{\color{black}\ding{55}}}%
\definecolor{bluecode}{RGB}{0, 150, 199}
\definecolor{lightcyan}{rgb}{0.88, 1, 1}

\definecolor{greensentence}{RGB}{112, 173, 73}
\definecolor{blueword}{RGB}{91, 155, 213}

\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

\newcommand{\hytt}[1]{\texttt{\hyphenchar \font=\defaulthyphenchar#1}}

\title{Scaling Sentence Embeddings with \\ Large Language Models}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{
\vspace{-0.25in} \\
\textbf{Ting Jiang} \quad \textbf{Shaohan Huang} \quad \textbf{Zhongzhi Luan}\\
\textbf{Deqing Wang$^\dagger$} \quad \textbf{Fuzhen Zhuang} \\
Beihang University\\%$$^{1}$\\
% \url{https://github.com/microsoft/unilm}
 \texttt{royokong@buaa.edu.cn}
\vspace{-0.4cm}
\\}
%\url{https://aka.ms/nlpagi} \\}
% ~~\textbf{Saurabh Tiwary,}~~\textbf{Lidong Zhou,}

\date{}

\begin{document}
\maketitle


\begin{abstract}
 \blfootnote{ $\dagger$ Corresponding Author.}
Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks.
However, the application of LLMs to sentence embeddings remains an area of ongoing research.
In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance.
Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes.
Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods.
By scaling model size,
we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks.
However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks.
We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks.
Our code is available at \url{https://github.com/kongds/scaling_sentemb}.

%However, as the input and output of tasks need to be transformed to text, this limits the application for task like sentence embeddings, which requires the output to be vectors that reflect the semantics of sentences.
%%Due to differences in model architecture
%Under this observation, we propose a in-context learning based method to improve sentence embeddings performance.
%We first refine the previous prompt based representation method to adapt it to autoregression models. Then, we propose a framework to automatically construct the demonstration set, which allows LLMs to perform in-contex learning.
%By scaling sentence representation with LLMs, we find that: 1. In-context learning makes LLMs to represent high quality sentence embeddings without any fine-tuning. It is comparative to current contrastive learning methods on unsupervised datasets. 2. For semantic textual similarity (STS) tasks, scaling LLMs up to over billion parameters do not achieve significant improvements. 3. For transfer tasks, scaling LLMs up to 65 billion parameters achieve the new state-of-the-art results.
%We also fine-tune LLMs with contrastive learning based on our representation method.
%Although we use memory-efficient fine-tuning, our model, with merely 2.7B parameters, outperforms the previous state-of-the-art method, which has 4.8B parameters.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Sentence embeddings is a fundamental problem in natural language processing, requiring language models to project sentences into a vector space based on their semantics. Current methods based on contrastive learning, such as SimCSE~\cite{gao2021simcse}, have successfully leveraged pretrained language models to generate high-quality embeddings. A significant amount of research has been devoted to refining the contrastive learning framework in order to further improve sentence embeddings~\cite{chuang2022diffcse, wu2022pcl, wu-etal-2022-infocse, cheng2023improving}.

Recently, large language models (LLMs), such as GPT-3~\cite{gpt3} and LLaMA~\cite{touvron2023llama}, have demonstrated significant potential on various natural language processing tasks such as translation, question answering, and text classification.
Current research has also explored the application of LLMs for data augmentation in sentence embeddings. By generating better sentence pairs for contrastive learning, LLMs can help alleviate the scarcity of labeled data~\cite{cheng2023improving, zhang2023contrastive}.
However, directly utilizing LLMs to generate sentence embeddings presents two primary challenges.
Firstly, LLMs, as autoregressive models, produce text instead of vectors, which necessitates vectorizing the output. Secondly, it is crucial to determine an effective approach for incorporating the capabilities of in-context learning into sentence embeddings.

In this work, we aim to investigate the capabilities of current LLMs for sentence embeddings, facilitated by the availability of open-source LLMs~\cite{touvron2023llama, zhang2022opt}. We address the following research questions: 1) How can LLMs be used to represent sentence embeddings, and does prompt engineering, as demonstrated by PromptBERT~\cite{jiang2022promptbert}?
2) Can in-context learning~\cite{liu2023pre} enhance the quality of sentence embeddings?
3) Does the scaling up the model parameters stil work when the number of parameters exceeds billions?
4) What improvements can be achieved by incorporating the current contrastive learning framework into LLMs?

To address these questions, we conduct a systematic study by evaluating LLaMA~\cite{touvron2023llama} and OPT~\cite{zhang2022opt} on both semantic textual similarity (STS) tasks and transfer tasks. Following~\cite{jiang2022promptbert}, we utilize a prompt such as \textit{This sentence: ``} \texttt{[text]} \textit{'' means} to enable LLMs to generate sentence embeddings, where \texttt{[text]} serves as the input slot. This method outperforms traditional representation methods, such as averaging output tokens to represent sentences.
Considering the causal architecture and pretraining tasks of LLMs compared to BERT, we can refine the prompt to generate better representations by instructing LLMs to encapsulate as much semantic information of the sentences as possible within the target token.

Inspired by~\cite{tsukagoshi-etal-2021-defsent}, which uses definition sentences from a word dictionary to learn sentence embeddings, we find that performance can be further improved by adding definition sentences and corresponding words as examples to perform in-context learning.
To mitigate the gap between examples and input sentences, we also use sentences from the STS-B~\cite{cer2017semeval} training set as examples by instructing ChatGPT to generate a single word to represent the meaning of sentences. By evaluating the demonstration examples based on the STS-B development set, LLMs can outperform previous contrastive learning-based sentence models, which were fine-tuned on unsupervised data.

%Previous work~\cite{sentencet5} demonstrates that scaling up the parameters to several billion can enhance the performance of sentence embeddings.
By scaling up the parameters of LLMs, we find that transitioning from millions to billions of parameters results in improvements on STS tasks.
However, continue scaling up may not yield further improvements. Even with in-context learning, 66B OPT still underperforms 6.7B OPT on STS tasks.
Nonetheless, scaling up improves performance on transfer tasks.
LLMs with tens of billions parameters exhibit strong performances, achieving state-of-the-art performance even without any fine-tuning.

With the advancement of parameter-efficient fine-tuning techniques\cite{hu2021lora, dettmers2023qlora} and post-training quantization methods\cite{frantar2022gptq}, we can also fine-tune LLMs with large batch sizes to conduct contrastive learning, even with limited computational resources.
For instance, fine-tuning 7B parameter LLMs can be accomplished using the same hardware employed for previous BERT-based models like SimCSE~\cite{gao2021simcse}. Even without fine-tuning the full parameters and using the 4-bit quantized method~\cite{dettmers2023qlora}, 2.7B OPT with our sentence embeddings method outperforms a 4.8B ST5~\cite{sentencet5} and achieves the state-of-the-art results on STS tasks. %By increasing the parameters, the best results can be obtained using 7B parameter LLMs.


Our main contributions are as follows:

\begin{enumerate}
\item We propose a sentence embeddings method that leverages LLMs to enhance the representation of sentences. Additionally, we incorporate in-context learning to further improve the quality of sentence embeddings. Our approach demonstrates that LLMs can generate high-quality sentence embeddings without the need for fine-tuning.
\item We conduct an analysis of scaling up the parameters of LLMs from millions to tens of billions in sentence embeddings. We observe scaling to more than tens of billion parameters may harm the performance on STS tasks. However, the largest model can outperform other counterparts on transfer tasks.
\item Based on our method, we discover that performance can be further enhanced by employing contrastive learning. By adopting efficient fine-tuning techniques, LLMs achieve state-of-the-art performance on STS tasks, even with limited computational resources.
\end{enumerate}

\section{Related Work}

\paragraph{Sentence Embeddings}
Sentence embeddings is to convert a sentence into a fixed-size vector, which captures the semantic meaning and context of the sentence.
It allows for the efficient retrieval of similar sentences through the similarity between vectors.
Recently, SimCSE~\cite{gao2021simcse} demonstrated that contrastive learning is an effective approach for learning sentence embeddings using BERT in both unsupervised and supervised settings. In the unsupervised setting, SimCSE predicts the input sentence itself from in-batch negatives, with different dropout~\cite{srivastava2014dropout} masks applied.
In the supervised setting, Natural Language Inference (NLI) datasets~\cite{conneau-etal-2017-supervised-infersent, reimers2019sentence} are used to provide positive and negative pairs.
Following the success of SimCSE, there has been a surge of work exploring contrastive learning-based methods. DiffCSE~\cite{chuang2022diffcse} incorporates a replaced token detection loss into the contrastive learning framework. PromptBERT~\cite{jiang2022promptbert} reveals that prompts can enhance BERT's ability to represent sentences.
%RankEncoder~\cite{seonwoo2022ranking} uses neighboring sentences to capture fine-grained semantic differences.
Additionally, several studies~\cite{cheng2023improving, zhang2023contrastive} have investigated data augmentation for sentence embeddings using LLMs.
SentenceT5 (ST5)~\cite{sentencet5} leverages the encoder-decoder structure of models, such as T5~\cite{raffel2020exploring}, for generating sentence embeddings and demonstrates improvements by scaling T5 from millions to billions of parameters. However, directly using large language models (LLMs) to generate sentence embeddings remains an area of ongoing research.

\paragraph{Large Language Models}
LLMs~\cite{zhang2022opt,bloom,chowdhery2022palm,touvron2023llama} recently show impressive performance on various natural language process, benefiting from their large parameter sizes compared to previous pretrained language models.
LLMs can efficiently learn a new task with in-context learning by using training data as demonstrations~\cite{gpt3}.
Without any gradient updates, LLMs with in-context learning can solve challenging tasks like multitask language understanding~\cite{hendrycks2020measuring}, commonsense reasoning~\cite{lin2021truthfulqa}, and math problems~\cite{cobbe2021training}.
This performance can be further improved by scaling up language models~\cite{hoffmann2022training, kaplan2020scaling}.
%In this work, we use OPT~\cite{zhang2022opt} and LLaMA~\cite{touvron2023llama} to investigate the generation of sentence embeddings.

\section{Methodology}
In this section, we first discuss current sentence embeddings methods with LLMs, and then introduce a new Prompt-based method with Explicit One word Limitation (PromptEOL) for LLMs in Section~\ref{sec:represent_llm}.
Based on this method, we describe two settings: without and with fine-tuning.
For the setting without fine-tuning,
we utilize the in-context learning ability of LLMs to enhance sentence embeddings.
To address the issue of  lacking textual outputs, we propose two methods to automatically generate demonstrations for in-context learning in Section~\ref{sec:method_icl}.
For the setting with fine-tuning, we employ contrastive learning framework, and combine it with the efficient fine-tuning method to alleviate substantial memory requirement in Section~\ref{sec:contrastive_method}.
%First method is gradient free by leveraging in-context learning to help LLMs represent sentence.
%We focus on how to build demonstration set since sentence embeddings lack textual outputs.
%Second method is based on contrastive learning framework by fine-tuning LLMs.
%To overcome the

\subsection{Represent Sentence with LLMs}
\label{sec:represent_llm}
Previous works~\cite{li2020sentence, su2021whitening, jiang2022promptbert} have extensively studied on improving sentence embeddings from  encoder-based pretrained models, like BERT without fine-tuning.
Recently, PromptBERT~\cite{jiang2022promptbert} leverage a prompt-based method to represent sentence.
It uses manual templates like \textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK].}, where \texttt{[text]} is the placeholder for a sentence. The output vector of \texttt{[MASK]} token is used as sentence embeddings.
It demonstrates superior results compared to previous sentence representation methods like averaging output hidden vectors or the output vector of \texttt{[CLS]} token.

\begin{wrapfigure}[18]{r}{7cm}%右侧
\centering
\vspace{-10pt}
	\includegraphics[width=0.44\columnwidth]{LLM_represent.png}
\caption{
  Performances of OPT in STS-B development set with three representation methods.
  Dash lines represent the results of BERT. %\hytt{bert-base-uncased} with 110M parameters.
}
\label{fig:LLM_rep}
\end{wrapfigure}

Considering to LLMs as autoregression models, which do not have special tokens like \texttt{[CLS]} or \texttt{[MASK]}, we modify the prompt-based method in~\cite{jiang2022promptbert} to make it compatible with LLMs.
We use \textit{This sentence: ``} \texttt{[text]} \textit{'' means} to prompt LLMs generate next token and extract the hidden vectors of the final token as sentence embeddings.
To validate the prompt-based method with LLMs, we compare it with two other methods, such as averaging or using the last token as sentence embeddings.
For LLMs, we use OPT~\cite{zhang2022opt} from 125 million parameters to 66 billions and evaluate it on STS-B development set in Figure~\ref{fig:LLM_rep}.
Following the results in~\cite{jiang2022promptbert}, we observe that prompt-based method can enhance sentence representation across all OPTs, ranging from millions to billions parameters.
Despite that the previous prompt-based method also improved LLMs like OPT on sentence representations, OPT, even with significantly more parameters, still fails to outperform BERT.

%Although both BERT and OPT represent sentence embedding out of box without any gradient updates,
%OPT fails to outperform BERT with prompt-based method.

%Consider to bidirectional attention in BERT, we think BERT with tempaltes like like \textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK].} can implicitly dump the whole semantic information corresponding to sentence into one \texttt{[MASK]} token, since \texttt{[MASK]} follow up with period, which implicitly limit the corresponding meaning explain to one tokens.
Consider to bidirectional attention in BERT, we hypothesize that BERT can implicitly condense the entire semantic information corresponding to a sentence into a single \texttt{[MASK]} token when using templates like ``\textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK].}''.
Since the \texttt{[MASK]} token follows a period, this implicitly restricts BERT to explain meaning into one word.
However, this template fails to add the similar ``one word limitation'' when it is used in autoregression models like OPT with unidirectional attention.
To validate this, we simply remove the period in template to transfer it into ``\textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK]}''.
Despite only one word difference, and no modification to meaning of the template, the performance of BERT on STS-B development set plummeted from 73.44 to 33.89 Spearman correlation, which means BERT without this implicit ``one word limitation'' fails to represent sentence.

Inspired by this, our objective is to enhance prompt-based method for LLMs by introducing a ``one word limitation''.
We propose a new Prompt-based method with Explicit One word Limitation (PromptEOL) for LLMs.
PromptEOL is simple and straightforward by directly adding some tokens in the template to instruct LLMs in predicting the meaning of sentence in one word.
The template we used after modification is following:

\centerline{\textit{This sentence: ``} \texttt{[text]} \textit{'' means in one word: ``}}

Compared to the template in~\cite{jiang2022promptbert}, we introduce two simple modifications for LLMs.
First, we append \textit{in one word} to the prompt to constrain LLMs in predicting semantic information in next token.
Secondly, we incorporate \textit{: ``} at the end of template to prevent model form generating punctuations in next token, as \textit{This sentence: ``} is used to indicate the input of a sentence.
We find this template improve all OPT models and allow them to match or even outperform BERT with prompt-based method in Figure~\ref{fig:PromptEOL_rep}.

\begin{figure}
\centering
%\vspace{-13pt}
\vspace{-13pt}
	\includegraphics[width=\columnwidth]{framework.png}
\caption{
  An illustration of in-context learning based sentence embeddings.
  The \textcolor{greensentence}{green} sentences denote the demonstration sentence, and the \textcolor{blueword}{blue} words denote the demonstration words. The corresponding color blocks refer to their slots in the template.
} \label{fig:framework}
\vspace{-10pt}
\end{figure}

\subsection{Improve Sentence Embeddings with In-context Learning}\label{sec:method_icl}
In-context learning is widely utilized as an effective method to help LLMs understand problems. It improves their comprehension of inputs and outputs by directly adding a few examples in the prompts.
However, when considering the problem of sentence embeddings, we need to project sentences into vectors based on their semantic information, separately.
In other word, sentence embeddings lack textual outputs that could be used as examples to perform in-context learning, such as answers for question answer problems or labels for text classification problems. Moreover, there are also no predetermined gold vectors for a given sentence.

To leverage in-context learning in sentence embeddings, we propose an framework to automatically build demonstration sets and search demonstration to improve LLMs sentence embeddings in Figure~\ref{fig:framework}.
For the demonstration set, the goal is to create sentence and word pairs, where the word can represents the semantic information of the sentence.
We propose two methods to generate pairs.

The first method involves using ChatGPT to generate corresponding words according to the semantic information of given sentences from STS-B training set.
By asking ChatGPT with same template in Figure~\ref{fig:framework}, ChatGPT outputs one word summary for the given sentence.
We also find ``one word limitation''  in Section~\ref{sec:represent_llm} is important for ChatGPT.
%By removing \textit{in one word} from the template, ChatGPT explain the meaing of sentence in long way, and the first word will be articles such as ``The''.
Consider to our prompt-based representation method, we employ the hidden state of the next token as the sentence embeddings.
By removing \textit{in one word} from the template, it tends to explain the meaning of a sentence in a lengthy way, and the first word often becomes an article such as ``The'', which lacks clear meaning.
For example,
given the sentence ``A jockey riding a horse.'',
the hidden state achieves the highest dot product similarity for ``Equestrain'' among its word embeddings.
However, without ``one word limitation'', it will achieve the highest dot product similarity for word without specific meaning such as ``The'' among its word embeddings, which can not represent sentence properly.
Inspired by DefSent~\cite{tsukagoshi-etal-2021-defsent}, which leverages definition sentences with their words as labels to train unsupervised sentence embedding,
our second method is also based on a word dictionary. We directly use words and their definition sentences in the Oxford dictionary as word-sentence pairs.

%\begin{wrapfigure}{r}{7cm}%右侧
%\centering
%%\vspace{-13pt}
%	\includegraphics[width=0.45\columnwidth]{LLM_icl.png}
%\caption{
%  Performance of OPT in STS-B development set with three representation methods.
%}
%\label{fig:LLM_icl}
%\end{wrapfigure}

Based on these methods, we construct a demonstration set consisting of 300 pairs of sentences and words.
100 pairs are from STS-B training set, with words labeled by ChatGPT, while the remaining are from the Oxford dictionary.
To find demonstration that help model to represent sentences,
we directly evaluate each demonstration on the STS-B development set and use the demonstration with the best Spearman correlation as the demonstration for corresponding models.
We also visualize the distribution of Spearman correlations for OPT from 125M to 66B parameters in Figure~\ref{fig:LLM_icl_hist}.
Following the previous study~\cite{kaplan2020scaling}, we notice that in-context learning achieves better performance, when increasing model parameter from 125M to 2.7B.
For example, there are only one demonstration that helps the 125M OPT achieve better performance compared to without demonstration.
However, around 98\% of demonstrations improve the performance of the 2.7B OPT.
In-context learning significantly enhance the sentence embeddings, especially for OPT with more than 1B parameters.
%Even if we not use methods like contrastive learning to avoid anisotropy
With only in-context learning, OPT with more than 1.3B parameters even achieve better results on STS tasks compared to contrastive learning based method like SimCSE~\cite{gao2021simcse} in Table~\ref{tab:sts_wo_ft}.

\begin{figure}
\centering
\vspace{-13pt}
	\includegraphics[width=\columnwidth]{LLM_icl_hist.png}
\caption{
  Distribution of Spearman correlations on the STS-B development set with different in-context learning demonstrations.
  The red dash line represents the Spearman correlation of the corresponding model without any demonstration.
  The blue area represents demonstrations that negatively impact the performance, and the percentage refers to the proportion of these demonstrations to the total number of demonstrations.
}
\vspace{-10pt}
\label{fig:LLM_icl_hist}
\end{figure}


\subsection{Contrastive Learning with Efficient Fine-tuning}
\label{sec:contrastive_method}

Since in-context learning boosts sentence embeddings performances  without any gradient update,
we also exploit contrastive learning on LLMs, which has been demonstrated as an efficient way to learn sentence embeddings~\cite{gao2021simcse}.
It can be divided into unsupervised and supervised settings, according to the datasets.
For unsupervised setting, the sentences in dataset lack corresponding positive and negative sentences to perform contrastive learning.
For supervised setting, natural language inference (NLI) datasets are used as the datasets, and each sentence has corresponding positive and negative sentences.
In this section, we focus on the supervised setting to fully leverage LLMs for sentence embeddings.

However, contrastive learning requires a large batch size to increase the number of negative samples, which demands a high amount of GPU memory, especially in the supervised setting.
For example, SimCSE uses a batch size of 512 to fine-tune 110M BERT in the supervised setting. Each batch includes 1536 sentences, containing both their positive and hard negative sentences. It requires 58GB of GPU memory on 4 GPUs.
As a result, fine-tuning LLMs with contrastive learning becomes challenging due to the memory requirements, particularly for models with significantly larger parameter sizes than BERT.
%Thus, it would be unafforded for us to fine-tune LLMs, which can have more than hundreds of times parameters compared to BERT.

To solve this problem, we leverage current efficient fine-tuning method QLoRA~\cite{dettmers2023qlora}.
QLoRA combines two techniques to significantly reduces memory usage: 4-bit quantization and parameter efficient fine-tuning.
Quantization reduces the memory usage of LLMs by quantizing their weight from 16-bit to 4-bit.
Parameter efficient fine-tuning with LoRA~\cite{hu2021lora} significantly reduces the memory usage of optimizer compared to full fine-tuning by only fine-tuning small proportion of weight.

Following~\cite{gao2021simcse}, we use SNLI and MNLI datasets where each sentence $x_i$ has corresponding a positive sentence $x_i^{+}$ and a hard negative sentence $x_i^{-}$. To represent sentence, we use our prompt-based method in Section~\ref{sec:represent_llm}.
Formally, given sentence $x_i$, we first add $x_i$ to the template and get hidden states:

\begin{equation}
\begin{split}
  \mathbf{h}_{i1}, \ldots, \mathbf{h}_{il} = {\rm LLM}(\textit{This  sen}&\textit{tence: ``} x_i \textit{'' means in one word: ``})\\
  %h &= h_L
\end{split}
\end{equation}

where $l$ is the number of hidden states. We then use last token hidden state as its sentence embedding $\mathbf{h}_{i} = \mathbf{h}_{il}$.
Since we can represent the sentence pair $(x_i, x_i^{+}, x_i^{-})$ to their embeddings $(\mathbf{h}_i, \mathbf{h}_i^{+}, \mathbf{h}_i^{-})$.
Our training objective is following:

\begin{equation}
\ell_{i}=-\log \frac{e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_i^{+}\right) / \tau}}{\sum_{j=1}^N\left(e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_j^{+}\right) / \tau}+e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right)}
\end{equation}

where $N$ is the batch size and \(\tau\) is the temperature hyperparameter in contrastive learning.

\section{Experiment}
\subsection{Implementation Details}
For the setting without fine-tuning, we use OPT from 125M to 66B parameters, and LLaMA from 7B to 65B parameters.
All models use the same template in Section~\ref{sec:represent_llm}.
 We use 300 pairs of sentences and words as demonstration set for in-context learning. % 100 pairs are from STS-B training set, and we use gpt-3.5-turbo to label their words. The remaining 200 pairs from oxford dictionary.
 Among these, 100 pairs are from the STS-B training set, and we use \hytt{gpt-3.5-turbo} to label their words. The remaining 200 pairs are from the Oxford dictionary. We provide all demonstrations in Appendix~\ref{apx:demo}.
For each model, we choose only one demonstration that has the highest Spearman correlation on the STS-B development set as their demonstration for evaluation.
All results from models with 16-bit weights. We also present results using quantization methods in Appendix~\ref{apx:quant}.

For the setting with fine-tuning,
we use QLoRA~\cite{dettmers2023qlora} to fine-tune OPT and LLaMA with contrastive learning.
Following QLoRA, we use LoRA $r=64, \alpha=16$, dropout $=0.05$, and add LoRA modules on all linear layers of the 4-bit quantized model.
We fine-tune models on the NLI datasets~\cite{gao2021simcse} with one epoch, temperature \(\tau = 0.5\) and learning rate 5e-4.
Due to hardware limitations, we only conduct our experiments with model parameters less than or equal to 13B with 8 RTX-3090 GPUs.
For models with fewer than 7B parameters, we fine-tune them on 2 GPUs with a batch size of 256. For 7B models, we use 4 GPUs with a batch size of 256. For 13B models, we use 8 GPUs with a batch size of 200.
%For models with less than 7B parameters, we fine-tune them on 2 GPUs with 256 batch size. For 7B models, we use 4 GPUs with 256 batch size.For 13B models, we use 8 GPUs with 200 batch size.
%We find contrastive learning with our

\subsection{Dataset}
Following previous works~\cite{gao2021simcse,  jiang2022promptbert},
We use the SentEval toolkit~\cite{conneau2018senteval} to conduct our experiments on seven STS datasets and seven transfer learning datasets.
The STS datasets include STS tasks 2012-2016~\cite{agirre2012semeval, agirre2013sem, agirre2014semeval, agirre2015semeval, agirre2016semeval} STS-B~\cite{cer2017semeval}, SICK-R~\cite{marelli2014sick}. Sentence pairs in each STS dataset are scored from 0 to 5 to indicate semantic similarity.
Spearman correlation is used as a metric to evaluate the correlation between the cosine similarity of sentence embeddings  and the golden similarity scores.
The transfer learning datasets include MR~\cite{pang2005seeing_mr}, CR~\cite{hu2004mining_cr}, SUBJ~\cite{pang2004sentimental_subj}, MPQA~\cite{wiebe2005annotating_mpqa}, SST-2~\cite{socher2013recursive_sst-2}, TREC~\cite{voorhees2000building_trec} and MRPC~\cite{mrpc2005}.
Sentence embeddings are used as input feature to train corresponding logistic regression classification.

\begin{table*}[t]
\vspace{-13pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on unsupervised datasets}}\\
\midrule
SimCSE-BERT\(^\dagger \)  & 110M & 68.40 & 82.41 & 74.38 & 80.91 & 78.56 &    76.85     &      72.23      & 76.25 \\
SimCSE-RoBERTa\(^\dagger \) & 123M & 70.16 &  81.77 & 73.24 & 81.36 & 80.65 & 80.22 & 68.56 & 76.57 \\
PromptBERT\(^\ddagger \)  & 110M & 71.56 & 84.58 & 76.98 & 84.47 & 80.60 &  81.60 & 69.87& 78.54 \\
PromptRoBERTa\(^\ddagger \) & 123M &  73.94 & 84.74 & 77.28 & 84.99 & 81.74 & 81.88 & 69.50 & 79.15 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
BERT avg.\(^\dagger \) & 110M & 30.87 & 59.89 & 47.73 & 60.29 & 63.73 & 47.29 & 58.22 & 52.57 \\
%BERT last-first avg.\(^\dagger\) & 110M & 39.70 & 59.38 & 49.67 & 66.03 & 66.19 & 53.87 & 62.06 & 56.70\\
%BERT-flow \(^\dagger\) & 58.40 & 67.10 & 60.85 & 75.16 & 71.22 & 68.66 & 64.47 & 66.55 \\
%BERT-whitening  \(^\dagger\) & 57.83 & 66.90 & 60.90 & 75.08 & 71.31 & 68.24 & 63.73 & 66.28 \\
BERT prompt\(^\ddagger \) & 110M & 60.96  & 73.83 & 62.18 & 71.54 & 68.68 & 70.60 & 67.16 & 67.85 \\
ST5-Enc\(^\S\) & 4.8B & 34.97 & 60.19 & 47.59 & 66.40 & 70.62 & 62.83 & 63.57 & 58.02 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
& 125M & 59.90 & 71.55 & 60.93 & 70.76 & 72.83 & 67.89 & 65.14 & 67.00 \\
& 350M & 54.70 & 71.52 & 59.99 & 64.51 & 71.39 & 66.55 & 66.58 & 65.03 \\
& 1.3B\cellcolor{lightcyan} & 64.59\cellcolor{lightcyan} & 79.06\cellcolor{lightcyan} & 68.46\cellcolor{lightcyan} & 78.88\cellcolor{lightcyan} & 78.64\cellcolor{lightcyan} & 73.22\cellcolor{lightcyan} & 69.41\cellcolor{lightcyan} & 73.18\cellcolor{lightcyan} \\
& 2.7B & 60.03 & 75.51 & 64.30 & 74.56 & 77.62 & 67.73 & 65.35 & 69.30 \\
& 6.7B & 60.91 & 80.05 & 67.65 & 75.49 & 80.11 & 72.91 & 67.57 & 72.10 \\
& 13B &  60.21 & 81.36 & 69.69 & 75.46 & 79.58 & 70.73 & 65.99 & 71.86 \\
& 30B &  59.99 & 80.52 & 69.80 & 75.20 & 78.03 & 73.57 & 69.87 & 72.43\\
& 66B &  55.66 & 74.62 & 64.90 & 72.34 & 75.21 & 71.72 & 67.43 & 68.84 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT}
 &  125M& 62.22 & 73.10 & 61.84 & 71.09 & 72.08 & 67.80 & 64.10 & 67.46 \\
 &  350M& 63.87 & 73.85 & 63.41 & 72.45 & 73.13 & 70.84 & 65.61 & 69.02 \\
 &  1.3B& 72.78 & 83.77 & 73.61 & 83.42 & 80.60 & 78.80 & 69.69 & 77.52 \\
 &  2.7B& 68.49 & 84.72 & 75.15 & 83.62 & 81.34 & 80.94 & 72.97 & 78.18 \\
 &  6.7B\cellcolor{lightcyan}& 70.65\cellcolor{lightcyan} & 84.51\cellcolor{lightcyan} & 75.01\cellcolor{lightcyan} & 83.51\cellcolor{lightcyan} & 82.00\cellcolor{lightcyan} & 81.12\cellcolor{lightcyan} & 76.77\cellcolor{lightcyan} & 79.08\cellcolor{lightcyan} \\
 &  13B & 71.99 & 85.22 & 76.04 & 82.23 & 81.38 & 81.42 & 75.00 & 79.04 \\
 &  30B & 69.99 & 83.35 & 74.75 & 83.14 & 82.42 & 81.45 & 77.46 & 78.94 \\
 &  66B & 69.93 & 83.29 & 74.88 & 80.10 & 81.11 & 81.76 & 76.26 & 78.19 \\

\bottomrule
\end{tabular}
}
\caption{ Performances of our method on STS tasks without fine-tuning. ICL denotes in-context learning with our demonstration set.
  %Avg. denotes averaging the last layer output of the model.
  %Prompt denots using the template from~\cite{jiang2022promptbert}.
  \(\dagger\): results from~\cite{gao2021simcse}.
  \(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:sts_wo_ft}
\vspace{-10pt}
\end{table*}

\subsection{Results}
We compare our method with BERT-based methods such as SBERT~\cite{reimers2019sentence}, SimCSE~\cite{gao2021simcse}, and PromptBERT~\cite{jiang2022promptbert}. In addition, we include other sentence methods based on LLMs as baselines, such as ST5~\cite{sentencet5} and SGPT~\cite{muennighoff2022sgpt}.
Among these baselines, ST5 achieves state-of-the-art results on both STS and transfer learning tasks by further fine-tuning 4.8B parameters T5 encoder  with contrastive learning.




\begin{table*}[t]
\vspace{-13pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on supervised datasets}}\\
\midrule
SBERT-NLI\(^\dagger \) & 220M & 72.27 & 78.46 & 74.90 & 80.99 & 76.25 & 79.23 & 73.75 & 76.55 \\
\multirowcell{2}{SimCSE-RoBERTa\(^\dagger \)} & 123M & 76.53 & 85.21 & 80.95 & 86.03 & 82.57 & 85.83 & 80.50 & 82.52 \\
 & 354M & 77.46 & 87.27 & 82.36 & 86.66 & 83.93 & 86.70 & 81.95 & 83.76 \\
PromptRoBERTa\(^\ddagger \) & 123M & 76.75 & 85.93 & 82.28 & 86.69 & 82.80 & 86.14 & 80.04 & 82.95 \\
SGPT\(^\mathparagraph\) & 5.8B & 74.28 & 85.35 & 79.21 & 85.52 & 82.54 & 85.50 &  79.53 & 81.70 \\
ST5-Enc\(^\S\) & 4.8B & 80.10 & 88.75 & 84.70 & 88.86 & 85.17 & 86.77 & 80.39 & 84.96 \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL+CSE\\OPT}
 & 1.3B & 79.01 & 89.26 & 84.10 & 88.30 & 84.62 & 87.71 & 80.52 & 84.79\\
 & 2.7B & 79.49 & 89.64 & 84.80 & 89.51 & 85.91 & 88.33 & 81.64 & 85.62\\
 & 6.7B & 80.14 & 90.02 & 84.94 & 89.78 & 85.84 & 88.75 & 81.29 & 85.82\\
 &\cellcolor{lightcyan}13B &\cellcolor{lightcyan}80.20 &\cellcolor{lightcyan}90.24 &\cellcolor{lightcyan}85.34 &\cellcolor{lightcyan}89.52 &\cellcolor{lightcyan}85.90 &\cellcolor{lightcyan}88.56 &\cellcolor{lightcyan}82.06 &\cellcolor{lightcyan}85.97\\
\midrule
\multirowcell{2}[0pt][l]{PromptEOL+CSE\\LLaMA}
%& 13B &
& 7B & 79.16 & 90.22 & 85.40 & 88.99 & 86.25 & 88.37 & 81.51 & 85.70 \\
&\cellcolor{lightcyan}13B &\cellcolor{lightcyan}78.63 &\cellcolor{lightcyan}90.03 &\cellcolor{lightcyan}85.46 &\cellcolor{lightcyan}89.48 &\cellcolor{lightcyan}86.18 &\cellcolor{lightcyan}88.45 &\cellcolor{lightcyan}82.69 &\cellcolor{lightcyan}85.85 \\
\bottomrule
\end{tabular}
}
\caption{ Performances of our method on STS tasks with fine-tuning. CSE denotes contrastive learning for sentence embeddings.
  \(\dagger\): results from~\cite{gao2021simcse}.
  %\(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
  \( \mathparagraph \): results from evaluation the public checkpoint~\cite{muennighoff2022sgpt} on STS tasks.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:sts_w_ft}
\end{table*}


\begin{table*}[t]
\vspace{-8pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on supervised datasets}}\\
\midrule
\multirowcell{2}{SimCSE-RoBERTa$^\dagger $} & 123M & 84.92 & 92.00 & 94.11 & 89.82 & 91.27 & 88.80 & 75.65 & 88.08 \\
 & 220M & 81.12 & 92.37 & 95.11 & 90.49 & 92.75 & 91.80 & 76.64 & 89.61 \\
PromptRoBERTa$^\ddagger$ & 123M & 85.74 & 91.47 & 94.81 & 90.93 & 92.53 & 90.40 & 77.10 & 89.00\\
ST5-Enc$^\S$ & 4.8B & 90.83 & 94.44 & 96.33 & 91.68 & 94.84 & 95.40 & 77.91 & 91.63 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
%SimCSE-RoBERTa\(^\dagger \) & 220M & 881.12 & 92.37 & 95.11 & 90.49 & 92.75 & 91.80 & 76.64 & 89.61 \\
%SGPT & 5.8B & 74.28 & 85.35 & 79.21 & 85.52 & 82.54 & 85.50 &  79.53 & 81.70 \\
%OPT avg. & 66B & 89.17 & 92.00 & 96.86 & 90.80 & 94.67 & 96.40 & 71.07 & 90.14\\
%OPT prompt & 66B & 89.15 & 91.90 & 97.17 & 91.32 & 93.96 & 96.20 & 71.42 & 90.16\\
%LLaMA avg. & 65B & 89.59 & 91.97 & 97.64 & 89.94 & 93.74 & 97.00 & 72.17 & 90.29 \\
%LLaMA prompt & 65B  & 89.73 & 91.58 & 97.65 & 90.58 & 94.40 & 96.80 & 69.33 & 90.01 \\
BERT avg. & 110M & 78.66 & 86.25 & 94.37 & 88.66 & 84.40 & 92.80 & 69.54 & 84.94 \\
ST5-Enc$^\S$ & 4.8B & 91.15 & 93.33 & 97.55 & 90.20 & 94.07 & 94.40 & 74.26 & 90.71 \\
\midrule
\multirowcell{6}[0pt][l]{PromptEOL\\OPT}
 %&  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 %&  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66\cellcolor{lightcyan}B  & 90.96\cellcolor{lightcyan} & 93.40\cellcolor{lightcyan} & 97.01\cellcolor{lightcyan} & 91.93\cellcolor{lightcyan} & 95.22\cellcolor{lightcyan} & 96.40\cellcolor{lightcyan} & 75.25\cellcolor{lightcyan} & 91.45\cellcolor{lightcyan} \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL\\LLaMA}
& 7B & 90.40 & 92.90 & 96.88 & 91.57 & 95.11 & 95.40 & 75.13 & 91.06 \\
& 13B & 92.02 & 93.22 & 97.29 & 91.40 & 95.66 & 95.80 & 76.46 & 91.69 \\
& 30B & 91.64 & 93.27 & 97.10 & 91.86 & 95.99 & 95.80 & 78.43 & 92.01 \\
& 65B\cellcolor{lightcyan} & 92.13\cellcolor{lightcyan} & 93.43\cellcolor{lightcyan} & 97.16\cellcolor{lightcyan} & 91.91\cellcolor{lightcyan} & 95.33\cellcolor{lightcyan} & 97.40\cellcolor{lightcyan} & 77.28\cellcolor{lightcyan} & 92.09\cellcolor{lightcyan} \\
\bottomrule
\end{tabular}}
\caption{ Performances of our method on transfer learning tasks.
  \(\dagger\): results from~\cite{gao2021simcse}.
  \(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:tran_w_ft}
\vspace{-15pt}
\end{table*}

\textbf{STS tasks without fine-tuning}
Table~\ref{tab:sts_wo_ft} shows the results of PromptEOL with and without in-context learning on STS tasks.
Even without corresponding textual outputs for sentence embeddings, in-context learning still helps model to generate better embeddings.
As the model size grows, improvements from in-context learning also increase.
Moreover, in-context learning shows significantly improvements on STS tasks for model with more than billions parameters.
For instances, it raises the Spearman correlation from 68.84 to 78.19 on 66B OPT.
Our method with in-context learning also outperforms among methods without fine-tuning.
%Even these modeles are only pretrained according to next token prediction task, without any
Even if we do not use any method to avoid anisotropy~\cite{ethayarajh2019contextual}, which is widely regarded as the main reason for poor performance on STS tasks~\cite{gao2021simcse, sentencet5}, our method still outperforms unsupervised methods such as SimCSE and PromptBERT, which use contrastive learning to avoid anistoropy.
Additionally, we find the performance is not sensitive to the model size while scaling model beyond a billion parameters.
Smaller models, such as 1.3B OPT, even outperforms SimCSE without fine-tuning.

\textbf{STS tasks with fine-tuning}
Table~\ref{tab:sts_w_ft} shows the results by fine-tuning with PromptEOL on the supervised dataset.
%We use the same dataset with SimCSE and PromptBERT.
Compared to ST5-Enc, which fine-tuned all 4.8B parameters on Community QA and NLI datasets,
our method with 2.7B OPT achieves superior results through parameter-efficient fine tuning on the 4-bit model with only NLI datasets.
%we find that our method by fine-tuning 2.7B OPT with parameter-efficient tuning and 4-bit quantization can surpass it.
Keep scaling up the parameters size, 13B OPT and LLaMA achieve the best performance on STS tasks.
However, the improvement in scaling model parameters from 2.7B to 13B is not significant.
%Due to hardware limitations, we are unable to fine-tune it with
%Each models can be trained in several hours on RTX 3090. We report training time in Appendix~\ref{apx:train_time}.


\textbf{Transfer tasks}
We also report the results of our method on the transfer learning tasks in Table~\ref{tab:tran_w_ft}.
Unlike STS tasks, we observe that LLMs achieve better performance as the model size increases.
Specifically, the 66B OPT and 65B LLaMA models outperform their smaller counterparts with our representation method.
Based on our representation method, LLMs show good performance without in-context learning and contrastive learning.
Following ST5~\cite{sentencet5}, we find that applying contrastive learning solely on NLI datasets can even harm performance on transfer tasks.
To solve this problem, ST5 utilizes additional datasets, such as the Community QA dataset, to enhance its performance in transfer tasks.
For in-context learning, as it is widely used in text classification,
we find that using examples not relevant to tasks, such as STS-B or the dictionary, does not enhance transfer task performance.
We present these results in Appendix~\ref{apx:transfer_task}.



\section{Analysis}
\subsection{Sentence Representation Methods}
We present the results obtained using three sentence representation methods, across models ranging in size from 125M to 66B parameters, as shown in Figure~\ref{fig:PromptEOL_rep}. Different representation methods can yield significantly different results.
Prompt-based methods outperform direct averaging in three settings.
Among these methods, PromptEOL exhibits the best performance, as it introduces an explicit ``one-word limitation''.
More detail results can be find in Appendix~\ref{apx:sentence_rep}.

\begin{figure}[h]
\centering
%\vspace{-13pt}
	\includegraphics[width=\columnwidth]{PromptEOL_compare.png}
\caption{
  Influence of different sentence representation methods on three settings.
  ``avg.'' refers to use averaging output tokens as sentence embeddings.
  ``prompt'' refers to extract sentence embeddings using the template from~\cite{jiang2022promptbert} .
  Dash lines represent the results from the base-size BERT.
}
\label{fig:PromptEOL_rep}
\end{figure}

\subsection{In-context Learning}

\begin{wraptable}[10]{r}{7cm}
\tiny
\centering
\vspace{-18pt}
\begin{tabular}{lp{2.8cm}cc}
\hline
& Sentence & Word & Improve\\
\hline
125M & A man is smoking. & Smoking & 0.46\\
350M & A man is playing on a guitar and singing. & Music & 3.99\\
1.3B & relating to switzerland or its people. & Swiss & 4.34\\
2.7B & A jockey riding a horse. & Equestrian & 8.88\\
6.7B & The man is riding a horse. & Horseback-riding & 6.98\\
13B & meat from a deer. & Venison & 7.18\\
30B & The man is riding a motorcycle down the road. & Motorcycling & 6.51\\
66B & of or relating to tutors or tutoring. & Tutorial & 9.35\\
\hline
\end{tabular}
\caption{ In-context learning examples used in various model size.
}
\end{wraptable}
We demonstrate in-context learning examples that were obtained from each model on the STS-B development set, along with corresponding improvements on Spearman correlation for STS tasks.
As the size of the model increases to 2.7B, the improvements in in-context learning become more and more pronounced, and related examples are usually more implicit. For instance, the 125M OPT uses examples where words are incorporated within the sentence.

\section{Conclusion}

In this paper, we focus on exploiting Large Language Models (LLMs) to improve sentence embeddings. To achieve this, we propose a new sentence embeddings method called PromptEOL, which adapts previous prompt-based methods to autoregression models.
Furthermore, we leverage in-context learning to generate superior sentence embeddings by utilizing ChatGPT and the Oxford dictionary to create sentence embeddings demonstrations.
It demonstrates in-context learning allows LLMs to achieve performance comparable to current contrastive learning methods.
With our promtp-based method, we also discover that further fine-tuning of LLMs can achieve the state-of-the-art performance using only efficient fine-tuning methods.

\bibliographystyle{alpha}
\bibliography{scal_sent}


\appendix
%\section{Training Time}
\input{all-300-examples}

\newpage

\section{Influence of Quantization}
\label{apx:quant}

We analyze the influence of quantization in Table~\ref{tab:quantization} between the 16bit models and 4bit models, which are quantized by bitsandbytes~\footnote{https://github.com/TimDettmers/bitsandbytes} with 4-bit normalfloat and double quantization.
We find large models tend to show better results on STS tasks after 4-bit quantization.
For example, PromptEOL+ICL with 6.7B OPT improve Spearman correlation from 79.08 to 79.38.

\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT(16-bit)}
& 125M & 59.90 & 71.55 & 60.93 & 70.76 & 72.83 & 67.89 & 65.14 & 67.00 \\
& 350M & 54.70 & 71.52 & 59.99 & 64.51 & 71.39 & 66.55 & 66.58 & 65.03 \\
& 1.3B & 64.59 & 79.06 & 68.46 & 78.88 & 78.64 & 73.22 & 69.41 & 73.18 \\
& 2.7B & 60.03 & 75.51 & 64.30 & 74.56 & 77.62 & 67.73 & 65.35 & 69.30 \\
& 6.7B & 60.91 & 80.05 & 67.65 & 75.49 & 80.11 & 72.91 & 67.57 & 72.10 \\
& 13B &  60.21 & 81.36 & 69.69 & 75.46 & 79.58 & 70.73 & 65.99 & 71.86 \\
& 30B &  59.99 & 80.52 & 69.80 & 75.20 & 78.03 & 73.57 & 69.87 & 72.43\\
& 66B &  55.66 & 74.62 & 64.90 & 72.34 & 75.21 & 71.72 & 67.43 & 68.84 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT(4-bit)}
& 125M & 60.53 & 70.03 & 59.02 & 69.77 & 72.38 & 66.47 & 65.17 & 66.20 \\
& 350M & 58.03 & 72.61 & 61.34 & 66.14 & 72.99 & 67.27 & 65.10 & 66.21 \\
& 1.3B & 63.72 & 79.32 & 68.13 & 77.92 & 78.56 & 72.03 & 68.80 & 72.64 \\
& 2.7B & 57.80 & 72.45 & 61.09 & 73.33 & 76.22 & 64.71 & 64.07 & 67.10 \\
& 6.7B & 63.81 & 81.45 & 69.90 & 77.68 & 80.92 & 75.51 & 69.28 & 74.08 \\
& 13B &  60.91 & 80.97 & 70.22 & 76.93 & 79.46 & 72.84 & 66.34 & 72.52 \\
& 30B &  59.33 & 79.65 & 69.25 & 73.87 & 77.79 & 71.72 & 69.07 & 71.53 \\
& 66B &  59.35 & 77.33 & 68.33 & 74.45 & 77.25 & 73.93 & 69.27 & 71.42 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT(16-bit)}
 &  125M& 62.22 & 73.10 & 61.84 & 71.09 & 72.08 & 67.80 & 64.10 & 67.46 \\
 &  350M& 63.87 & 73.85 & 63.41 & 72.45 & 73.13 & 70.84 & 65.61 & 69.02 \\
 &  1.3B& 72.78 & 83.77 & 73.61 & 83.42 & 80.60 & 78.80 & 69.69 & 77.52 \\
 &  2.7B& 68.49 & 84.72 & 75.15 & 83.62 & 81.34 & 80.94 & 72.97 & 78.18 \\
 &  6.7B& 70.65 & 84.51 & 75.01 & 83.51 & 82.00 & 81.12 & 76.77 & 79.08 \\
 &  13B & 71.99 & 85.22 & 76.04 & 82.23 & 81.38 & 81.42 & 75.00 & 79.04 \\
 &  30B & 69.99 & 83.35 & 74.75 & 83.14 & 82.42 & 81.45 & 77.46 & 78.94 \\
 &  66B & 69.93 & 83.29 & 74.88 & 80.10 & 81.11 & 81.76 & 76.26 & 78.19 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT(4-bit)}
&  125M& 61.02 & 71.00 & 59.75 & 69.67 & 70.52 & 65.14 & 63.45 & 65.79 \\
&  350M& 64.14 & 72.45 & 62.58 & 71.05 & 70.18 & 67.67 & 65.52 & 67.66 \\
&  1.3B& 73.45 & 82.55 & 73.11 & 83.63 & 80.60 & 78.72 & 69.06 & 77.30 \\
&  2.7B& 68.50 & 84.73 & 74.62 & 82.23 & 80.87 & 80.81 & 72.30 & 77.72 \\
&  6.7B& 70.23 & 84.64 & 76.08 & 83.73 & 82.06 & 81.66 & 77.29 & 79.38 \\
&  13B & 71.79 & 84.23 & 75.57 & 81.75 & 80.71 & 80.89 & 74.46 & 78.49 \\
&  30B & 70.61 & 84.05 & 75.27 & 83.23 & 82.77 & 81.45 & 77.31 & 79.24 \\
&  66B & 71.67 & 83.95 & 75.67 & 81.33 & 81.86 & 82.58 & 76.54 & 79.09 \\
\bottomrule
\end{tabular}
}
\caption{ Influence of quantization on STS tasks. ICL denotes in-context learning with our demonstration set.
} \label{tab:quantization}
\vspace{40pt}
\end{table*}

\newpage


\section{Transfer Tasks}\label{apx:transfer_task}
The results of PromptEOL with in-context learning (ICL) and contrastive learning (CSE) are shown in Table~\ref{fig:transfer_icl_cse}. Compared to PromptEOL, both PromptEOL+ICL and PromptEOL+CSE appeared to hinder performance on transfer tasks. We anticipate that the incorporation of additional datasets, such as the Community QA dataset, in accordance with ST5~\cite{sentencet5}, or the implementation of full-model fine-tuning, might enhance the performance of PromptEOL+CSE in transfer tasks, which we leave in future.
For PromptEOL+ICL, using STS-B or a dictionary as the example did not improve the performance on transfer tasks.
We discover that using examples from a task with the label as the word in the example can improve the original performance.
For instance, if we use one positive example and one negative example from training set of MR tasks, it increases the accuracy on MR in 6.7B OPT by approximately one point.
We find these examples also beneficial to other transfer tasks, improving the average accuracy from 91.34 to 91.78, which can exceed 66B OPT performance.
\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
 &  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 &  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66B  & 90.96 & 93.40 & 97.01 & 91.93 & 95.22 & 96.40 & 75.25 & 91.45 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT}
 &  125M & 80.86 & 87.10 & 93.08 & 89.55 & 87.10 & 92.00 & 73.28 & 86.14\\
 &  350M & 82.20 & 86.65 & 93.21 & 89.70 & 87.86 & 87.60 & 72.52 & 85.68\\
 &  1.3B & 87.05 & 90.49 & 95.34 & 91.54 & 90.72 & 95.80 & 72.64 & 89.08\\
 &  2.7B & 88.73 & 91.79 & 95.44 & 91.54 & 93.52 & 95.20 & 75.30 & 90.22\\
 &  6.7B & 89.80 & 93.27 & 96.32 & 91.46 & 93.79 & 95.40 & 74.43 & 90.64\\
 &  13B  & 89.45 & 92.98 & 96.23 & 91.28 & 94.51 & 95.40 & 75.71 & 90.79\\
 &  30B  & 90.27 & 92.82 & 96.46 & 91.76 & 94.34 & 97.00 & 76.29 & 91.28\\
 &  66B  & 90.40 & 92.50 & 97.08 & 91.24 & 94.34 & 97.40 & 75.01 & 91.14\\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL+CSE\\OPT}
 &  1.3B& 88.62 & 91.89 & 95.49 & 91.64 & 94.29 & 94.80 & 73.22 & 89.99 \\
 &  2.7B& 88.40 & 92.16 & 95.57 & 91.51 & 94.12 & 95.20 & 74.09 & 90.15 \\
 &  6.7B& 89.60 & 92.05 & 95.91 & 91.09 & 94.78 & 95.80 & 75.71 & 90.71 \\
 &  13B & 89.20 & 92.40 & 95.92 & 90.86 & 93.74 & 95.40 & 73.10 & 90.09 \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL\\LLaMA}
& 7B & 90.40 & 92.90 & 96.88 & 91.57 & 95.11 & 95.40 & 75.13 & 91.06 \\
& 13B & 92.02 & 93.22 & 97.29 & 91.40 & 95.66 & 95.80 & 76.46 & 91.69 \\
& 30B & 91.64 & 93.27 & 97.10 & 91.86 & 95.99 & 95.80 & 78.43 & 92.01 \\
& 65B & 92.13 & 93.43 & 97.16 & 91.91 & 95.33 & 97.40 & 77.28 & 92.09 \\
\midrule
\multirowcell{2}[0pt][l]{PromptEOL+CSE\\LLaMA}
& 7B & 90.28 & 93.27 & 96.67 & 91.45 & 94.73 & 95.60 & 75.54 & 91.08 \\
& 13B& 91.22 & 93.22 & 96.83 & 91.52 & 94.89 & 95.80 & 74.26 & 91.11 \\
\bottomrule
\end{tabular}}
\caption{ Performances of our method with in-context learning and contrastive learning on transfer learning tasks.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
} \label{fig:transfer_icl_cse}
\end{table*}

\newpage

\section{Sentence Representation Methods}\label{apx:sentence_rep}
We supplemented detail results in Table 1 and 2 for different sentence representation methods.

\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
\multirowcell{8}[0pt][l]{OPT avg.}
& 125M& 44.27 & 50.38 & 44.95 & 62.39 & 55.52 & 45.39 & 53.24 & 50.88 \\
& 350M& 40.61 & 47.25 & 40.45 & 55.12 & 55.57 & 40.53 & 47.66 & 46.74 \\
& 1.3B& 45.12 & 54.01 & 46.52 & 62.94 & 55.96 & 46.31 & 54.32 & 52.17 \\
& 2.7B& 44.11 & 54.35 & 47.89 & 63.91 & 57.02 & 47.85 & 54.44 & 52.80 \\
& 6.7B& 43.61 & 51.69 & 45.86 & 60.11 & 55.41 & 45.42 & 54.93 & 51.00 \\
& 13B & 46.95 & 54.92 & 48.74 & 60.13 & 54.96 & 48.07 & 53.93 & 52.53 \\
& 30B & 43.93 & 52.44 & 46.04 & 58.80 & 55.15 & 47.13 & 53.46 & 50.99 \\
& 66B & 40.81 & 47.98 & 44.21 & 59.37 & 56.37 & 43.80 & 53.19 & 49.39 \\
\midrule
\multirowcell{8}[0pt][l]{OPT prompt}
& 125M & 56.25 & 71.61 & 58.62 & 63.47 & 70.29 & 59.77 & 63.23 & 63.32 \\
& 350M & 56.56 & 69.27 & 55.81 & 60.05 & 68.73 & 61.75 & 64.15 & 62.33 \\
& 1.3B & 60.26 & 75.64 & 62.93 & 70.63 & 76.52 & 67.31 & 65.95 & 68.46 \\
& 2.7B & 59.34 & 75.47 & 62.64 & 69.76 & 75.65 & 68.35 & 67.48 & 68.38 \\
& 6.7B & 55.20 & 76.91 & 62.53 & 69.41 & 76.39 & 67.33 & 65.86 & 67.66 \\
& 13B  & 49.60 & 75.43 & 61.58 & 67.33 & 75.53 & 65.98 & 63.79 & 65.61 \\
& 30B  & 46.69 & 72.42 & 58.00 & 67.52 & 72.98 & 64.77 & 65.66 & 64.01 \\
& 66B  & 50.21 & 69.65 & 56.78 & 70.20 & 73.37 & 64.31 & 66.93 & 64.49 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
& 125M & 59.90 & 71.55 & 60.93 & 70.76 & 72.83 & 67.89 & 65.14 & 67.00 \\
& 350M & 54.70 & 71.52 & 59.99 & 64.51 & 71.39 & 66.55 & 66.58 & 65.03 \\
& 1.3B & 64.59 & 79.06 & 68.46 & 78.88 & 78.64 & 73.22 & 69.41 & 73.18 \\
& 2.7B & 60.03 & 75.51 & 64.30 & 74.56 & 77.62 & 67.73 & 65.35 & 69.30 \\
& 6.7B & 60.91 & 80.05 & 67.65 & 75.49 & 80.11 & 72.91 & 67.57 & 72.10 \\
& 13B &  60.21 & 81.36 & 69.69 & 75.46 & 79.58 & 70.73 & 65.99 & 71.86 \\
& 30B &  59.99 & 80.52 & 69.80 & 75.20 & 78.03 & 73.57 & 69.87 & 72.43 \\
& 66B &  55.66 & 74.62 & 64.90 & 72.34 & 75.21 & 71.72 & 67.43 & 68.84 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on unsupervised datasets}}\\
\midrule
\multirowcell{6}[0pt][l]{PromptEOL\\OPT}
 & 125M & 76.53 & 85.56 & 79.75 & 85.43 & 81.17 & 84.32 & 79.04 & 81.69 \\
 & 350M & 75.96 & 85.51 & 81.32 & 86.50 & 81.42 & 85.24 & 80.35 & 82.33 \\
 & 1.3B & 79.01 & 89.26 & 84.10 & 88.30 & 84.62 & 87.71 & 80.52 & 84.79\\
 & 2.7B & 79.49 & 89.64 & 84.80 & 89.51 & 85.91 & 88.33 & 81.64 & 85.62\\
 & 6.7B & 80.14 & 90.02 & 84.94 & 89.78 & 85.84 & 88.75 & 81.29 & 85.82\\
 & 13B  & 80.20 & 90.24 & 85.34 & 89.52 & 85.90 & 88.56 & 82.06 & 85.97\\
\midrule
\multirowcell{6}[0pt][l]{OPT avg.}

 &125M  &74.08  & 82.70 & 77.76 & 83.65 & 79.74 & 82.43 & 78.55 & 79.84 \\
 &350M  &74.07  & 83.78 & 78.06 & 84.62 & 80.70 & 83.93 & 78.61 & 80.54 \\
 &1.3B  &75.38  & 84.99 & 80.34 & 86.10 & 81.49 & 84.35 & 79.98 & 81.80 \\
 & 2.7B &75.31 & 85.66 & 80.73 & 86.71 & 81.84 & 84.92 & 79.66 & 82.12\\
 & 6.7B &76.02 & 86.22 & 81.30 & 87.07 & 82.54 & 85.28 & 80.53 & 82.71 \\
 & 13B  & 75.86 & 86.32 & 80.73 & 86.25 & 82.13 & 85.55 & 79.62 & 82.35 \\
\midrule
\multirowcell{6}[0pt][l]{OPT prompt}
 & 125M & 76.05 & 85.24 & 79.82 & 85.27 & 81.30 & 84.56 & 79.09 & 81.62 \\
 & 350M & 76.28 & 86.01 & 80.96 & 86.13 & 81.87 & 85.33 & 79.73 & 82.33 \\
 & 1.3B & 78.56 & 89.21 & 84.21 & 88.71 & 84.17 & 87.39 & 81.16 & 84.77 \\
 & 2.7B & 78.89 & 89.21 & 84.43 & 89.43 & 85.75 & 88.07 & 81.40 & 85.31 \\
 & 6.7B & 78.66 & 89.81 & 84.45 & 89.70 & 85.71 & 88.63 & 81.79 & 85.54 \\
 & 13B  & 79.66 & 89.84 & 84.88 & 89.54 & 85.59 & 88.65 & 81.93 & 85.73 \\
\bottomrule
\end{tabular}
}
\caption{ Comparison of three sentence representation methods on STS tasks.
}\label{tab:sent_rep1}
\vspace{-30pt}
\end{table*}


\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
 &  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 &  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66B  & 90.96 & 93.40 & 97.01 & 91.93 & 95.22 & 96.40 & 75.25 & 91.45 \\
\midrule
\multirowcell{8}[0pt][l]{OPT avg.}
 &  125M & 80.63 & 86.41 & 93.91 & 87.85 & 86.22 & 92.60 & 71.83 & 85.64 \\
 &  350M & 80.73 & 85.16 & 93.42 & 87.26 & 86.11 & 87.80 & 69.57 & 84.29 \\
 &  1.3B & 85.89 & 90.04 & 95.71 & 90.10 & 91.38 & 94.20 & 72.99 & 88.62 \\
 &  2.7B & 87.55 & 90.76 & 95.78 & 90.26 & 91.71 & 94.40 & 68.00 & 88.35 \\
 &  6.7B & 87.93 & 91.07 & 96.58 & 90.65 & 92.70 & 96.20 & 72.17 & 89.61 \\
 &  13B  & 88.33 & 91.76 & 96.74 & 90.78 & 93.25 & 95.20 & 70.90 & 89.57 \\
 &  30B  & 88.54 & 92.11 & 96.85 & 90.61 & 93.74 & 94.40 & 70.72 & 89.57 \\
 &  66B  & 89.17 & 92.00 & 96.86 & 90.80 & 94.67 & 96.40 & 71.07 & 90.14 \\
\midrule
\multirowcell{8}[0pt][l]{OPT prompt}
 &  125M & 83.54 & 87.60 & 94.28 & 89.36 & 88.74 & 91.60 & 67.01 & 86.02 \\
 &  350M & 80.99 & 84.08 & 93.30 & 89.38 & 86.88 & 88.80 & 60.99 & 83.49 \\
 &  1.3B & 87.31 & 90.68 & 95.73 & 91.30 & 93.47 & 94.40 & 72.99 & 89.41 \\
 &  2.7B & 88.58 & 91.60 & 96.22 & 91.36 & 93.90 & 95.80 & 70.96 & 89.77 \\
 &  6.7B & 90.55 & 92.21 & 97.09 & 91.31 & 95.06 & 96.60 & 74.90 & 91.10 \\
 &  13B  & 90.45 & 92.66 & 96.85 & 91.57 & 95.44 & 96.00 & 74.55 & 91.07 \\
 &  30B  & 90.56 & 92.79 & 97.28 & 91.93 & 94.78 & 96.00 & 72.93 & 90.90 \\
 &  66B  & 90.95 & 92.48 & 97.27 & 91.72 & 95.55 & 95.80 & 75.30 & 91.30 \\
\bottomrule
\end{tabular}}
\caption{ Comparison of three sentence representation methods on STS tasks.
}\label{tab:sent_rep2}
\end{table*}



\end{document}
