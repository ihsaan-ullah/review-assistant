\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel et~al.(2018)Abel, Arumugam, Lehnert, and Littman]{abel2018state}
David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman.
\newblock State abstractions for lifelong reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10--19.
  PMLR, 2018.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47:\penalty0 235--256, 2002.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Besta et~al.(2024)Besta, Blach, Kubicek, Gerstenberger, Podstawski,
  Gianinazzi, Gajda, Lehmann, Niewiadomski, Nyczyk, et~al.]{besta2024graph}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal
  Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert
  Niewiadomski, Piotr Nyczyk, et~al.
\newblock Graph of thoughts: Solving elaborate problems with large language
  models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  17682--17690, 2024.

\bibitem[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner,
  Luko{\v{s}}i{\=u}t{\.e}, Askell, Jones, Chen, et~al.]{bowman2022measuring}
Samuel~R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott
  Heiner, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Amanda Askell, Andy Jones, Anna
  Chen, et~al.
\newblock Measuring progress on scalable oversight for large language models.
\newblock \emph{arXiv preprint arXiv:2211.03540}, 2022.

\bibitem[Chen et~al.(2024)Chen, Deng, Yuan, Ji, and Gu]{chen2024self}
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu.
\newblock Self-play fine-tuning converts weak language models to strong
  language models.
\newblock \emph{arXiv preprint arXiv:2401.01335}, 2024.

\bibitem[Chentanez et~al.(2004)Chentanez, Barto, and
  Singh]{chentanez2004intrinsically}
Nuttapong Chentanez, Andrew Barto, and Satinder Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 17, 2004.

\bibitem[Chern et~al.(2023)Chern, Zou, Li, Hu, Feng, Li, and Liu]{abel}
Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and
  Pengfei Liu.
\newblock Generative ai for math: Abel.
\newblock \url{https://github.com/GAIR-NLP/abel}, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Clouse(1996)]{clouse1996integrating}
Jeffery~Allen Clouse.
\newblock \emph{On integrating apprentice learning and reinforcement learning}.
\newblock University of Massachusetts Amherst, 1996.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[De~Waard et~al.(2016)De~Waard, Roijers, and Bakkes]{de2016monte}
Maarten De~Waard, Diederik~M Roijers, and Sander~CJ Bakkes.
\newblock Monte carlo tree search with options for general video game playing.
\newblock In \emph{2016 IEEE Conference on Computational Intelligence and Games
  (CIG)}, pp.\  1--8. IEEE, 2016.

\bibitem[Ding et~al.(2023)Ding, Zhang, Wang, Xu, Ma, Zhang, Qin, Rajmohan, Lin,
  and Zhang]{ding2023everything}
Ruomeng Ding, Chaoyun Zhang, Lu~Wang, Yong Xu, Minghua Ma, Wei Zhang, Si~Qin,
  Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang.
\newblock Everything of thoughts: Defying the law of penrose triangle for
  thought generation.
\newblock \emph{arXiv preprint arXiv:2311.04254}, 2023.

\bibitem[Feng et~al.(2023)Feng, Wan, Wen, Wen, Zhang, and
  Wang]{feng2023alphazero}
Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang.
\newblock Alphazero-like tree-search can guide large language model decoding
  and training.
\newblock \emph{arXiv preprint arXiv:2309.17179}, 2023.

\bibitem[Fu et~al.(2024)Fu, Sun, Nie, and Gao]{fu2024accelerating}
Yangqing Fu, Ming Sun, Buqing Nie, and Yue Gao.
\newblock Accelerating monte carlo tree search with probability tree state
  abstraction.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen,
  et~al.]{gou2023tora}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan,
  Weizhu Chen, et~al.
\newblock Tora: A tool-integrated reasoning agent for mathematical problem
  solving.
\newblock \emph{arXiv preprint arXiv:2309.17452}, 2023.

\bibitem[Guo et~al.(2024)Guo, Yao, Shen, Wei, Zhang, Wang, and
  Liu]{guo2024human}
Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang,
  and Yang Liu.
\newblock Human-instruction-free llm self-alignment with limited samples.
\newblock \emph{arXiv preprint arXiv:2401.06785}, 2024.

\bibitem[Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and
  Hu]{hao2023reasoning}
Shibo Hao, Yi~Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu.
\newblock Reasoning with language model is planning with world model.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  8154--8173, 2023.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset, 2021.

\bibitem[Hong et~al.(2023)Hong, Zhang, Pang, Yu, and Zhang]{hong2023closer}
Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang.
\newblock A closer look at the self-verification abilities of large language
  models in logical reasoning.
\newblock \emph{arXiv preprint arXiv:2311.07954}, 2023.

\bibitem[Huang et~al.(2023)Huang, Chen, Mishra, Zheng, Yu, Song, and
  Zhou]{huang2023large}
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu~Steven Zheng, Adams~Wei Yu,
  Xinying Song, and Denny Zhou.
\newblock Large language models cannot self-correct reasoning yet.
\newblock \emph{arXiv preprint arXiv:2310.01798}, 2023.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
  Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
  Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 3843--3857, 2022.

\bibitem[Li et~al.(2023)Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and
  Lewis]{li2023self}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy,
  Jason Weston, and Mike Lewis.
\newblock Self-alignment with instruction backtranslation.
\newblock \emph{arXiv preprint arXiv:2308.06259}, 2023.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Liu et~al.(2023)Liu, Cohen, Pasunuru, Choi, Hajishirzi, and
  Celikyilmaz]{liu2023making}
Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh
  Hajishirzi, and Asli Celikyilmaz.
\newblock Making ppo even better: Value-guided monte-carlo tree search
  decoding.
\newblock \emph{arXiv preprint arXiv:2309.15028}, 2023.

\bibitem[Long(2023)]{long2023large}
Jieyi Long.
\newblock Large language model guided tree-of-thought.
\newblock \emph{arXiv preprint arXiv:2305.08291}, 2023.

\bibitem[Luketina et~al.(2019)Luketina, Nardelli, Farquhar, Foerster, Andreas,
  Grefenstette, Whiteson, and Rockt{\"a}schel]{Luketina2019ASO}
Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob~N. Foerster, Jacob
  Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rockt{\"a}schel.
\newblock A survey of reinforcement learning informed by natural language.
\newblock \emph{ArXiv}, abs/1906.03926, 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:182952502}.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and
  Zhang]{wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo
  Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language
  models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[Madaan et~al.(2024)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt}
R~OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv}, pp.\  2303--08774, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Peng et~al.(2017)Peng, Li, Li, Gao, Celikyilmaz, Lee, and
  Wong]{peng2017composite}
Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee,
  and Kam-Fai Wong.
\newblock Composite task-completion dialogue policy learning via hierarchical
  deep reinforcement learning.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  2017.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and
  Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Ramamurthy et~al.(2022)Ramamurthy, Ammanabrolu, Brantley, Hessel,
  Sifa, Bauckhage, Hajishirzi, and Choi]{Ramamurthy2022IsRL}
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant{\'e} Brantley, Jack Hessel,
  Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
\newblock Is reinforcement learning (not) for natural language processing?:
  Benchmarks, baselines, and building blocks for natural language policy
  optimization.
\newblock \emph{ArXiv}, abs/2210.01241, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:252693405}.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and
  Leike]{saunders2022self}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan
  Ward, and Jan Leike.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Stechly et~al.(2024)Stechly, Valmeekam, and
  Kambhampati]{stechly2024self}
Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati.
\newblock On the self-verification limitations of large language models on
  reasoning and planning tasks.
\newblock \emph{arXiv preprint arXiv:2402.08115}, 2024.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and
  Gan]{sun2023principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David
  Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with
  minimal human supervision.
\newblock \emph{arXiv preprint arXiv:2305.03047}, 2023.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999{\natexlab{a}})Sutton, Precup, and
  Singh]{option_mcts}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1):\penalty0 181--211,
  1999{\natexlab{a}}.
\newblock ISSN 0004-3702.
\newblock \doi{https://doi.org/10.1016/S0004-3702(99)00052-1}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0004370299000521}.

\bibitem[Sutton et~al.(1999{\natexlab{b}})Sutton, Precup, and
  Singh]{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999{\natexlab{b}}.

\bibitem[Sutton(1984)]{sutton1984temporal}
Richard~Stuart Sutton.
\newblock \emph{Temporal credit assignment in reinforcement learning}.
\newblock University of Massachusetts Amherst, 1984.

\bibitem[Taylor et~al.(2014)Taylor, Carboni, Fachantidis, Vlahavas, and
  Torrey]{taylor2014reinforcement}
Matthew~E Taylor, Nicholas Carboni, Anestis Fachantidis, Ioannis Vlahavas, and
  Lisa Torrey.
\newblock Reinforcement learning agents providing advice in complex video
  games.
\newblock \emph{Connection Science}, 26\penalty0 (1):\penalty0 45--63, 2014.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,
  Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang,
  Creswell, Irving, and Higgins]{uesato2022solving}
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa
  Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and
  Kambhampati]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for llms on
  planning and reasoning about change).
\newblock \emph{arXiv preprint arXiv:2206.10498}, 2022.

\bibitem[Van~Eyck \& M{\"u}ller(2012)Van~Eyck and
  M{\"u}ller]{van2012revisiting}
Gabriel Van~Eyck and Martin M{\"u}ller.
\newblock Revisiting move groups in monte-carlo tree search.
\newblock In \emph{Advances in Computer Games: 13th International Conference,
  ACG 2011, Tilburg, The Netherlands, November 20-22, 2011, Revised Selected
  Papers 13}, pp.\  13--23. Springer, 2012.

\bibitem[Wang et~al.(2023)Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and
  Sui]{wang2023math}
Peiyi Wang, Lei Li, Zhihong Shao, RX~Xu, Damai Dai, Yifei Li, Deli Chen, Y~Wu,
  and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human
  annotations.
\newblock \emph{CoRR, abs/2312.08935}, 2023.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Xie et~al.(2024)Xie, Kawaguchi, Zhao, Zhao, Kan, He, and
  Xie]{xie2024self}
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James~Xu Zhao, Min-Yen Kan, Junxian He,
  and Michael Xie.
\newblock Self-evaluation guided beam search for reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and
  Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex
  instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan]{yao2024tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and
  Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and
  Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang,
  James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large
  language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Yuan et~al.(2024{\natexlab{a}})Yuan, Cui, Wang, Ding, Wang, Deng,
  Shan, Chen, Xie, Lin, et~al.]{yuan2024advancing}
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji
  Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et~al.
\newblock Advancing llm reasoning generalists with preference trees.
\newblock \emph{arXiv preprint arXiv:2404.02078}, 2024{\natexlab{a}}.

\bibitem[Yuan et~al.(2024{\natexlab{b}})Yuan, Pang, Cho, Sukhbaatar, Xu, and
  Weston]{yuan2024self}
Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing
  Xu, and Jason Weston.
\newblock Self-rewarding language models.
\newblock \emph{arXiv preprint arXiv:2401.10020}, 2024{\natexlab{b}}.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 15476--15488, 2022.

\bibitem[Zelikman et~al.(2024)Zelikman, Harik, Shao, Jayasiri, Haber, and
  Goodman]{zelikman2024quiet}
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and
  Noah~D Goodman.
\newblock Quiet-star: Language models can teach themselves to think before
  speaking.
\newblock \emph{arXiv preprint arXiv:2403.09629}, 2024.

\bibitem[Zhu et~al.(2024)Zhu, Zhang, Xie, and Su]{zhu2024deductive}
Tinghui Zhu, Kai Zhang, Jian Xie, and Yu~Su.
\newblock Deductive beam search: Decoding deducible rationale for
  chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2401.17686}, 2024.

\end{thebibliography}
