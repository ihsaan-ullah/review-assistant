\documentclass{article}


% % if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% % before loading neurips_2023


% % ready for submission
% % \usepackage{neurips_2023}
% \usepackage[final]{neurips_2024}


% % to compile a preprint version, e.g., for submission to arXiv, add add the
% % [preprint] option:
%     % \usepackage[preprint]{neurips_2023}


% % to compile a camera-ready version, add the [final] option, e.g.:
% %     \usepackage[final]{neurips_2023}

% % \usepackage[final]{neurips_2023}

% % to avoid loading the natbib package, add option nonatbib:
% %    \usepackage[nonatbib]{neurips_2023}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
% \usepackage{multirow}
% \usepackage{amsmath}
% \usepackage[ruled]{algorithm2e} % Required for the algorithm environment
% \usepackage{graphicx} % Required for inserting images
% \usepackage{tcolorbox}
% \usepackage{caption}
% \usepackage{menukeys}
% \usepackage{amsthm}
% % \usepackage[margin=1in]{geometry} % Adjust margins to ensure the table fits well
% \usepackage{tabularx} % Import tabularx for tables that can adjust their width automaticallyu
% % \usepackage{wraptable}
% \usepackage{wrapfig,lipsum,booktabs}
% \usepackage{enumitem}

% % \usepackage{algorithm}
% % \usepackage{algpseudocode}

% \usepackage{array}
% \usepackage{amssymb}

% \newcolumntype{M}[1]{>{\centering\arraybackslash}p{#1}}


% \newcommand{\model}[0]{\textsc{AlphaLLM}}
% \newcommand{\emcts}[0]{$\eta$\textsc{Mcts}}
% \newcommand{\prm}[0]{\texttt{PRM}}
% \newcommand{\orm}[0]{\texttt{ORM}}

% \newcommand{\ie}[0]{\emph{i.e., }}
% \newcommand{\ea}[0]{\emph{et al. }}
% \newcommand{\eg}[0]{\emph{e.g., }}
% \newcommand{\cf}[0]{\emph{cf. }}
% \newcommand{\etc}[0]{\emph{etc.}}
% \newcommand{\aka}[0]{\emph{a.k.a. }}
% \newcommand{\RN}[1]{%
% 	\textup{\lowercase\expandafter{\it \romannumeral#1}}%
% }
% \newcommand{\enterkey}[0]{{\scriptsize{\keys{\return}}}}

% \input{math_command}

% \title{Formatting Instructions For NeurIPS 2023}
\title{Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing}
% \title{A Preliminary exploration of LLMs Self-improvement with Search and Learning}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{Ye Tian\textsuperscript{1,2}\thanks{Equal Contribution; {\textdagger}Corresponding Author}, Baolin Peng\textsuperscript{1}\textsuperscript{$*$}, Linfeng Song\textsuperscript{1}\textsuperscript{$*$}, Lifeng Jin\textsuperscript{1}, Dian Yu\textsuperscript{1}, Lei Han\textsuperscript{2}, Haitao Mi\textsuperscript{1}\textsuperscript{\textdagger}, Dong Yu\textsuperscript{1}\\
% \textsuperscript{1}Tencent AI Lab, Bellevue, WA\\
% \textsuperscript{2}Tencent Robotics X \\
% \texttt{\{baolinpeng,lfsong,lifengjin,yudian,haitaomi,dyu\}@global.tencent.com} \\
% \texttt{\{yaptian,lxhan\}@tencent.com} \\\\
% }
\author{Ye Tian\textsuperscript{1,2}\thanks{Equal Contribution; {\textdagger}Corresponding Author}, Baolin Peng\textsuperscript{1}\footnotemark[1], Linfeng Song\textsuperscript{1}\footnotemark[1], Lifeng Jin\textsuperscript{1}, Dian Yu\textsuperscript{1}, Lei Han\textsuperscript{2}\\
\bf{Haitao Mi}\textsuperscript{1}\textsuperscript{\textdagger}, \bf{Dong Yu}\textsuperscript{1}\\
\textsuperscript{1}Tencent AI Lab, Bellevue, WA\\
\textsuperscript{2}Tencent Robotics X \\
\texttt{\{baolinpeng,lfsong,lifengjin,yudian,haitaomi,dyu\}@global.tencent.com} \\
\texttt{\{yaptian,lxhan\}@tencent.com} \\\\
}

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
% }


\begin{document}
% \footnotetext{*Equal Contribution; \textsuperscript{\textdagger}Corresponding Author}

\maketitle


\begin{abstract}

% Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving feedback loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis module, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, shedding lights on the promise of self-improvement in LLMs. 

% Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce \model{} for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, \model{} addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. \model{} is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that \model{} significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.

Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce \model{} for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, \model{} addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. \model{} is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that \model{} significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs. The code is available at \url{https://github.com/YeTianJHU/AlphaLLM}.
  
\end{abstract}

\section{Introduction}
\label{sec:intro}
Hello

\subsection{Some Section}
Some text before the wrapped table.
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam convallis libero in consequat.

\begin{table}[htb]
\label{tab:option_critic_example}
\centering % Often good to center the table content
\begin{tabular}{lcc}
\toprule
Method & \texttt{Acc} & \texttt{\#Rollout} \\
\midrule
emcts & 45.4 & 148 \\
w/o option & 44.1 & 198 \\
orm w/o tool  & 38.8 & 201 \\
\bottomrule
\end{tabular}
\caption{Comparison results of options formulation on MATH (example).}
\end{table}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lllll}
    \toprule
         Task & $\pi^\text{strong}_\text{base}$ & $\pi^\text{weak}_\text{base}$ & $\pi^\text{strong}$ & High-quality demonstrations \\
         \midrule
         Code  & Deepseek-7B-Coder & Pythia-1B & $\pi^\text{strong}_\text{base}$, SFT on GPT-4 T=1 & GPT-4 T=0 \\
         MATH & Deepseek-7B-Math & Pythia-1B & $\pi^\text{strong}_\text{base}$ & GPT-4 T=0 \\
         Critique & Deepseek-7B-Coder & Pythia-1B & $\pi^\text{strong}_\text{base}$, SFT + Iterated DPO & Reference critiques \\
         MMLU & Mistral-7B & Pythia-7B & Ground-truth labels & Ground-truth labels \\ \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{
        Summary of the models and policies used for each task. We
        study the sensitivity of the results to these choices in Appendix~\ref{sec:invariance}. 
        We rely on pre-trained models from the Deepseek~\citep{bi2024deepseek, shao2024deepseekmath} and Pythia~\citep{biderman2023pythia} families, as well as Mistral-7B \citep{jiang2023mistral} and GPT-4 \citep{openai2023gpt}.
    }
    \label{tab:demos}
\end{table}

\begin{table}[!htb]
\footnotesize
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{c|c|c}

    \toprule
    \texttt{Search Node} & \texttt{Example} & \texttt{Termination}  \\
    \midrule
    Token-level & $y_0 \rightarrow y_1 \rightarrow y_2 \rightarrow y_3 \rightarrow y_5 \rightarrow y_6 \rightarrow y_7 \rightarrow y_8$ &  token\\
    \midrule
    Sentence-level & $y_0 y_1 y_2$ \enterkey{}  $\rightarrow y_4 y_5 y_6$ \enterkey{} $\rightarrow y_7 y_8 y_9 y_{10}$ & new line\\
    \midrule
    Option-level & $y_0$  $\rightarrow y_1 y_2$ \enterkey{} $\rightarrow y_4 y_5 y_6$ \enterkey{} $y_7 y_8 y_9$ \enterkey{} $\rightarrow y_{10}$& termination function\\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Comparative illustration of token-level, sentence-level, and option-level MCTS search nodes. $y$ denotes a token sampled from the policy model. The arrow $\rightarrow$ represents the transition from one search node to the subsequent node within the search process.}
    \label{tab:option}
\end{table}


Some text after the wrapped table. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.

\subsection{Another Section with a standard table}
\begin{table}[!htb]
    \centering
    \begin{tabular}{l|c|c|c}
        Model               & Precision & Recall & ECE \\
        \hline
        Value Function      & 0.82      & 0.79   & 0.032 \\
        prm              & 0.62      & 0.90   & 0.375 \\
        \hline
    \end{tabular}
    \vspace{4mm}
    \caption{Performance comparison of the Value Function model and \prm{} on the GSM8K test set.}
    \label{table:ablation_critic_example}
\end{table}
% \section{Related Work}
% \label{sec:related_work}
% \input{related_work}

% \section{Preliminaries}
% \label{sec:pre}
% \input{preliminaries.tex}

% \section{\model{}}
% \label{sec:method}
% \input{method.tex}

% \section{Experiments}
% \label{sec:exp}
% \input{exp.tex}

% % \section{Limitations and Future Work}
% % \input{limitations}

% \section{Conclusion}
% \label{sec:con}
% \input{conclusion}

% \section{Limitations}
% \label{sec:limitations}

% \medskip
% \newpage
% \bibliography{neurips_2023}
% \bibliographystyle{iclr2021_conference}

\end{document}