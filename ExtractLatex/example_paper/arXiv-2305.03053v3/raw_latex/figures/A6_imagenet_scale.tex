% \begin{table}[t]
% \centering
% \tablestyle{5pt}{1.1}
% \begin{tabular}{y{56}x{20}x{28}|x{24}x{24}x{24}}
%     & FLOPs& Joint & \multicolumn{3}{c}{Per-Task (\%)}\\
%     Method & (G) & Acc (\%) & \modela{Task A} & \modelb{Task B} & Avg\\
%     \shline
%     \multicolumn{6}{c}{$1\times$ Width} \\
%     \hline
%     \gc{Ensemble} & \gc{8.22} & \gc{63.3\conf{4.9}} & \gc{74.3\conf{4.0}} & \gc{70.5\conf{3.2}} & \gc{72.4\conf{2.5}} \\
%     \default{{\bf \name{}}$_\text{50/50}$} & 4.11 & {8.6\conf{4.7}} & {12.4\conf{5.9}} & {14.7\conf{7.8}} & {13.5\conf{6.6}} \\
%     \default{{\bf \name{}}$_\text{37/50}$} & 4.92 & {33.1\conf{5.9}} & {41.8\conf{5.3}} & {42.3\conf{8.2}} & {42.0\conf{6.2}} \\
%     \default{{\bf \name{}}$_\text{22/50}$} & 6.39 & {55.8\conf{4.1}} & {65.9\conf{2.5}} & {64.1\conf{3.0}} & {65.0\conf{2.3}} \\
%     \default{{\bf \name{}}$_\text{10/50}$} & 7.43 & {60.9\conf{4.1}} & {70.7\conf{3.0}} & {69.0\conf{2.9}} & {69.9\conf{1.9}} \\
%     \hline
%     \multicolumn{6}{c}{$1.5\times$ Width} \\
%     \hline
%     \gc{Ensemble} & \gc{32.6} & \gc{67.8\conf{3.5}} & \gc{76.7\conf{4.1}} & \gc{72.6\conf{2.8}} & \gc{74.7\conf{2.5}} \\
%     \default{{\bf \name{}}$_\text{50/50}$} & 16.3 & {9.7\conf{6.9}} & {13.2\conf{9.5}} & {16.0\conf{10.0}} & {14.6\conf{9.3}} \\
%     \default{{\bf \name{}}$_\text{37/50}$} & 19.5 & {49.0\conf{2.5}} & {56.2\conf{4.2}} & {56.7\conf{2.1}} & {56.4\conf{2.8}} \\
%     \default{{\bf \name{}}$_\text{22/50}$} & 25.5 & {64.1\conf{2.7}} & {71.6\conf{2.3}} & {70.4\conf{2.3}} & {71.0\conf{1.8}} \\
%     \default{{\bf \name{}}$_\text{10/50}$} & 29.7 & {66.8\conf{3.2}} & {74.9\conf{3.5}} & {72.1\conf{2.3}} & {73.5\conf{2.1}} \\
% \end{tabular}

% \caption{\textbf{ImageNet-1k (200+200) Width Comparison.} We show how \name{}\ is able to make use of the extra model width when merging models together. For instance, merging 37 layers goes from 33\% joint accuracy with $1\times$ width to 49\% with $1.5\times$, while the ensemble only improves by 4\%. Because these models use cross-entropy, merging the entire network still results in poor performance.
% }
% \label{tab:imagenet200x5width}
% \end{table}

\begin{wrapfigure}{r}{0.48\linewidth}
\vspace{-10pt}
\centering
\resizebox{\linewidth}{!}{
    \tablestyle{5pt}{1.1}
    {\renewcommand\conf[1]{}
    \tablestyle{5pt}{1.1}
    \begin{tabular}{y{56}x{40}|x{30}x{30}x{30}x{30}}
        & & \multicolumn{4}{c}{Accuracies (\%)}\\
        Method & FLOPs (G) & Joint & \modela{Task A} & \modelb{Task B} & Avg \\
    \shline
    \multicolumn{6}{c}{$1\times$ Width} \\
    \hline
    \gc{Ensemble} & \gc{8.22} & \gc{63.3\conf{4.9}} & \gc{74.3\conf{4.0}} & \gc{70.5\conf{3.2}} & \gc{72.4\conf{2.5}} \\
    \default{{\bf \name{}}$_\text{50/50}$} & 4.11 & {8.6\conf{4.7}} & {12.4\conf{5.9}} & {14.7\conf{7.8}} & {13.5\conf{6.6}} \\
    \default{{\bf \name{}}$_\text{37/50}$} & 4.92 & {33.1\conf{5.9}} & {41.8\conf{5.3}} & {42.3\conf{8.2}} & {42.0\conf{6.2}} \\
    \default{{\bf \name{}}$_\text{22/50}$} & 6.39 & {55.8\conf{4.1}} & {65.9\conf{2.5}} & {64.1\conf{3.0}} & {65.0\conf{2.3}} \\
    \default{{\bf \name{}}$_\text{10/50}$} & 7.43 & {60.9\conf{4.1}} & {70.7\conf{3.0}} & {69.0\conf{2.9}} & {69.9\conf{1.9}} \\
    \hline
    \multicolumn{6}{c}{$1.5\times$ Width} \\
    \hline
    \gc{Ensemble} & \gc{32.6} & \gc{67.8\conf{3.5}} & \gc{76.7\conf{4.1}} & \gc{72.6\conf{2.8}} & \gc{74.7\conf{2.5}} \\
    \default{{\bf \name{}}$_\text{50/50}$} & 16.3 & {9.7\conf{6.9}} & {13.2\conf{9.5}} & {16.0\conf{10.0}} & {14.6\conf{9.3}} \\
    \default{{\bf \name{}}$_\text{37/50}$} & 19.5 & {49.0\conf{2.5}} & {56.2\conf{4.2}} & {56.7\conf{2.1}} & {56.4\conf{2.8}} \\
    \default{{\bf \name{}}$_\text{22/50}$} & 25.5 & {64.1\conf{2.7}} & {71.6\conf{2.3}} & {70.4\conf{2.3}} & {71.0\conf{1.8}} \\
    \default{{\bf \name{}}$_\text{10/50}$} & 29.7 & {66.8\conf{3.2}} & {74.9\conf{3.5}} & {72.1\conf{2.3}} & {73.5\conf{2.1}} \\
\end{tabular}
    }
}
\captionof{table}{\textbf{ImageNet-1k (200+200) Width Comparison.} We show how \name{}\ is able to make use of the extra model width when merging models together. For instance, merging 37 layers goes from 33\% joint accuracy with $1\times$ width to 49\% with $1.5\times$, while the ensemble only improves by 4\%. These models use cross-entropy, so merging the entire network results in poor performance.
% all pairs (2-way merging) and per-task accuracy for each head (4-way merging).
% We compare to our strong baseline as \cite{ainsworth2022git} doesn't support models with different outputs.
}
\label{tab:imagenet200x5width}
% \end{table}
\vspace{-20pt}
\end{wrapfigure}
