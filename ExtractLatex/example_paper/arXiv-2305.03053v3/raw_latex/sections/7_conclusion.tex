% \vspace{-0.5em}
\section{Conclusion}
% \vspace{-0.4em}
In this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks \textit{without additional training}.
We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities.
We introduce \name{}, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings, comprehensively analyzing each.
% We hope \name{}\ can serve as a strong starting point for practical applications of merging models trained on different tasks.


% In this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks \textit{without additional training}.
% We show experimentally how prior work falls short in this setting and posit that this is due to not merging features \textit{within models} as well as merging \textit{every} layer in the model at once. 
% We then introduce \name{}, a generalized framework for merging models that deals with these issues and find it to significantly outperform both prior work \cite{ainsworth2022git} and our own strong baseline on a number of difficult model merging settings. 
% We then analyze the behavior of our method and find that at smaller model capacities, it performs similarly to permutation-based methods, but can perform much better as the model capacity increases.
% We hope \name{}\ can serve as a strong starting point for practical applications of merging models trained on different tasks.
