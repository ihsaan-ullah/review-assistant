% L2M_PROCESSED_BBL_V1_DO_NOT_EDIT_MANUALLY_BELOW_THIS_LINE
\begin{thebibliography}{}
\bibitem{AbsMahSep2008} P.-A. Absil, R.~Mahony, and R.~Sepulchre \newblock \emph{Optimization Algorithms on Matrix Manifolds} \newblock Princeton University Press, Princeton, NJ, 2008. \newblock ISBN 978-0-691-13298-3. \newblock \textbf{Abstract:} Many problems in the sciences and engineering can be rephrased as optimization problems on matrix search spaces endowed with a so-called manifold structure. This book shows how to exploit the special structure of such problems to develop efficient numerical algorithms. It places careful emphasis on both the numerical formulation of the algorithm and its differential geometric abstraction--illustrating how good algorithms draw equally from the insights of differential geometry, optimization, and numerical analysis. Two more theoretical chapters provide readers with the background in differential geometry necessary to algorithmic development. In the other chapters, several well-known optimization methods such as steepest descent and conjugate gradients are generalized to abstract manifolds. The book provides a generic development of each of these methods, building upon the material of the geometric chapters. It then guides readers through the calculations that turn these geometrically formulated methods into concrete numerical algorithms. The state-of-the-art algorithms given as examples are competitive with the best existing algorithms for a selection of eigenspace problems in numerical linear algebra. Optimization Algorithms on Matrix Manifolds offers techniques with broad applications in linear algebra, signal processing, data mining, computer vision, and statistical analysis. It can serve as a graduate-level textbook and will be of interest to applied mathematicians, engineers, and computer scientists. \newblock (@AbsMahSep2008)

\bibitem{aubry2024transformeralignmentlargelanguage} Murdock Aubry, Haoming Meng, Anton Sugolov, and Vardan Papyan \newblock {Transformer Alignment in Large Language Models} \newblock \emph{arXiv preprint arXiv:2407.07810}, 2024. \newblock \textbf{Abstract:} Large Language Models (LLMs) have made significant strides in natural language processing, and a precise understanding of the internal mechanisms driving their success is essential. In this work, we analyze the trajectories of token embeddings as they pass through transformer blocks, linearizing the system along these trajectories through their Jacobian matrices. By examining the relationships between these block Jacobians, we uncover the phenomenon of \textbackslash\{\}textbf\{transformer block coupling\} in a multitude of LLMs, characterized by the coupling of their top singular vectors across tokens and depth. Our findings reveal that coupling \textbackslash\{\}textit\{positively correlates\} with model performance, and that this relationship is stronger than with other hyperparameters such as parameter count, model depth, and embedding dimension. We further investigate how these properties emerge during training, observing a progressive development of coupling, increased linearity, and layer-wise exponential growth in token trajectories. Additionally, experiments with Vision Transformers (ViTs) corroborate the emergence of coupling and its relationship with generalization, reinforcing our findings in LLMs. Collectively, these insights offer a novel perspective on token interactions in transformers, opening new directions for studying their mechanisms as well as improving training and generalization. \newblock (@aubry2024transformeralignmentlargelanguage)

\bibitem{ba2016layernormalization} Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton \newblock Layer normalization \newblock \emph{arXiv preprint arXiv:1607.06450}, 2016. \newblock \textbf{Abstract:} Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques. \newblock (@ba2016layernormalization)

\bibitem{bricken2023monosemanticity} Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah~E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah \newblock Towards monosemanticity: Decomposing language models with dictionary learning \newblock \emph{Transformer Circuits Thread}, 2023. \newblock \url{https://transformer-circuits.pub/2023/monosemantic-features/index.html}. \newblock \textbf{Abstract:} Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Monet. \newblock (@bricken2023monosemanticity)

\bibitem{geshkovski2024emergenceclustersselfattentiondynamics} Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet \newblock The emergence of clusters in self-attention dynamics \newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, 2024. \newblock \textbf{Abstract:} Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. Cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by Transformers. Using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. [VSP'17] that leaders appear in a sequence of tokens when processed by Transformers. \newblock (@geshkovski2024emergenceclustersselfattentiondynamics)

\bibitem{gruver2024largelanguagemodelszeroshot} Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew~G Wilson \newblock Large language models are zero-shot time series forecasters \newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, 2024. \newblock \textbf{Abstract:} By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF. \newblock (@gruver2024largelanguagemodelszeroshot)

\bibitem{gurnee2024languagemodelsrepresentspace} Wes Gurnee and Max Tegmark \newblock Language models represent space and time \newblock \emph{arXiv preprint arXiv:2310.02207}, 2023. \newblock \textbf{Abstract:} The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model. \newblock (@gurnee2024languagemodelsrepresentspace)

\bibitem{jiang2023mistral7b} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al \newblock {Mistral 7B} \newblock \emph{arXiv preprint arXiv:2310.06825}, 2023. \newblock \textbf{Abstract:} We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. \newblock (@jiang2023mistral7b)

\bibitem{jiang2024originslinearrepresentationslarge} Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch \newblock On the origins of linear representations in large language models \newblock \emph{arXiv preprint arXiv:2403.03867}, 2024. \newblock \textbf{Abstract:} Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights. \newblock (@jiang2024originslinearrepresentationslarge)

\bibitem{liu2024llmslearngoverningprinciples} Toni~JB Liu, Nicolas Boull{\'e}, Rapha{\"e}l Sarfati, and Christopher~J Earls \newblock {LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law} \newblock \emph{arXiv preprint arXiv:2402.00795}, 2024. \newblock \textbf{Abstract:} Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. We study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs. \newblock (@liu2024llmslearngoverningprinciples)

\bibitem{Markov_2006} A.~A. Markov \newblock An example of statistical investigation of the text eugene onegin concerning the connection of samples in chains \newblock \emph{Science in Context}, 19\penalty0 (4):\penalty0 591–600, 2006. \newblock \doi{10.1017/S0269889706001074}. \newblock (@Markov\_2006)

\bibitem{marks2024geometrytruthemergentlinear} Samuel Marks and Max Tegmark \newblock {The geometry of truth: Emergent linear structure in large language model representations of true/false datasets} \newblock \emph{arXiv preprint arXiv:2310.06824}, 2023. \newblock \textbf{Abstract:} Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs. \newblock (@marks2024geometrytruthemergentlinear)

\bibitem{meta2024llama3_2} MetaAI \newblock Llama 3.2 model card \newblock \url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md}, 2024. \newblock Accessed: 2024-09-25. \newblock \textbf{Abstract:} Language models (LMs) are no longer restricted to ML community, and instruction-tuned LMs have led to a rise in autonomous AI agents. As the accessibility of LMs grows, it is imperative that an understanding of their capabilities, intended usage, and development cycle also improves. Model cards are a popular practice for documenting detailed information about an ML model. To automate model card generation, we introduce a dataset of 500 question-answer pairs for 25 ML models that cover crucial aspects of the model, such as its training configurations, datasets, biases, architecture details, and training resources. We employ annotators to extract the answers from the original paper. Further, we explore the capabilities of LMs in generating model cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses. We posit that our dataset can be used to train models to automate the generation of model cards from paper text and reduce human effort in the model card curation process. The complete dataset is available on https://osf.io/hqt7p/?view\_only=3b9114e3904c4443bcd9f5c270158d37 \newblock (@meta2024llama3\_2)

\bibitem{pavliotis2014stochastic} Grigorios~A Pavliotis \newblock \emph{Stochastic processes and applications}, volume~60 \newblock Springer, 2014. \newblock \textbf{Abstract:} This volume contains the contributions to a conference that is among the most important meetings in financial mathematics. Serving as a bridge between probabilists in Japan (called the Ito School and known for its highly sophisticated mathematics) and mathematical finance and financial engineering, the conference elicits the very highest quality papers in the field of financial mathematics. \newblock (@pavliotis2014stochastic)

\bibitem{PRAVEEN2023115971} Harshwardhan Praveen, Nicolas Boullé, and Christopher Earls \newblock Principled interpolation of green’s functions learned from data \newblock \emph{Comput. Methods Appl. Mech. Eng.}, 409:\penalty0 115971, 2023. \newblock \textbf{Abstract:} Recent advances in diffusion models have demonstrated their remarkable ability to capture complex image distributions, but the geometric properties of the learned data manifold remain poorly understood. We address this gap by introducing a score-based Riemannian metric that leverages the Stein score function from diffusion models to characterize the intrinsic geometry of the data manifold without requiring explicit parameterization. Our approach defines a metric tensor in the ambient space that stretches distances perpendicular to the manifold while preserving them along tangential directions, effectively creating a geometry where geodesics naturally follow the manifold's contours. We develop efficient algorithms for computing these geodesics and demonstrate their utility for both interpolation between data points and extrapolation beyond the observed data distribution. Through experiments on synthetic data with known geometry, Rotated MNIST, and complex natural images via Stable Diffusion, we show that our score-based geodesics capture meaningful transformations that respect the underlying data distribution. Our method consistently outperforms baseline approaches on perceptual metrics (LPIPS) and distribution-level metrics (FID, KID), producing smoother, more realistic image transitions. These results reveal the implicit geometric structure learned by diffusion models and provide a principled way to navigate the manifold of natural images through the lens of Riemannian geometry. \newblock (@PRAVEEN2023115971)

\bibitem{gutenberg_project} {Project Gutenberg} \newblock {Project Gutenberg} \newblock \url{https://www.gutenberg.org/about/}, 2024. \newblock Accessed: 2024-09-07. \newblock \textbf{Abstract:} Project Gutenberg is lauded as one of the earliest digitisation initiatives, a mythology that Michael Hart, its founder perpetuated through to his death in 2011. In this Element, the author re-examines the extant historical evidence to challenge some of Hart's bolder claims and resituates the significance of Project Gutenberg in relation to broader trends in online document delivery and digitisation in the latter half of the twentieth century, especially in the World Wide Web's first decade (the 1990s). Through this re-appraisal, the author instead suggests that Hart's Project is significant as an example of what Millicent Weber has termed a “digital publishing collective” whereby a group of volunteers engage in producing content and that process is as meaningful as the final product. \newblock (@gutenberg\_project)

\bibitem{radford2019language} Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al \newblock Language models are unsupervised multitask learners \newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019. \newblock \textbf{Abstract:} While large language models (LLMs) have revolutionized natural language processing with their task-agnostic capabilities, visual generation tasks such as image translation, style transfer, and character customization still rely heavily on supervised, task-specific datasets. In this work, we introduce Group Diffusion Transformers (GDTs), a novel framework that unifies diverse visual generation tasks by redefining them as a group generation problem. In this approach, a set of related images is generated simultaneously, optionally conditioned on a subset of the group. GDTs build upon diffusion transformers with minimal architectural modifications by concatenating self-attention tokens across images. This allows the model to implicitly capture cross-image relationships (e.g., identities, styles, layouts, surroundings, and color schemes) through caption-based correlations. Our design enables scalable, unsupervised, and task-agnostic pretraining using extensive collections of image groups sourced from multimodal internet articles, image galleries, and video frames. We evaluate GDTs on a comprehensive benchmark featuring over 200 instructions across 30 distinct visual generation tasks, including picture book creation, font design, style transfer, sketching, colorization, drawing sequence generation, and character customization. Our models achieve competitive zero-shot performance without any additional fine-tuning or gradient updates. Furthermore, ablation studies confirm the effectiveness of key components such as data scaling, group size, and model design. These results demonstrate the potential of GDTs as scalable, general-purpose visual generation systems. \newblock (@radford2019language)

\bibitem{ruoss2024grandmasterlevelchesssearch} Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li~Kevin Wenliang, Elliot Catt, John Reid, and Tim Genewein \newblock Grandmaster-level chess without search \newblock 2024. \newblock URL \url{https://arxiv.org/abs/2402.04494}. \newblock \textbf{Abstract:} This paper uses chess, a landmark planning problem in AI, to assess transformers' performance on a planning task where memorization is futile \$\textbackslash\{\}unicode\{x2013\}\$ even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points) provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research. \newblock (@ruoss2024grandmasterlevelchesssearch)

\bibitem{sharma2023understandingsycophancylanguagemodels} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da~Yan, Miranda Zhang, and Ethan Perez \newblock Towards understanding sycophancy in language models \newblock 2023. \newblock URL \url{https://arxiv.org/abs/2310.13548}. \newblock \textbf{Abstract:} Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses. \newblock (@sharma2023understandingsycophancylanguagemodels)

\bibitem{song2024uncoveringhiddengeometrytransformers} Jiajun Song and Yiqiao Zhong \newblock Uncovering hidden geometry in transformers via disentangling position and context 2024. \newblock URL \url{https://arxiv.org/abs/2310.04861}. \newblock \textbf{Abstract:} Transformers are widely used to extract semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor \$\textbackslash\{\}boldsymbol\{h\} \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}\textasciicircum{}\{C \textbackslash\{\}times T \textbackslash\{\}times d\}\$. Given embedding vector \$\textbackslash\{\}boldsymbol\{h\}\_\{c,t\} \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}\textasciicircum{}d\$ at sequence position \$t \textbackslash\{\}le T\$ in a sequence (or context) \$c \textbackslash\{\}le C\$, extracting the mean effects yields the decomposition \textbackslash\{\}[ \textbackslash\{\}boldsymbol\{h\}\_\{c,t\} = \textbackslash\{\}boldsymbol\{\textbackslash\{\}mu\} + \textbackslash\{\}mathbf\{pos\}\_t + \textbackslash\{\}mathbf\{ctx\}\_c + \textbackslash\{\}mathbf\{resid\}\_\{c,t\} \textbackslash\{\}] where \$\textbackslash\{\}boldsymbol\{\textbackslash\{\}mu\}\$ is the global mean vector, \$\textbackslash\{\}mathbf\{pos\}\_t\$ and \$\textbackslash\{\}mathbf\{ctx\}\_c\$ are the mean vectors across contexts and across positions respectively, and \$\textbackslash\{\}mathbf\{resid\}\_\{c,t\}\$ is the residual vector. For popular transformer architectures and diverse text datasets, empirically we find pervasive mathematical structure: (1) \$(\textbackslash\{\}mathbf\{pos\}\_t)\_\{t\}\$ forms a low-dimensional, continuous, and often spiral shape across layers, (2) \$(\textbackslash\{\}mathbf\{ctx\}\_c)\_c\$ shows clear cluster structure that falls into context topics, and (3) \$(\textbackslash\{\}mathbf\{pos\}\_t)\_\{t\}\$ and \$(\textbackslash\{\}mathbf\{ctx\}\_c)\_c\$ are mutually nearly orthogonal. We argue that smoothness is pervasive and beneficial to transformers trained on languages, and our decomposition leads to improved model interpretability. \newblock (@song2024uncoveringhiddengeometrytransformers)

\bibitem{templeton2024scaling} Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas~L Turner, Callum McDougall, Monte MacDiarmid, C.~Daniel Freeman, Theodore~R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan \newblock Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet \newblock \emph{Transformer Circuits Thread}, 2024. \newblock URL \url{https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}. \newblock (@templeton2024scaling)

\bibitem{touvron2023llama2openfoundation} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al \newblock Llama 2: Open foundation and fine-tuned chat models \newblock \emph{arXiv preprint arXiv:2307.09288}, 2023. \newblock \textbf{Abstract:} In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. \newblock (@touvron2023llama2openfoundation)

\bibitem{valeriani2023geometryhiddenrepresentationslarge} Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga \newblock The geometry of hidden representations of large transformer models 2023. \newblock URL \url{https://arxiv.org/abs/2302.00294}. \newblock \textbf{Abstract:} Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be observed across many models trained on diverse datasets. Based on our findings, we point out an explicit strategy to identify, without supervision, the layers that maximize semantic content: representations at intermediate layers corresponding to a relative minimum of the ID profile are more suitable for downstream learning tasks. \newblock (@valeriani2023geometryhiddenrepresentationslarge)

\bibitem{JMLR:v9:vandermaaten08a} Laurens van~der Maaten and Geoffrey Hinton \newblock Visualizing data using t-sne \newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (86):\penalty0 2579--2605, 2008. \newblock URL \url{http://jmlr.org/papers/v9/vandermaaten08a.html}. \newblock (@JMLR:v9:vandermaaten08a)

\bibitem{vaswani2023attentionneed} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin \newblock {Attention is All you Need} \newblock In \emph{Advances in Neural Information Processing Systems}, volume~30, 2017. \newblock \textbf{Abstract:} The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \newblock (@vaswani2023attentionneed)

\bibitem{vig2019visualizingattentiontransformerbasedlanguage} Jesse Vig \newblock Visualizing attention in transformer-based language representation models 2019. \newblock URL \url{https://arxiv.org/abs/1904.02679}. \newblock \textbf{Abstract:} We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior. \newblock (@vig2019visualizingattentiontransformerbasedlanguage)

\bibitem{wolf2020huggingfacestransformersstateoftheartnatural} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush \newblock Huggingface's transformers: State-of-the-art natural language processing \newblock 2020. \newblock URL \url{https://arxiv.org/abs/1910.03771}. \newblock \textbf{Abstract:} Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash\{\}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash\{\}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash\{\}url\{https://github.com/huggingface/transformers\}. \newblock (@wolf2020huggingfacestransformersstateoftheartnatural)

\bibitem{zhang2023sirenssongaiocean} Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, Longyue Wang, Anh~Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi \newblock Siren's song in the ai ocean: A survey on hallucination in large language models 2023. \newblock URL \url{https://arxiv.org/abs/2309.01219}. \newblock \textbf{Abstract:} While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research. \newblock (@zhang2023sirenssongaiocean)

\bibitem{zhou2024llmsreasonmusicevaluation} Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu~Wang, Emmanouil Benetos, Wei Xue, and Yike Guo \newblock Can llms "reason" in music? an evaluation of llms' capability of music understanding and generation \newblock 2024. \newblock URL \url{https://arxiv.org/abs/2407.21531}. \newblock \textbf{Abstract:} Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process. This study conducts a thorough investigation of LLMs' capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs' responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not intrinsically obtained by LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians. \newblock (@zhou2024llmsreasonmusicevaluation)
\end{thebibliography}