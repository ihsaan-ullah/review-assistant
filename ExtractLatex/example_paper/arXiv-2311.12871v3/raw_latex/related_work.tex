\section{Related Work}\label{app:sec:related}

\paragraph{Generalist agents.} The AI community has witnessed the rising generalist models in both vision~\citep{lu2023unified,wang2023images,kirillov2023segment} and language~\citep{openai2022chatgpt,openai2023gpt4} domains. A generalist agent requires additional embodiment knowledge to interact with the environment and complete embodied acting tasks. Existing efforts towards generalist agents include: grounded reasoning and task planning in the real world~\citep{ahn2022can,huang2022inner}, skill generalization in open-world environment~\citep{fan2022minedojo,cai2023open,wang2023describe,wang2023voyager,cai2023groot,jxma_llm_vla_vlm_mas_multiagent_2023}, general robotic manipulation~\citep{brohan2022rt,jiang2023vima,gong2023arnold}, and unified vision-language-action (VLA) models such as Gato~\citep{reed2022generalist}, PaLM-E~\citep{driess2023palm}, EmbodiedGPT~\citep{mu2023embodiedgpt}, and RT-2~\citep{brohan2023rt}. \agent belongs to the \ac{vla} model, however, its goal is to build a generalist agent that can understand the real 3D world beyond 2D images, which is absent in existing works.

\paragraph{Multi-modal instruction tuning.} Pre-trained LLMs demonstrated practical for solving vision-language tasks~\citep{tsimpoukelli2021multimodal,alayrac2022flamingo,guo2023images,li2023blip,jxma_vlm_multimodal_2023}. Meanwhile, the instruction-tuning paradigm exhibited strong zero-shot generalization in NLP tasks~\citep{wei2022finetuned,sanh2022multitask,ouyang2022training,chung2022scaling}. The two streams merged into instruction-tuned LVLMs~\citep{liu2023visual,zhu2023minigpt,ye2023mplug,gao2023llama,li2023otter,gong2023multimodal,dai2023instructblip}. Despite the burst, these models are confined to 2D visual modalities, \eg, image or video. Concurrent works~\citep{yin2023lamm,hong20233d,wang2023chat,xu2023pointllm} extend to 3D vision tasks, but these models either lack the acting capability or unified efficient architecture.

\paragraph{Grounded 3D scene understanding.}
One key obstacle to building \agent is grounding the 3D world with natural languages. There exist diverse methods of grounded scene understanding, \eg, spatial relation modeling \citep{zhao20213dvg,chen2022language,zhu20233d} and fine-grained open-scene understanding \citep{peng2023openscene,kerr2023lerf}. However, due to data scarcity, how to utilize \ac{llm} to ground the 3D scene is rarely explored. Recently, 3D-LLM \citep{hong20233d} leverages multi-view images and Chat-3D~\citep{wang2023chat} uses object-centric point clouds to enable the \ac{llm} with 3D grounding. In this work, we devise both 2D and 3D encoders for grounding various visual representations and employ LoRA~\citep{hu2022lora} to efficiently fine-tune the \ac{llm}.

\paragraph{3D data prompting from LLMs.} LLMs exhibit extraordinary capabilities of text generation and serve as a source for collecting diverse instruction-following data~\citep{wang2023self,alpaca,peng2023instruction}. However, the lack of access to visual modalities makes it troublesome to collect visual instruction-tuning data. To address this issue, existing methods provide bounding boxes~\citep{liu2023visual} and add dense captions~\citep{li2023mimic,liu2023aligning} as image descriptions or directly use off-the-shelf \ac{lvlm}~\citep{zhu2023chatgpt,luo2023scalable} to help collect such data. Unlike concurrent attempts~\citep{yin2023lamm,hong20233d,wang2023chat} in collecting 3D instruction-tuning data, our approach features a scene-graph-based prompting and refinement method to prompt and correct the data.
