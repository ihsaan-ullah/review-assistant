@string {S = "Science"}
@string {N = "Nature"}
@string {C = "Cell"}
@string {SAdv = "Science Advances"}
@string {NComm = "Nature Communications"}
@string {PNAS = "Proceedings of the National Academy of Sciences (PNAS)"}
% CV
@string {PAMI = "Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"}
@string {IJCV = "International Journal of Computer Vision (IJCV)"}
@string {CVIU = "Computer Vision and Image Understanding (CVIU)"}
@string {TIP = "Transactions on Image Processing (TIP)"}
@string {CVPR = "Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string {ICCV = "International Conference on Computer Vision (ICCV)"}
@string {ECCV = "European Conference on Computer Vision (ECCV)"}
@string {ACCV = "Asian Conference on Computer Vision (ACCV)"}
@string {BMVC = "British Machine Vision Conference (BMVC)"}
@string {WACV = "Proceedings of Winter Conference on Applications of Computer Vision (WACV)"}
@string {ICIP = "IEEE International Conference on Image Processing (ICIP)"}
% ML
@string {JMLR = "Journal of Machine Learning Research (JMLR)"}
@string {TMLR = "Transactions on Machine Learning Research (TMLR)"}
@string {NIPS = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string {NIPSD = "Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS Datasets and Benchmarks)"}
@string {ICML = "International Conference on Machine Learning (ICML)"}
@string {ICLR = "International Conference on Learning Representations (ICLR)"}
% AI
@string {JAIR = "Journal of Artificial Intelligence Research"}
@string {AIJ = "Artificial Intelligence"}
@string {AAAI = "AAAI Conference on Artificial Intelligence (AAAI)"}
@string {IJCAI = "International Joint Conference on Artificial Intelligence (IJCAI)"}
@string {AISTATS = "International Conference on Artificial Intelligence and Statistics (AISTATS)"}
% Robotics
@string {TRO = "Transactions on Robotics (T-RO)"}
@string {IJRR = "International Journal of Robotics Research (IJRR)"}
@string {RA-L = "IEEE Robotics and Automation Letters (RA-L)"}
@string {IROS = "International Conference on Intelligent Robots and Systems (IROS)"}
@string {ICRA = "International Conference on Robotics and Automation (ICRA)"}
@string {CoRL = "Conference on Robot Learning (CoRL)"}
@string {RSS = "Robotics: Science and Systems (RSS)"}
@string {ROMAN = "International Symposium on Robot and Human Interactive Communication (RO-MAN)"}
@string {HRI = "ACM/IEEE International Conference on Human-Robot Interaction (HRI)"}
% NLP
@string {TACL = "Transactions of the Association for Computational Linguistics (TACL)"}
@string {ACL = "Annual Meeting of the Association for Computational Linguistics (ACL)"}
@string {ACLF = "Findings of the Association for Computational Linguistics (ACL-IJCNLP Findings)"}
@string {EMNLP = "Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)"}
@string {EMNLPF = "Findings of Annual Conference on Empirical Methods in Natural Language Processing (EMNLP Findings)"}
@string {NAACL = "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"}
@string {COLING = "International Conference on Computational Linguistics (COLING)"}
% Psychology
@string {BBS = "Behavioral and Brain Sciences"}
@string {TiCS = "Trends in Cognitive Sciences"}
@string {AnnPsy = "Annual Review of Psychology"}
@string {AnnDevPsy = "Annual Review of Developmental Psychology"}
@string {JEP = "Journal of Experimental Psychology"}
@string {PsyRev = "Psychological Review"}
@string {PsySci = "Psychological Science"}
@string {PsyBul = "Psychological Bulletin"}
@string {CSJ = "Cognitive Science"}
@string {Cog = "Cognition"}
@string {CogSci = "Annual Meeting of the Cognitive Science Society (CogSci)"}
% Graphics
@string {TOG = "ACM Transactions on Graphics (TOG)"}
@string {TVCG = "IEEE Transactions on Visualization and Computer Graph (TVCG)"}
@string {SCA = "ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA)"}
@string {ThreeDV = "International Conference on 3D Vision (3DV)"}
@string {CGF = "Computer Graphics Forum"}
% HCI
@string {CHI = "ACM Conference on Human Factors in Computing Systems (CHI)"}
@string {UbiComp = "ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"}
@string {UIST = "ACM Symposium on User Interface Software and Technology (UIST)"}
% Others
@string {AnnStat = "The Annals of Statistics"}
@string {JASA = "Journal of the American Statistical Association"}
@string {AAMAS = "International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"}
@string {KDD = "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)"}
@string {ITSC = "IEEE Intelligent Transportation Systems Conference (ITSC)"}



@article{majumdar2023we,
  title={Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?},
  author={Majumdar, Arjun and Yadav, Karmesh and Arnaud, Sergio and Ma, Yecheng Jason and Chen, Claire and Silwal, Sneha and Jain, Aryan and Berges, Vincent-Pierre and Abbeel, Pieter and Malik, Jitendra and others},
  journal={arXiv preprint arXiv:2303.18240},
  year={2023}
}

@inproceedings{savva2019habitat,
  title={Habitat: A platform for embodied ai research},
  author={Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and others},
  booktitle=ICCV,
  year={2019}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle=ICML,
  year={2021}
}

@article{suglia2021embodied,
  title={Embodied bert: A transformer model for embodied, language-guided visual task completion},
  author={Suglia, Alessandro and Gao, Qiaozi and Thomason, Jesse and Thattai, Govind and Sukhatme, Gaurav},
  journal={arXiv preprint arXiv:2108.04927},
  year={2021}
}

@inproceedings{kamath2021mdetr,
  title={MDETR-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  booktitle=NIPS,
  year={2022}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver io: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  booktitle=ICLR,
  year={2022}
}

@inproceedings{ma2023sqa3d,
  title={Sqa3d: Situated question answering in 3d scenes},
  author={Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan},
  booktitle=ICLR,
  year={2023}
}

@inproceedings{huang2023diffusion,
  title={Diffusion-based Generation, Optimization, and Planning in 3D Scenes},
  author={Huang, Siyuan and Wang, Zan and Li, Puhao and Jia, Baoxiong and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Zhu, Song-Chun},
  booktitle=CVPR,
  year={2023}
}

@article{gao2022dialfred,
  title={Dialfred: Dialogue-enabled agents for embodied instruction following},
  author={Gao, Xiaofeng and Gao, Qiaozi and Gong, Ran and Lin, Kaixiang and Thattai, Govind and Sukhatme, Gaurav S},
  journal=RA-L,
  year={2022}
}

@inproceedings{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  booktitle=ICML,
  year={2023}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle=NIPS,
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle=ICML,
  year={2021}
}

@inproceedings{locatello2020object,
  title={Object-centric learning with slot attention},
  author={Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  booktitle=NIPS,
  year={2020}
}

@inproceedings{jia2023improving,
    title={Improving Object-centric Learning with Query Optimization},
    author={Baoxiong Jia and Yu Liu and Siyuan Huang},
    booktitle=ICLR,
    year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal=JMLR,
  year={2020}
}

@article{huang2022perceive,
  title={Perceive, Ground, Reason, and Act: A Benchmark for General-purpose Visual Representation},
  author={Huang, Jiangyong and Zhu, William Yicheng and Jia, Baoxiong and Wang, Zan and Ma, Xiaojian and Li, Qing and Huang, Siyuan},
  journal={arXiv preprint arXiv:2211.15402},
  year={2022}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal=NIPS,
  year={2017}
}

@article{kipf2021conditional,
  title={Conditional object-centric learning from video},
  author={Kipf, Thomas and Elsayed, Gamaleldin F and Mahendran, Aravindh and Stone, Austin and Sabour, Sara and Heigold, Georg and Jonschkowski, Rico and Dosovitskiy, Alexey and Greff, Klaus},
  journal={arXiv preprint arXiv:2111.12594},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@misc{alpaca,
  title={Stanford Alpaca: An Instruction-following LLaMA model},
  author={Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  howpublished={\url{https://github.com/tatsu-lab/stanford_alpaca}},
  year={2023}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal=NIPS,
  year={2020}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle=ECCV,
  year={2020}
}

@article{zou2023generalized,
  title={Generalized Decoding for Pixel, Image, and Language},
  author={Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and others},
  journal=CVPR,
  year={2023}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle=ICCV,
  year={2015}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal=NIPS,
  year={2022}
}

@article{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{schult2022mask3d,
  title={Mask3d for 3d semantic instance segmentation},
  author={Schult, Jonas and Engelmann, Francis and Hermans, Alexander and Litany, Or and Tang, Siyu and Leibe, Bastian},
  journal={arXiv preprint arXiv:2210.03105},
  year={2022}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{gong2023arnold,
  title={ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes},
  author={Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and others},
  booktitle=ICCV,
  year={2023}
}

@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{arumugam2017accurately,
  title={Accurately and efficiently interpreting human-robot instructions of varying granularities},
  author={Arumugam, Dilip and Karamcheti, Siddharth and Gopalan, Nakul and Wong, Lawson LS and Tellex, Stefanie},
  journal=RSS,
  year={2017}
}

@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{luo2023scalable,
  title={Scalable 3D Captioning with Pretrained Models},
  author={Luo, Tiange and Rockwell, Chris and Lee, Honglak and Johnson, Justin},
  journal={arXiv preprint arXiv:2306.07279},
  year={2023}
}

@inproceedings{azuma2022scanqa,
  title={ScanQA: 3D question answering for spatial scene understanding},
  author={Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{chen2020scanrefer,
  title={Scanrefer: 3d object localization in rgb-d scans using natural language},
  author={Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias},
  booktitle=ECCV,
  year={2020}
}

@inproceedings{achlioptas2020referit3d,
  title={Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes},
  author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
  booktitle=ECCV,
  year={2020}
}

@inproceedings{zhu20233d,
  title={3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment},
  author={Zhu, Ziyu and Ma, Xiaojian and Chen, Yixin and Deng, Zhidong and Huang, Siyuan and Li, Qing},
  booktitle=ICCV,
  year={2023}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal=NIPS,
  year={2021}
}

@article{dai2023instructblip,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  journal={arXiv preprint arXiv:2305.06500},
  year={2023}
}

@inproceedings{khandelwal2022simple,
  title={Simple but effective: Clip embeddings for embodied ai},
  author={Khandelwal, Apoorv and Weihs, Luca and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  booktitle=CVPR,
  year={2022}
}

@article{cai2023groot,
  title={GROOT: Learning to Follow Instructions by Watching Gameplay Videos},
  author={Cai, Shaofei and Zhang, Bowei and Wang, Zihao and Ma, Xiaojian and Liu, Anji and Liang, Yitao},
  journal={arXiv preprint arXiv:2310.08235},
  year={2023}
}

@inproceedings{cai2023open,
  title={Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction},
  author={Cai, Shaofei and Wang, Zihao and Ma, Xiaojian and Liu, Anji and Liang, Yitao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13734--13744},
  year={2023}
}

@article{chang2017matterport3d,
  title={Matterport3d: Learning from rgb-d data in indoor environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={arXiv preprint arXiv:1709.06158},
  year={2017}
}

@article{graepel2007bayesian,
  title={A Bayesian skill rating system},
  author={Graepel, Thore and Minka, Tom and Herbrich, R TrueSkill},
  journal={Advances in Neural Information Processing Systems},
  volume={19},
  pages={569--576},
  year={2007}
}

@article{ramakrishnan2021habitat,
  title={Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai},
  author={Ramakrishnan, Santhosh K and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X and others},
  journal={arXiv preprint arXiv:2109.08238},
  year={2021}
}

@article{wang2023describe,
  title={Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents},
  author={Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal={arXiv preprint arXiv:2302.01560},
  year={2023}
}

@article{hong20233d,
  title={3D-LLM: Injecting the 3D World into Large Language Models},
  author={Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
  journal={arXiv preprint arXiv:2307.12981},
  year={2023}
}

@inproceedings{gadre2023cows,
  title={Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation},
  author={Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23171--23181},
  year={2023}
}

@article{majumdar2022zson,
  title={Zson: Zero-shot object-goal navigation using multimodal goal embeddings},
  author={Majumdar, Arjun and Aggarwal, Gunjan and Devnani, Bhavika and Hoffman, Judy and Batra, Dhruv},
  journal=NIPS,
  year={2022}
}

@article{wang2023chat,
  title={Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes},
  author={Wang, Zehan and Huang, Haifeng and Zhao, Yang and Zhang, Ziang and Zhao, Zhou},
  journal={arXiv preprint arXiv:2308.08769},
  year={2023}
}

@article{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@inproceedings{jiang2023vima,
  title={Vima: General robot manipulation with multimodal prompts},
  author={Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  booktitle=ICML,
  year={2023}
}

@article{fan2022minedojo,
  title={Minedojo: Building open-ended embodied agents with internet-scale knowledge},
  author={Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  journal=NIPS,
  year={2022}
}

@article{mu2023embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2305.15021},
  year={2023}
}

@article{ahn2022can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@inproceedings{cliport,
  title     = {CLIPort: What and Where Pathways for Robotic Manipulation},
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = CORL,
  year      = {2021},
}

@article{li2023mimic,
  title={Mimic-it: Multi-modal in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@inproceedings{hong2021vlgrammar,
  title={Vlgrammar: Grounded grammar induction of vision and language},
  author={Hong, Yining and Li, Qing and Zhu, Song-Chun and Huang, Siyuan},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{kerr2023lerf,
  title={Lerf: Language embedded radiance fields},
  author={Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  booktitle=ICCV,
  year={2023}
}

@article{shafiullah2022clip,
  title={Clip-fields: Weakly supervised semantic fields for robotic memory},
  author={Shafiullah, Nur Muhammad Mahi and Paxton, Chris and Pinto, Lerrel and Chintala, Soumith and Szlam, Arthur},
  journal={arXiv preprint arXiv:2210.05663},
  year={2022}
}

@inproceedings{peng2023openscene,
  title={Openscene: 3d scene understanding with open vocabularies},
  author={Peng, Songyou and Genova, Kyle and Jiang, Chiyu and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas and others},
  booktitle=CVPR,
  year={2023}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@article{xu2023pointllm,
  title={PointLLM: Empowering Large Language Models to Understand Point Clouds},
  author={Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  journal={arXiv preprint arXiv:2308.16911},
  year={2023}
}

@inproceedings{thakur2021augmented,
  title={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks},
  author={Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},
  booktitle=NAACL,
  year={2021}
}

@inproceedings{wei2022finetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle=ICLR,
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal=NIPS,
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle=ICLR,
  year={2022}
}

@inproceedings{wang2023self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle=ACL,
  year={2023}
}

@inproceedings{guo2023images,
  title={From images to textual prompts: Zero-shot vqa with frozen large language models},
  author={Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven CH},
  booktitle=CVPR,
  year={2023}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@misc{openai2022chatgpt,
  title={ChatGPT},
  author={OpenAI},
  howpublished={\url{https://openai.com/blog/chatgpt/}},
  year={2022}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{liu2023aligning,
  title={Aligning Large Multi-Modal Model with Robust Instruction Tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}

@article{yin2023lamm,
  title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
  author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},
  journal={arXiv preprint arXiv:2306.06687},
  year={2023}
}

@article{zhu2023chatgpt,
  title={Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
  author={Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2303.06594},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{gong2023multimodal,
  title={Multimodal-gpt: A vision and language model for dialogue with humans},
  author={Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
  journal={arXiv preprint arXiv:2305.04790},
  year={2023}
}

@inproceedings{wu2021scenegraphfusion,
  title={Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences},
  author={Wu, Shun-Cheng and Wald, Johanna and Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
  booktitle=CVPR,
  year={2021}
}

@article{reed2022generalist,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal=TMLR,
  year={2022}
}

@inproceedings{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle=EMNLP,
  year={2019}
}

@inproceedings{chen2021scan2cap,
  title={Scan2cap: Context-aware dense captioning in rgb-d scans},
  author={Chen, Zhenyu and Gholami, Ali and Nie{\ss}ner, Matthias and Chang, Angel X},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{zhao20213dvg,
  title={3DVG-Transformer: Relation modeling for visual grounding on point clouds},
  author={Zhao, Lichen and Cai, Daigang and Sheng, Lu and Xu, Dong},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{huang2022multi,
  title={Multi-view transformer for 3d visual grounding},
  author={Huang, Shijia and Chen, Yilun and Jia, Jiaya and Wang, Liwei},
  booktitle=CVPR,
  year={2022}
}

@article{chen2022language,
  title={Language conditioned spatial relation reasoning for 3d object grounding},
  author={Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  journal=NIPS,
  year={2022}
}

@inproceedings{lu2023unified,
  title={Unified-io: A unified model for vision, language, and multi-modal tasks},
  author={Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  booktitle=ICLR,
  year={2023}
}

@inproceedings{wang2023images,
  title={Images speak in images: A generalist painter for in-context visual learning},
  author={Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun},
  booktitle=CVPR,
  year={2023}
}

@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  year={2015}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal=BBS,
  year={2017}
}

@article{zhu2020dark,
  title={Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense},
  author={Zhu, Yixin and Gao, Tao and Fan, Lifeng and Huang, Siyuan and Edmonds, Mark and Liu, Hangxin and Gao, Feng and Zhang, Chi and Qi, Siyuan and Wu, Ying Nian and others},
  journal={Engineering},
  year={2020}
}

@article{schmidhuber2018one,
  title={One big net for everything},
  author={Schmidhuber, Juergen},
  journal={arXiv preprint arXiv:1802.08864},
  year={2018}
}

@article{mountcastle1979organizing,
  title={An organizing principle for cerebral function: the unit module and the distributed system},
  author={Mountcastle, Vernon B},
  journal={The neurosciences. Fourth study program},
  year={1979}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal=NIPS,
  year={2022}
}

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle=CVPR,
  year={2017}
}

@misc{li2023multimodal,
      title={Multimodal Foundation Models: From Specialists to General-Purpose Assistants}, 
      author={Chunyuan Li and Zhe Gan and Zhengyuan Yang and Jianwei Yang and Linjie Li and Lijuan Wang and Jianfeng Gao},
      year={2023},
      eprint={2309.10020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bang2023hallucination,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@inproceedings{huang2022inner,
  title={Inner monologue: Embodied reasoning through planning with language models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  booktitle=CoRL,
  year={2022}
}

@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@inproceedings{ramrakhya2022habitat,
  title={Habitat-web: Learning embodied object-search strategies from human demonstrations at scale},
  author={Ramrakhya, Ram and Undersander, Eric and Batra, Dhruv and Das, Abhishek},
  booktitle=CVPR,
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal=NIPS,
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{deitke2023objaverse,
  title={Objaverse: A universe of annotated 3d objects},
  author={Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali},
  booktitle=CVPR,
  year={2023}
}

@inproceedings{hu2022lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle=ICLR,
  year={2022}
}

@inproceedings{wald2019rio,
  title={Rio: 3d object instance re-localization in changing indoor environments},
  author={Wald, Johanna and Avetisyan, Armen and Navab, Nassir and Tombari, Federico and Nie{\ss}ner, Matthias},
  booktitle=ICCV,
  year={2019}
}

@article{qi2017pointnet++,
  title={Pointnet++: Deep hierarchical feature learning on point sets in a metric space},
  author={Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  journal=NIPS,
  year={2017}
}

@inproceedings{yu2022point,
  title={Point-bert: Pre-training 3d point cloud transformers with masked point modeling},
  author={Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
  booktitle=CVPR,
  year={2022}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle=CVPR,
  year={2022}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  year={2016}
}

@inproceedings{cai20223djcg,
  title={3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds},
  author={Cai, Daigang and Zhao, Lichen and Zhang, Jing and Sheng, Lu and Xu, Dong},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{chen2023end,
  title={End-to-end 3d dense captioning with vote2cap-detr},
  author={Chen, Sijin and Zhu, Hongyuan and Chen, Xin and Lei, Yinjie and Yu, Gang and Chen, Tao},
  booktitle=CVPR,
  year={2023}
}

@inproceedings{lei2021less,
  title={Less is More: ClipBERT for Video-and-Language Learningvia Sparse Sampling},
  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{zeng2021transporter,
  title={Transporter networks: Rearranging the visual world for robotic manipulation},
  author={Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Sindhwani, Vikas and others},
  booktitle=CoRL,
  year={2021}
}

@article{jxma_llm_vla_vlm_mas_multiagent_2023,
  title={MindAgent: Emergent Gaming Interaction},
  author={Gong, Ran and Huang, Qiuyuan and Ma, Xiaojian and Vo, Hoi and Durante, Zane and Noda, Yusuke and Zheng, Zilong and Zhu, Song-Chun and Terzopoulos, Demetri and Fei-Fei, Li and others},
  journal={arXiv preprint arXiv:2309.09971},
  year={2023}
}

@article{jxma_vlm_multimodal_2023,
  title={MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning},
  author={Zhao, Haozhe and Cai, Zefan and Si, Shuzheng and Ma, Xiaojian and An, Kaikai and Chen, Liang and Liu, Zixuan and Wang, Sheng and Han, Wenjuan and Chang, Baobao},
  journal={arXiv preprint arXiv:2309.07915},
  year={2023}
}

@inproceedings{li2023evaluating,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle=EMNLP,
  year={2023}
}

@inproceedings{chen2024ll3da,
  title={LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning},
  author={Chen, Sijin and Chen, Xin and Zhang, Chi and Li, Mingsheng and Yu, Gang and Fei, Hao and Zhu, Hongyuan and Fan, Jiayuan and Chen, Tao},
  booktitle=CVPR,
  year={2024}
}
