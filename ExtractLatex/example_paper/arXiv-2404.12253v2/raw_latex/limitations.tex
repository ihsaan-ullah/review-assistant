Despite the promising results demonstrated by \model{} in this study, there are several limitations that requires further exploration. (\RN{1}) Our current implementation employs relatively simple methods for generating synthetic prompts. Future iterations of \model{} should explore advanced techniques, such as Self-Instruct, to create both diverse and model capability-awared prompts. (\RN{2}) Although \model{} demonstrates improvements over base models, its performance in greedy sampling is substantially inferior to that observed when decoded with \emcts{}. This indicates that the full potential of MCTS for self-improvement in LLMs has not yet been fully realized. Two potential factors contributing to this issue have been identified: a) the self-improvement loop may not be leveraging sufficient data; and b) the base model may be limited in its capacity for rapid learning. Addressing these concerns could lead to more significant improvemens. (\RN{3}) In our existing framework, the critic models remain static. We will explore mechanisms to continually update critic models to adapt to new policy models. This will help ensure the discriminator-generator gap and improve the overall training dynamics. (\RN{4}) The evaluation of \model{} has been limited to mathematical reasoning tasks. To verify the generalizability and broader applicability of the framework, future research will need to extend its application to other domains.