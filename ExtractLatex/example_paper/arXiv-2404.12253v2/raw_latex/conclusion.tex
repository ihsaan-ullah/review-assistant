In this paper, we introduce \model{}, an imagination-searching-criticizing framework designed for the self-improvement of LLMs without the necessity of additional annotations. At the heart of it is the integration of MCTS with LLMs. To tackle the inherent challenges associated with this integration, including data scarcity, the vastness of search spaces, and the subjective nature of feedback in language tasks, we introduce a data synthesizer for strategic prompt synthesis, an optimized MCTS tailored for efficient search in language tasks, and a trio of critic models to provide precise feedback. Our experimental findings on mathematical reasoning tasks reveal that \model{} significantly boosts the performance of LLMs without requiring extra data annotations. Moreover, when decoded with \emcts{}, \model{} performs comparably to GPT-4, highlighting the potential for self-improvement in LLMs.