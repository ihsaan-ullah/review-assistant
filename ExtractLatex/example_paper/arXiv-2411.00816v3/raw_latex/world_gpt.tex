{\color{warningcolor} In this subsection, all the following content comes from WhizResearch-12B and WhizReviewer-123B.}
\subsubsection{Outline}
\begin{example}{}{test3}
Reinforcement learning (RL) agents can significantly benefit from learning an internal world model to predict future observations, which can then be used to train a policy more efficiently. However, existing world models suffer from several limitations: they typically operate on static image representations, which leads to weak semantic priors, and their latent spaces are either continuous or only sparsely quantized, which hinders representational quality and generation capabilities. These limitations affect the agent's data efficiency, planning abilities, and generation capabilities. To address these issues, the paper proposes a novel approach that leverages both a semantic prior and a quantized latent space to capture complex environments more accurately and efficiently. This approach aims to enhance the agent's data efficiency and planning abilities while reducing computational costs.

\end{example}
\begin{note}{}{test3}
The paper introduces World GPT, an auto-regressive world model for reinforcement learning that combines a semantic prior with a quantized latent space to predict future observations. This approach enhances data efficiency and planning abilities in complex environments while reducing computational costs. World GPT leverages the latent space of a pre-trained VQ-GAN model to encode observations into discrete tokens and predicts the next visual token in the sequence. It is the first world model that can generate videos from textual descriptions, opening up new possibilities for exploration and real-world applications such as training free-form interactive agents.

\end{note}

\subsubsection{Experimental Setups}

\begin{itemize}
    \item \textbf{Atari 100K Benchmark Evaluation:} The experiment evaluates World GPT's performance in the Atari 100K benchmark, which measures sample efficiency in diverse environments. The benchmark includes 26 Atari 2600 games with human-normalized scores ranging from 0.0 to 100.0. The experiment compares World GPT with five baselines: PPO, DreamerV3, BBMB, EfficientZero, and STORM. Each method is trained with 100K steps of online interaction and evaluated without any task-specific hyperparameter tuning. The results are averaged over 3 seeds for each game, with 95\% confidence intervals. The goal is to demonstrate World GPT's superior data efficiency and planning abilities in complex environments while reducing computational costs.
    
    \item \textbf{Crafter Environment Evaluation:} The experiment evaluates World GPT's performance in the Crafter environment, which requires exploration, generalization, and long-term reasoning. Crafter is a three-dimensional survival game in which an agent collects resources and crafts tools to build structures. The agent acts every 5 seconds, and the episode lasts for 2000 in-game steps, corresponding to 1000 environment steps. The experiment compares World GPT with three baselines: PPO, DreamerV3, and a combination of DreamerV3 with a fixed action osceder (OSD). The learning curve for each approach is plotted, along with 95\% confidence intervals across 5 seeds. The goal is to demonstrate World GPT's ability to achieve super-human performance in the Crafter environment with minimal online interactions.
    
    \item \textbf{Offline World Model Pre-training Experiment:} This experiment investigates the impact of pre-training the world model offline using offline data. The experiment compares agents with access to either offline pre-training data or online pre-training through supervised pre-training on the Atari 100K benchmark. Each agent is trained for 500K steps on Atari 100K offline data. For agents with online pre-training, a separate replay buffer of 500K transitions is used for supervised pre-training. The results are averaged over 3 seeds for each game, with the first 20\% of transitions being used as warmup interactions without storing experience and the last 80\% of transitions as interaction data. The goal is to demonstrate the effectiveness of pre-training the world model offline as opposed to using supervised pre-training online.
    
    \item \textbf{Generative Video Prediction Experiment:} This experiment evaluates World GPT's video prediction capabilities in 4 Atari environments. The experiment compares World GPT with 4 baselines: EfficientZero, a recurrent transformer world model (TransformerEnc), a discrete latent recurrent world model (VQPlan), and a version of World GPT that uses the same architecture but predicts the next frame conditioned on the previous frame using pixel inputs. Each method is allowed 24 hours of online interaction with the Atari 2600 environment to train a single agent. For each agent, 100 videos are recorded during evaluation -- each video consists of 64 frames starting with the initial observation for the game and then 63 consecutive frames generated by the world model or predicted by the baselines. The goal is to evaluate the quality of the generated videos and demonstrate World GPT's strong semantic prior that allows the model to correctly re-grow legs for Pac-Man.
    
    \item \textbf{Exploration Experiment:} This experiment evaluates World GPT's exploration capabilities in the Crafter environment using the Plan2Explore self-supervised exploration objective. The experiment compares World GPT with a baseline which uses the same architecture but predicts rewards instead of the next observation. The learning curves for each approach are plotted, along with 95\% confidence intervals across 5 seeds. The goal is to demonstrate World GPT's improved exploration performance using Plan2Explore and its ability to achieve super-human performance with limited online interactions.
    
    \item \textbf{Learning a Prior for Image Generation Experiment:} This experiment investigates whether the quantized latent space and auto-regressive architecture of World GPT can be used to train a more flexible world model. The experiment uses the same architecture as World GPT but trains it on offline data to predict rewards as well as the next observation and uses a GPT-like head to predict text descriptions of images. The experiment evaluates this approach on the same set of videos used in the generative video prediction experiment. The goal is to evaluate the quality of the generated images and demonstrate World GPT's ability to generate coherent images with reasonable shapes and colors.
    
    \item \textbf{Applying World GPT to the Real World:} This experiment evaluates World GPT's real-world applications by training a single agent to drive an autonomous car on above-ground roads and find a drop-off point to cross the river. The agent uses a map of the area as input, which is encoded into discrete latent codes using the VQ-GAN trained to decode the training images. The agent uses the arguments to select a goal location and action based on the predicted future observations. The goal is to demonstrate World GPT's ability to train a free-form interactive agent that can drive to a goal location based on predicted future images without using any reinforcement learning.
\end{itemize}
